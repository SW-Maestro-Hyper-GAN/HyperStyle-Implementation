{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HyperStyle.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# configs.paths_config.py\n",
        "\n",
        "dataset_paths = {\n",
        "\t'cars_train': '',\n",
        "\t'cars_test': '',\n",
        "\n",
        "\t'celeba_train': '',\n",
        "\t'celeba_test': '',\n",
        "\t'celeba_test_w_inv': '',\n",
        "\t'celeba_test_w_latents': '',\n",
        "\n",
        "\t'ffhq': '',\n",
        "\t'ffhq_w_inv': '',\n",
        "\t'ffhq_w_latents': '',\n",
        "\n",
        "\t'afhq_wild_train': '',\n",
        "\t'afhq_wild_test': '',\n",
        "\n",
        "}\n",
        "\n",
        "model_paths = {\n",
        "\t# models for backbones and losses\n",
        "\t'ir_se50': 'pretrained_models/model_ir_se50.pth',\n",
        "\t'resnet34': 'pretrained_models/resnet34-333f7ec4.pth',\n",
        "\t'moco': 'pretrained_models/moco_v2_800ep_pretrain.pt',\n",
        "\t# stylegan2 generators\n",
        "\t'stylegan_ffhq': 'pretrained_models/stylegan2-ffhq-config-f.pt',\n",
        "\t'stylegan_cars': 'pretrained_models/stylegan2-car-config-f.pt',\n",
        "\t'stylegan_ada_wild': 'pretrained_models/afhqwild.pt',\n",
        "\t# model for face alignment\n",
        "\t'shape_predictor': 'pretrained_models/shape_predictor_68_face_landmarks.dat',\n",
        "\t# models for ID similarity computation\n",
        "\t'curricular_face': 'pretrained_models/CurricularFace_Backbone.pth',\n",
        "\t'mtcnn_pnet': 'pretrained_models/mtcnn/pnet.npy',\n",
        "\t'mtcnn_rnet': 'pretrained_models/mtcnn/rnet.npy',\n",
        "\t'mtcnn_onet': 'pretrained_models/mtcnn/onet.npy',\n",
        "\t# WEncoders for training on various domains\n",
        "\t'faces_w_encoder': 'pretrained_models/faces_w_encoder.pt',\n",
        "\t'cars_w_encoder': 'pretrained_models/cars_w_encoder.pt',\n",
        "\t'afhq_wild_w_encoder': 'pretrained_models/afhq_wild_w_encoder.pt',\n",
        "\t# models for domain adaptation\n",
        "\t'restyle_e4e_ffhq': 'pretrained_models/restyle_e4e_ffhq_encode.pt',\n",
        "\t'stylegan_pixar': 'pretrained_models/pixar.pt',\n",
        "\t'stylegan_toonify': 'pretrained_models/ffhq_cartoon_blended.pt',\n",
        "\t'stylegan_sketch': 'pretrained_models/sketch.pt',\n",
        "\t'stylegan_disney': 'pretrained_models/disney_princess.pt'\n",
        "}\n",
        "\n",
        "edit_paths = {\n",
        "\t'age': 'editing/interfacegan_directions/age.pt',\n",
        "\t'smile': 'editing/interfacegan_directions/smile.pt',\n",
        "\t'pose': 'editing/interfacegan_directions/pose.pt',\n",
        "\t'cars': 'editing/ganspace_directions/cars_pca.pt',\n",
        "\t'styleclip': {\n",
        "\t\t'delta_i_c': 'editing/styleclip/global_directions/ffhq/fs3.npy',\n",
        "\t\t's_statistics': 'editing/styleclip/global_directions/ffhq/S_mean_std',\n",
        "\t\t'templates': 'editing/styleclip/global_directions/templates.txt'\n",
        "\t}\n",
        "}"
      ],
      "metadata": {
        "id": "1GfD4cYX6v1X"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configs.data_configs.py\n",
        "\n",
        "# from configs import transforms_config\n",
        "# from configs.paths_config import dataset_paths\n",
        "\n",
        "\n",
        "DATASETS = {\n",
        "\t'ffhq_hypernet': {\n",
        "\t\t'transforms': EncodeTransforms,\n",
        "\t\t'train_source_root': dataset_paths['ffhq'],\n",
        "\t\t'train_target_root': dataset_paths['ffhq'],\n",
        "\t\t'test_source_root': dataset_paths['celeba_test'],\n",
        "\t\t'test_target_root': dataset_paths['celeba_test'],\n",
        "\t},\n",
        "\t'ffhq_hypernet_pre_extract': {\n",
        "\t\t'transforms': NoFlipTransforms,\n",
        "\t\t'train_source_root': dataset_paths['ffhq_w_inv'],\n",
        "\t\t'train_target_root': dataset_paths['ffhq'],\n",
        "\t\t'train_latents_path': dataset_paths['ffhq_w_latents'],\n",
        "\t\t'test_source_root': dataset_paths['celeba_test_w_inv'],\n",
        "\t\t'test_target_root': dataset_paths['celeba_test'],\n",
        "\t\t'test_latents_path': dataset_paths['celeba_test_w_latents']\n",
        "\t},\n",
        "\t\"cars_hypernet\": {\n",
        "\t\t'transforms': CarsEncodeTransforms,\n",
        "\t\t'train_source_root': dataset_paths['cars_train'],\n",
        "\t\t'train_target_root': dataset_paths['cars_train'],\n",
        "\t\t'test_source_root': dataset_paths['cars_test'],\n",
        "\t\t'test_target_root': dataset_paths['cars_test']\n",
        "\t},\n",
        "\t\"afhq_wild_hypernet\": {\n",
        "\t\t'transforms': EncodeTransforms,\n",
        "\t\t'train_source_root': dataset_paths['afhq_wild_train'],\n",
        "\t\t'train_target_root': dataset_paths['afhq_wild_train'],\n",
        "\t\t'test_source_root': dataset_paths['afhq_wild_test'],\n",
        "\t\t'test_target_root': dataset_paths['afhq_wild_test']\n",
        "\t}\n",
        "}"
      ],
      "metadata": {
        "id": "tTY9Eap5B71g"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# datasets.gt_res_dataset.py\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class GTResDataset(Dataset):\n",
        "\n",
        "\tdef __init__(self, root_path, gt_dir=None, transform=None, transform_train=None):\n",
        "\t\tself.pairs = []\n",
        "\t\tfor f in os.listdir(root_path):\n",
        "\t\t\timage_path = os.path.join(root_path, f)\n",
        "\t\t\tgt_path = os.path.join(gt_dir, f)\n",
        "\t\t\tif f.endswith(\".jpg\") or f.endswith(\".png\") or f.endswith(\".jpeg\"):\n",
        "\t\t\t\tself.pairs.append([image_path, gt_path, None])\n",
        "\t\tself.transform = transform\n",
        "\t\tself.transform_train = transform_train\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.pairs)\n",
        "\n",
        "\tdef __getitem__(self, index):\n",
        "\t\tfrom_path, to_path, _ = self.pairs[index]\n",
        "\t\tfrom_im = Image.open(from_path).convert('RGB')\n",
        "\t\tto_im = Image.open(to_path).convert('RGB')\n",
        "\t\tif self.transform:\n",
        "\t\t\tto_im = self.transform(to_im)\n",
        "\t\t\tfrom_im = self.transform(from_im)\n",
        "\t\treturn from_im, to_im"
      ],
      "metadata": {
        "id": "GPLypio1_FDt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# datasets.latents_images_dataset.py\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "# from utils import data_utils\n",
        "\n",
        "\n",
        "class LatentsImagesDataset(Dataset):\n",
        "\n",
        "\tdef __init__(self, source_root, target_root, latents_path, opts, target_transform=None, source_transform=None):\n",
        "\t\t# path to inversions directory\n",
        "\t\tself.source_root = source_root \n",
        "\t\t# path to original dataset\n",
        "\t\tself.target_paths = sorted(data_utils.make_dataset(target_root)) \n",
        "\t\t# path to latents corresponding to inversions\n",
        "\t\t# this should be a dictionary mapping image name to the image's latent code\n",
        "\t\tself.latents = torch.load(latents_path, map_location='cpu')  \n",
        "\t\tself.latents.requires_grad = False\n",
        "\t\tself.source_transform = source_transform\n",
        "\t\tself.target_transform = target_transform\n",
        "\t\tself.opts = opts\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.target_paths)\n",
        "\n",
        "\tdef __getitem__(self, index):\n",
        "\t\tfrom_path = os.path.join(self.source_root, f'{index+1:05d}.png')\n",
        "\t\tto_path = self.target_paths[index]\n",
        "\n",
        "\t\tfrom_im = Image.open(from_path).convert('RGB')\n",
        "\t\tto_im = Image.open(to_path).convert('RGB')\n",
        "\n",
        "\t\tif self.target_transform:\n",
        "\t\t\tto_im = self.target_transform(to_im)\n",
        "\n",
        "\t\tif self.source_transform:\n",
        "\t\t\tfrom_im = self.source_transform(from_im)\n",
        "\t\telse:\n",
        "\t\t\tfrom_im = to_im\n",
        "\n",
        "\t\tlatent = self.latents[os.path.basename(from_path)]\n",
        "\t\tif latent.ndim == 1:\n",
        "\t\t\tlatent = latent.repeat(18, 1)\n",
        "\n",
        "\t\treturn from_im, to_im, latent"
      ],
      "metadata": {
        "id": "3s6xZ1q7Bzfo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configs.transforms_config.py\n",
        "\n",
        "from abc import abstractmethod\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "class TransformsConfig(object):\n",
        "\n",
        "\tdef __init__(self, opts):\n",
        "\t\tself.opts = opts\n",
        "\n",
        "\t@abstractmethod\n",
        "\tdef get_transforms(self):\n",
        "\t\tpass\n",
        "\n",
        "\n",
        "class EncodeTransforms(TransformsConfig):\n",
        "\n",
        "\tdef __init__(self, opts):\n",
        "\t\tsuper(EncodeTransforms, self).__init__(opts)\n",
        "\n",
        "\tdef get_transforms(self):\n",
        "\t\ttransforms_dict = {\n",
        "\t\t\t'transform_gt_train': transforms.Compose([\n",
        "\t\t\t\ttransforms.Resize((256, 256)),\n",
        "\t\t\t\ttransforms.RandomHorizontalFlip(0.5),\n",
        "\t\t\t\ttransforms.ToTensor(),\n",
        "\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n",
        "\t\t\t'transform_source': None,\n",
        "\t\t\t'transform_test': transforms.Compose([\n",
        "\t\t\t\ttransforms.Resize((256, 256)),\n",
        "\t\t\t\ttransforms.ToTensor(),\n",
        "\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n",
        "\t\t\t'transform_inference': transforms.Compose([\n",
        "\t\t\t\ttransforms.Resize((256, 256)),\n",
        "\t\t\t\ttransforms.ToTensor(),\n",
        "\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "\t\t}\n",
        "\t\treturn transforms_dict\n",
        "\n",
        "\n",
        "class NoFlipTransforms(TransformsConfig):\n",
        "\n",
        "\tdef __init__(self, opts):\n",
        "\t\tsuper(NoFlipTransforms, self).__init__(opts)\n",
        "\n",
        "\tdef get_transforms(self):\n",
        "\t\ttransforms_dict = {\n",
        "\t\t\t'transform_gt_train': transforms.Compose([\n",
        "\t\t\t\ttransforms.Resize((256, 256)),\n",
        "\t\t\t\ttransforms.ToTensor(),\n",
        "\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n",
        "\t\t\t'transform_source':  transforms.Compose([\n",
        "\t\t\t\ttransforms.Resize((256, 256)),\n",
        "\t\t\t\ttransforms.ToTensor(),\n",
        "\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n",
        "\t\t\t'transform_test': transforms.Compose([\n",
        "\t\t\t\ttransforms.Resize((256, 256)),\n",
        "\t\t\t\ttransforms.ToTensor(),\n",
        "\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n",
        "\t\t\t'transform_inference': transforms.Compose([\n",
        "\t\t\t\ttransforms.Resize((256, 256)),\n",
        "\t\t\t\ttransforms.ToTensor(),\n",
        "\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "\t\t}\n",
        "\t\treturn transforms_dict\n",
        "\n",
        "\n",
        "class CarsEncodeTransforms(TransformsConfig):\n",
        "\n",
        "\tdef __init__(self, opts):\n",
        "\t\tsuper(CarsEncodeTransforms, self).__init__(opts)\n",
        "\n",
        "\tdef get_transforms(self):\n",
        "\t\ttransforms_dict = {\n",
        "\t\t\t'transform_gt_train': transforms.Compose([\n",
        "\t\t\t\ttransforms.Resize((192, 256)),\n",
        "\t\t\t\ttransforms.RandomHorizontalFlip(0.5),\n",
        "\t\t\t\ttransforms.ToTensor(),\n",
        "\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n",
        "\t\t\t'transform_source': None,\n",
        "\t\t\t'transform_test': transforms.Compose([\n",
        "\t\t\t\ttransforms.Resize((192, 256)),\n",
        "\t\t\t\ttransforms.ToTensor(),\n",
        "\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n",
        "\t\t\t'transform_inference': transforms.Compose([\n",
        "\t\t\t\ttransforms.Resize((192, 256)),\n",
        "\t\t\t\ttransforms.ToTensor(),\n",
        "\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "\t\t}\n",
        "\t\treturn transforms_dict"
      ],
      "metadata": {
        "id": "-0DcCGggDCfy"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# datasets.images_dataset.py\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from utils import data_utils\n",
        "\n",
        "\n",
        "class ImagesDataset(Dataset):\n",
        "\n",
        "\tdef __init__(self, source_root, target_root, opts, target_transform=None, source_transform=None):\n",
        "\t\tself.source_paths = sorted(data_utils.make_dataset(source_root))\n",
        "\t\tself.target_paths = sorted(data_utils.make_dataset(target_root))\n",
        "\t\tself.source_transform = source_transform\n",
        "\t\tself.target_transform = target_transform\n",
        "\t\tself.opts = opts\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.source_paths)\n",
        "\n",
        "\tdef __getitem__(self, index):\n",
        "\t\tfrom_path = self.source_paths[index]\n",
        "\t\tto_path = self.target_paths[index]\n",
        "\n",
        "\t\tfrom_im = Image.open(from_path).convert('RGB')\n",
        "\t\tto_im = Image.open(to_path).convert('RGB')\n",
        "\n",
        "\t\tif self.target_transform:\n",
        "\t\t\tto_im = self.target_transform(to_im)\n",
        "\n",
        "\t\tif self.source_transform:\n",
        "\t\t\tfrom_im = self.source_transform(from_im)\n",
        "\t\telse:\n",
        "\t\t\tfrom_im = to_im\n",
        "\n",
        "\t\treturn from_im, to_im"
      ],
      "metadata": {
        "id": "88TBBlXjBMiM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# datasets.inference_dataset.py\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from utils import data_utils\n",
        "\n",
        "\n",
        "class InferenceDataset(Dataset):\n",
        "\n",
        "\tdef __init__(self, root, opts, transform=None):\n",
        "\t\tself.paths = sorted(data_utils.make_dataset(root))\n",
        "\t\tself.transform = transform\n",
        "\t\tself.opts = opts\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.paths)\n",
        "\n",
        "\tdef __getitem__(self, index):\n",
        "\t\tfrom_path = self.paths[index]\n",
        "\t\tfrom_im = Image.open(from_path).convert('RGB')\n",
        "\t\tif self.transform:\n",
        "\t\t\tfrom_im = self.transform(from_im)\n",
        "\t\treturn from_im"
      ],
      "metadata": {
        "id": "SBMqBaBYBuVd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training.coach_hyperstyle.ipynb\n",
        "import os\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# from datasets.dataset_fetcher import DatasetFetcher\n",
        "\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# from utils import common, train_utils\n",
        "# from criteria import id_loss, moco_loss\n",
        "# from configs import data_configs\n",
        "# from datasets.images_dataset import ImagesDataset\n",
        "# from datasets.latents_images_dataset import LatentsImagesDataset\n",
        "# from criteria.lpips.lpips import LPIPS\n",
        "# from models.hyperstyle import HyperStyle\n",
        "# from training.ranger import Ranger\n",
        "\n",
        "\n",
        "class Coach:\n",
        "    def __init__(self, opts):\n",
        "        self.opts = opts\n",
        "\n",
        "        self.global_step = 0\n",
        "\n",
        "        self.device = 'cuda:0'\n",
        "        self.opts.device = self.device\n",
        "\n",
        "        # Initialize network\n",
        "        self.net = HyperStyle(self.opts).to(self.device)\n",
        "\n",
        "        # Estimate latent_avg via dense sampling if latent_avg is not available\n",
        "        if self.net.latent_avg is None:\n",
        "            self.net.latent_avg = self.net.decoder.mean_latent(int(1e5))[0].detach()\n",
        "\n",
        "        # Initialize loss\n",
        "        if self.opts.id_lambda > 0 and self.opts.moco_lambda > 0:\n",
        "            raise ValueError('Both ID and MoCo loss have lambdas > 0! Please select only one to have non-zero lambda!')\n",
        "        self.mse_loss = nn.MSELoss().to(self.device).eval()\n",
        "        if self.opts.lpips_lambda > 0:\n",
        "            self.lpips_loss = LPIPS(net_type='alex').to(self.device).eval()\n",
        "        if self.opts.id_lambda > 0:\n",
        "            self.id_loss = id_loss.IDLoss(opts).to(self.device).eval()\n",
        "        if self.opts.moco_lambda > 0:\n",
        "            self.moco_loss = moco_loss.MocoLoss()\n",
        "\n",
        "        # Initialize optimizer\n",
        "        self.optimizer = self.configure_optimizers()\n",
        "\n",
        "        # Initialize dataset\n",
        "        self.train_dataset, self.test_dataset = self.configure_datasets()\n",
        "        self.train_dataloader = DataLoader(self.train_dataset,\n",
        "                                           batch_size=self.opts.batch_size,\n",
        "                                           shuffle=True,\n",
        "                                           num_workers=int(self.opts.workers),\n",
        "                                           drop_last=True)\n",
        "        self.test_dataloader = DataLoader(self.test_dataset,\n",
        "                                          batch_size=self.opts.test_batch_size,\n",
        "                                          shuffle=False,\n",
        "                                          num_workers=int(self.opts.test_workers),\n",
        "                                          drop_last=True)\n",
        "\n",
        "        # Initialize logger\n",
        "        log_dir = os.path.join(opts.exp_dir, 'logs')\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "        self.logger = SummaryWriter(log_dir=log_dir)\n",
        "\n",
        "        # Initialize checkpoint dir\n",
        "        self.checkpoint_dir = os.path.join(opts.exp_dir, 'checkpoints')\n",
        "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
        "        self.best_val_loss = None\n",
        "        if self.opts.save_interval is None:\n",
        "            self.opts.save_interval = self.opts.max_steps\n",
        "\n",
        "    def parse_batch(self, batch):\n",
        "        x, y, y_hat, latents = None, None, None, None\n",
        "        if isinstance(self.train_dataset, ImagesDataset):\n",
        "            x, y = batch\n",
        "            x, y = x.to(self.device).float(), y.to(self.device).float()\n",
        "        elif isinstance(self.train_dataset, LatentsImagesDataset):\n",
        "            y_hat, y, latents = batch  # source (inversion), target, and latent code\n",
        "            y_hat, y, latents = y_hat.to(self.device).float(), y.to(self.device).float(), latents.to(self.device)\n",
        "            x = y\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported dataset type\")\n",
        "        return x, y, y_hat, latents\n",
        "\n",
        "    def perform_forward_on_batch(self, batch, train=False):\n",
        "        latent, weights_deltas, w_inversion, initial_inversion = None, None, None, None\n",
        "        cur_loss_dict, id_logs = None, None\n",
        "        x, y, y_hat, codes = self.parse_batch(batch)\n",
        "        y_hats = {idx: [] for idx in range(x.shape[0])}\n",
        "        for iter in range(self.opts.n_iters_per_batch):\n",
        "            if iter > 0 and train:\n",
        "                weights_deltas = [w.clone().detach().requires_grad_(True) if w is not None else w\n",
        "                                  for w in weights_deltas]\n",
        "                y_hat = y_hat.clone().detach().requires_grad_(True)\n",
        "            y_hat, latent, weights_deltas, codes, w_inversion = self.net.forward(x,\n",
        "                                                                                 y_hat=y_hat,\n",
        "                                                                                 codes=codes,\n",
        "                                                                                 weights_deltas=weights_deltas,\n",
        "                                                                                 return_latents=True,\n",
        "                                                                                 randomize_noise=False,\n",
        "                                                                                 return_weight_deltas_and_codes=True,\n",
        "                                                                                 resize=True)\n",
        "            if iter == 0:\n",
        "                initial_inversion = w_inversion\n",
        "            if \"cars\" in self.opts.dataset_type:\n",
        "                y_hat = y_hat[:, :, 32:224, :]\n",
        "            loss, cur_loss_dict, id_logs = self.calc_loss(x=y,\n",
        "                                                          y=y,\n",
        "                                                          y_hat=y_hat,\n",
        "                                                          latent=latent,\n",
        "                                                          weights_deltas=weights_deltas)\n",
        "            if train:\n",
        "                loss.backward()\n",
        "\n",
        "            # store intermediate outputs\n",
        "            for idx in range(x.shape[0]):\n",
        "                y_hats[idx].append([y_hat[idx].detach().cpu(), id_logs[idx]['diff_target']])\n",
        "        return x, y, y_hats, cur_loss_dict, id_logs, initial_inversion\n",
        "\n",
        "    def train(self):\n",
        "        self.net.train()\n",
        "        while self.global_step < self.opts.max_steps:\n",
        "            for batch_idx, batch in enumerate(self.train_dataloader):\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                x, y, y_hat, loss_dict, id_logs, w_inversion = self.perform_forward_on_batch(batch, train=True)\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # Logging related\n",
        "                if self.global_step % self.opts.image_interval == 0 or (self.global_step < 1000 and self.global_step % 25 == 0):\n",
        "                    self.parse_and_log_images(id_logs, x, y, y_hat, w_inversion, title='images/train')\n",
        "\n",
        "                if self.global_step % self.opts.board_interval == 0:\n",
        "                    self.print_metrics(loss_dict, prefix='train')\n",
        "                    self.log_metrics(loss_dict, prefix='train')\n",
        "\n",
        "                # Validation related\n",
        "                val_loss_dict = None\n",
        "                if self.global_step % self.opts.val_interval == 0 or self.global_step == self.opts.max_steps:\n",
        "                    val_loss_dict = self.validate()\n",
        "                    if val_loss_dict and (self.best_val_loss is None or val_loss_dict['loss'] < self.best_val_loss):\n",
        "                        self.best_val_loss = val_loss_dict['loss']\n",
        "                        self.checkpoint_me(val_loss_dict, is_best=True)\n",
        "\n",
        "                if self.global_step > 0 and (self.global_step % self.opts.save_interval == 0 or self.global_step == self.opts.max_steps):\n",
        "                    if val_loss_dict is not None:\n",
        "                        self.checkpoint_me(val_loss_dict, is_best=False)\n",
        "                    else:\n",
        "                        self.checkpoint_me(loss_dict, is_best=False)\n",
        "\n",
        "                if self.global_step == self.opts.max_steps:\n",
        "                    print('OMG, finished training!')\n",
        "                    break\n",
        "\n",
        "                self.global_step += 1\n",
        "\n",
        "    def validate(self):\n",
        "        self.net.eval()\n",
        "        agg_loss_dict = []\n",
        "        for batch_idx, batch in enumerate(self.test_dataloader):\n",
        "            if self.opts.max_val_batches is not None and batch_idx > self.opts.max_val_batches:\n",
        "                break\n",
        "            with torch.no_grad():\n",
        "                x, y, y_hat, cur_loss_dict, id_logs, w_inversion = self.perform_forward_on_batch(batch)\n",
        "            agg_loss_dict.append(cur_loss_dict)\n",
        "\n",
        "            # Logging related\n",
        "            self.parse_and_log_images(id_logs, x, y, y_hat, w_inversion,\n",
        "                                      title='images/test', subscript='{:04d}'.format(batch_idx))\n",
        "\n",
        "            # For first step just do sanity test on small amount of data\n",
        "            if self.global_step == 0 and batch_idx >= 4:\n",
        "                self.net.train()\n",
        "                return None  # Do not log, inaccurate in first batch\n",
        "\n",
        "        loss_dict = train_utils.aggregate_loss_dict(agg_loss_dict)\n",
        "        self.log_metrics(loss_dict, prefix='test')\n",
        "        self.print_metrics(loss_dict, prefix='test')\n",
        "\n",
        "        self.net.train()\n",
        "        return loss_dict\n",
        "\n",
        "    def checkpoint_me(self, loss_dict, is_best):\n",
        "        save_name = 'best_model.pt' if is_best else 'iteration_{}.pt'.format(self.global_step)\n",
        "        save_dict = self.__get_save_dict()\n",
        "        checkpoint_path = os.path.join(self.checkpoint_dir, save_name)\n",
        "        torch.save(save_dict, checkpoint_path)\n",
        "        with open(os.path.join(self.checkpoint_dir, 'timestamp.txt'), 'a') as f:\n",
        "            if is_best:\n",
        "                f.write('**Best**: Step - {}, Loss - {:.3f} \\n{}\\n'.format(self.global_step, self.best_val_loss, loss_dict))\n",
        "            else:\n",
        "                f.write('Step - {}, \\n{}\\n'.format(self.global_step, loss_dict))\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        params = list(self.net.hypernet.parameters())\n",
        "        if self.opts.train_decoder:\n",
        "            params += list(self.net.decoder.parameters())\n",
        "        if self.opts.optim_name == 'adam':\n",
        "            optimizer = torch.optim.Adam(params, lr=self.opts.learning_rate)\n",
        "        else:\n",
        "            optimizer = Ranger(params, lr=self.opts.learning_rate)\n",
        "        return optimizer\n",
        "\n",
        "    def configure_datasets(self):\n",
        "        if self.opts.dataset_type not in data_configs.DATASETS.keys():\n",
        "            raise Exception('{} is not a valid dataset_type'.format(self.opts.dataset_type))\n",
        "        print('Loading dataset for {}'.format(self.opts.dataset_type))\n",
        "        dataset_args = data_configs.DATASETS[self.opts.dataset_type]\n",
        "        transforms_dict = dataset_args['transforms'](self.opts).get_transforms()\n",
        "        train_dataset, test_dataset = DatasetFetcher().get_dataset(self.opts, dataset_args, transforms_dict)\n",
        "        print(\"Number of training samples: {}\".format(len(train_dataset)))\n",
        "        print(\"Number of test samples: {}\".format(len(test_dataset)))\n",
        "        return train_dataset, test_dataset\n",
        "\n",
        "    def calc_loss(self, x, y, y_hat, latent, weights_deltas):\n",
        "        loss_dict = {}\n",
        "        loss = 0.0\n",
        "        id_logs = None\n",
        "\n",
        "        if \"cars\" in self.opts.dataset_type:\n",
        "            y_hat_resized = torch.nn.AdaptiveAvgPool2d((192, 256))(y_hat)\n",
        "            y_resized = torch.nn.AdaptiveAvgPool2d((192, 256))(y)\n",
        "            x_resized = torch.nn.AdaptiveAvgPool2d((192, 256))(x)\n",
        "        else:\n",
        "            y_hat_resized = self.net.face_pool(y_hat)\n",
        "            y_resized = self.net.face_pool(y)\n",
        "            x_resized = self.net.face_pool(x)\n",
        "\n",
        "        if self.opts.id_lambda > 0:\n",
        "            loss_id, sim_improvement, id_logs = self.id_loss(y_hat_resized, y_resized, x_resized)\n",
        "            loss_dict['loss_id'] = float(loss_id)\n",
        "            loss_dict['id_improve'] = float(sim_improvement)\n",
        "            loss = loss_id * self.opts.id_lambda\n",
        "        if self.opts.l2_lambda > 0:\n",
        "            loss_l2 = F.mse_loss(y_hat, y)\n",
        "            loss_dict['loss_l2'] = float(loss_l2)\n",
        "            loss += loss_l2 * self.opts.l2_lambda\n",
        "        if self.opts.lpips_lambda > 0:\n",
        "            loss_lpips = self.lpips_loss(y_hat_resized, y_resized)\n",
        "            loss_dict['loss_lpips'] = float(loss_lpips)\n",
        "            loss += loss_lpips * self.opts.lpips_lambda\n",
        "        if self.opts.moco_lambda > 0:\n",
        "            loss_moco, sim_improvement, id_logs = self.moco_loss(y_hat_resized, y_resized, x_resized)\n",
        "            loss_dict['loss_moco'] = float(loss_moco)\n",
        "            loss_dict['id_improve'] = float(sim_improvement)\n",
        "            loss += loss_moco * self.opts.moco_lambda\n",
        "\n",
        "        loss_dict['loss'] = float(loss)\n",
        "        return loss, loss_dict, id_logs\n",
        "\n",
        "    def log_metrics(self, metrics_dict, prefix):\n",
        "        for key, value in metrics_dict.items():\n",
        "            self.logger.add_scalar('{}/{}'.format(prefix, key), value, self.global_step)\n",
        "\n",
        "    def print_metrics(self, metrics_dict, prefix):\n",
        "        print('Metrics for {}, step {}'.format(prefix, self.global_step))\n",
        "        for key, value in metrics_dict.items():\n",
        "            print('\\t{} = '.format(key), value)\n",
        "\n",
        "    def parse_and_log_images(self, id_logs, x, y, y_hat, w_inversion, title, subscript=None, display_count=2):\n",
        "        im_data = []\n",
        "        display_count = min(x.shape[0], display_count)\n",
        "        for i in range(display_count):\n",
        "            if type(y_hat) == dict:\n",
        "                output_face = [\n",
        "                    [common.tensor2im(y_hat[i][iter_idx][0]), y_hat[i][iter_idx][1]]\n",
        "                    for iter_idx in range(len(y_hat[i]))\n",
        "                ]\n",
        "            else:\n",
        "                output_face = [common.tensor2im(y_hat[i])]\n",
        "            cur_im_data = {\n",
        "                'input_face': common.tensor2im(x[i]),\n",
        "                'target_face': common.tensor2im(y[i]),\n",
        "                'output_face': output_face,\n",
        "                'w_inversion': common.tensor2im(w_inversion[i])\n",
        "            }\n",
        "            if id_logs is not None:\n",
        "                for key in id_logs[i]:\n",
        "                    cur_im_data[key] = id_logs[i][key]\n",
        "            im_data.append(cur_im_data)\n",
        "        self.log_images(title, im_data=im_data, subscript=subscript)\n",
        "\n",
        "    def log_images(self, name, im_data, subscript=None, log_latest=False):\n",
        "        fig = common.vis_faces(im_data)\n",
        "        step = self.global_step\n",
        "        if log_latest:\n",
        "            step = 0\n",
        "        if subscript:\n",
        "            path = os.path.join(self.logger.log_dir, name, '{}_{:04d}.jpg'.format(subscript, step))\n",
        "        else:\n",
        "            path = os.path.join(self.logger.log_dir, name, '{:04d}.jpg'.format(step))\n",
        "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "        fig.savefig(path)\n",
        "        plt.close(fig)\n",
        "\n",
        "    def __get_save_dict(self):\n",
        "        save_dict = {\n",
        "            'state_dict': {k: v for k, v in self.net.state_dict().items() if 'w_encoder' not in k},\n",
        "            'opts': vars(self.opts),\n",
        "            'latent_avg': self.net.latent_avg,\n",
        "            'global_step': self.global_step,\n",
        "            'best_val_loss': self.best_val_loss\n",
        "        }\n",
        "        return save_dict\n",
        "\n",
        "    def __load_train_checkpoint(self, checkpoint):\n",
        "        print('Loading previous training data...')\n",
        "        self.global_step = checkpoint['global_step'] + 1\n",
        "        self.best_val_loss = checkpoint['best_val_loss']\n",
        "        # self.net.load_state_dict(checkpoint['state_dict'])\n",
        "        print(f'Resuming training from step: {self.global_step}')\n",
        "        print(f'Current best validation loss: {self.best_val_loss}')\n"
      ],
      "metadata": {
        "id": "SWcNn12d7BXS"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utils.data_utils.py\n",
        "\"\"\"\n",
        "Code adopted from pix2pixHD:\n",
        "https://github.com/NVIDIA/pix2pixHD/blob/master/data/image_folder.py\n",
        "\"\"\"\n",
        "import os\n",
        "\n",
        "IMG_EXTENSIONS = [\n",
        "    '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
        "    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP', '.tiff'\n",
        "]\n",
        "\n",
        "\n",
        "def is_image_file(filename):\n",
        "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
        "\n",
        "\n",
        "def make_dataset(dir):\n",
        "    images = []\n",
        "    assert os.path.isdir(dir), '%s is not a valid directory' % dir\n",
        "    for fname in os.listdir(dir):\n",
        "        if is_image_file(fname):\n",
        "            path = os.path.join(dir, fname)\n",
        "            images.append(path)\n",
        "    return images"
      ],
      "metadata": {
        "id": "3tfCr5VABfJj"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.dataset_fetcher.py\n",
        "# from datasets.images_dataset import ImagesDataset\n",
        "# from datasets.latents_images_dataset import LatentsImagesDataset\n",
        "\n",
        "\n",
        "class DatasetFetcher:\n",
        "\n",
        "    def get_dataset(self, opts, dataset_args, transforms_dict):\n",
        "        if opts.dataset_type in ['ffhq_hypernet_pre_extract']:\n",
        "            return self.__get_latents_dataset(opts, dataset_args, transforms_dict)\n",
        "        else:\n",
        "            return self.__get_images_dataset(opts, dataset_args, transforms_dict)\n",
        "\n",
        "    @staticmethod\n",
        "    def __get_latents_dataset(opts, dataset_args, transforms_dict):\n",
        "        train_dataset = LatentsImagesDataset(source_root=dataset_args['train_source_root'],\n",
        "                                             target_root=dataset_args['train_target_root'],\n",
        "                                             latents_path=dataset_args['train_latents_path'],\n",
        "                                             source_transform=transforms_dict['transform_source'],\n",
        "                                             target_transform=transforms_dict['transform_gt_train'],\n",
        "                                             opts=opts)\n",
        "        test_dataset = LatentsImagesDataset(source_root=dataset_args['test_source_root'],\n",
        "                                            target_root=dataset_args['test_target_root'],\n",
        "                                            latents_path=dataset_args['test_latents_path'],\n",
        "                                            source_transform=transforms_dict['transform_source'],\n",
        "                                            target_transform=transforms_dict['transform_test'],\n",
        "                                            opts=opts)\n",
        "        return train_dataset, test_dataset\n",
        "\n",
        "    @staticmethod\n",
        "    def __get_images_dataset(opts, dataset_args, transforms_dict):\n",
        "        train_dataset = ImagesDataset(source_root=dataset_args['train_source_root'],\n",
        "                                      target_root=dataset_args['train_target_root'],\n",
        "                                      source_transform=transforms_dict['transform_source'],\n",
        "                                      target_transform=transforms_dict['transform_gt_train'],\n",
        "                                      opts=opts)\n",
        "        test_dataset = ImagesDataset(source_root=dataset_args['test_source_root'],\n",
        "                                     target_root=dataset_args['test_target_root'],\n",
        "                                     source_transform=transforms_dict['transform_source'],\n",
        "                                     target_transform=transforms_dict['transform_test'],\n",
        "                                     opts=opts)\n",
        "        return train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "_jkuV34O-3iF"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models.encoders.e4e.py\n",
        "\"\"\"\n",
        "This file defines the core research contribution\n",
        "\"\"\"\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# from models.stylegan2.model import Generator\n",
        "# from configs.paths_config import model_paths\n",
        "# from models.encoders import restyle_e4e_encoders\n",
        "# from utils.resnet_mapping import RESNET_MAPPING\n",
        "\n",
        "\n",
        "class e4e(nn.Module):\n",
        "\n",
        "    def __init__(self, opts):\n",
        "        super(e4e, self).__init__()\n",
        "        self.set_opts(opts)\n",
        "        self.n_styles = int(math.log(self.opts.output_size, 2)) * 2 - 2\n",
        "        # Define architecture\n",
        "        self.encoder = self.set_encoder()\n",
        "        self.decoder = Generator(self.opts.output_size, 512, 8, channel_multiplier=2)\n",
        "        self.face_pool = torch.nn.AdaptiveAvgPool2d((256, 256))\n",
        "        # Load weights if needed\n",
        "        self.load_weights()\n",
        "\n",
        "    def set_encoder(self):\n",
        "        if self.opts.encoder_type == 'ProgressiveBackboneEncoder':\n",
        "            encoder = restyle_e4e_encoders.ProgressiveBackboneEncoder(50, 'ir_se', self.n_styles, self.opts)\n",
        "        elif self.opts.encoder_type == 'ResNetProgressiveBackboneEncoder':\n",
        "            encoder = restyle_e4e_encoders.ResNetProgressiveBackboneEncoder(self.n_styles, self.opts)\n",
        "        else:\n",
        "            raise Exception(f'{self.opts.encoder_type} is not a valid encoders')\n",
        "        return encoder\n",
        "\n",
        "    def load_weights(self):\n",
        "        if self.opts.checkpoint_path is not None:\n",
        "            print(f'Loading ReStyle e4e from checkpoint: {self.opts.checkpoint_path}')\n",
        "            ckpt = torch.load(self.opts.checkpoint_path, map_location='cpu')\n",
        "            self.encoder.load_state_dict(self.__get_keys(ckpt, 'encoder'), strict=True)\n",
        "            self.decoder.load_state_dict(self.__get_keys(ckpt, 'decoder'), strict=True)\n",
        "            self.__load_latent_avg(ckpt)\n",
        "        else:\n",
        "            encoder_ckpt = self.__get_encoder_checkpoint()\n",
        "            self.encoder.load_state_dict(encoder_ckpt, strict=False)\n",
        "            print(f'Loading decoder weights from pretrained path: {self.opts.stylegan_weights}')\n",
        "            ckpt = torch.load(self.opts.stylegan_weights)\n",
        "            self.decoder.load_state_dict(ckpt['g_ema'], strict=True)\n",
        "            self.__load_latent_avg(ckpt, repeat=self.n_styles)\n",
        "\n",
        "    def forward(self, x, latent=None, resize=True, input_code=False, randomize_noise=True,\n",
        "                return_latents=False, average_code=False, input_is_full=False):\n",
        "\n",
        "        if input_code:\n",
        "            codes = x\n",
        "        else:\n",
        "            codes = self.encoder(x)\n",
        "            # residual step\n",
        "            if x.shape[1] == 6 and latent is not None:\n",
        "                # learn error with respect to previous iteration\n",
        "                codes = codes + latent\n",
        "            else:\n",
        "                # first iteration is with respect to the avg latent code\n",
        "                codes = codes + self.latent_avg.repeat(codes.shape[0], 1, 1)\n",
        "\n",
        "        if average_code:\n",
        "            input_is_latent = True\n",
        "        else:\n",
        "            input_is_latent = (not input_code) or (input_is_full)\n",
        "\n",
        "        images, result_latent = self.decoder([codes],\n",
        "                                             input_is_latent=input_is_latent,\n",
        "                                             randomize_noise=randomize_noise,\n",
        "                                             return_latents=return_latents)\n",
        "\n",
        "        if resize:\n",
        "            images = self.face_pool(images)\n",
        "\n",
        "        if return_latents:\n",
        "            return images, result_latent\n",
        "        else:\n",
        "            return images\n",
        "\n",
        "    def set_opts(self, opts):\n",
        "        self.opts = opts\n",
        "\n",
        "    def __load_latent_avg(self, ckpt, repeat=None):\n",
        "        if 'latent_avg' in ckpt:\n",
        "            self.latent_avg = ckpt['latent_avg'].to(self.opts.device)\n",
        "            if repeat is not None:\n",
        "                self.latent_avg = self.latent_avg.repeat(repeat, 1)\n",
        "        else:\n",
        "            self.latent_avg = None\n",
        "\n",
        "    def __get_encoder_checkpoint(self):\n",
        "        if \"ffhq\" in self.opts.dataset_type:\n",
        "            print('Loading encoders weights from irse50!')\n",
        "            encoder_ckpt = torch.load(model_paths['ir_se50'])\n",
        "            # Transfer the RGB input of the irse50 network to the first 3 input channels of pSp's encoder\n",
        "            if self.opts.input_nc != 3:\n",
        "                shape = encoder_ckpt['input_layer.0.weight'].shape\n",
        "                altered_input_layer = torch.randn(shape[0], self.opts.input_nc, shape[2], shape[3], dtype=torch.float32)\n",
        "                altered_input_layer[:, :3, :, :] = encoder_ckpt['input_layer.0.weight']\n",
        "                encoder_ckpt['input_layer.0.weight'] = altered_input_layer\n",
        "            return encoder_ckpt\n",
        "        else:\n",
        "            print('Loading encoders weights from resnet34!')\n",
        "            encoder_ckpt = torch.load(model_paths['resnet34'])\n",
        "            # Transfer the RGB input of the resnet34 network to the first 3 input channels of pSp's encoder\n",
        "            if self.opts.input_nc != 3:\n",
        "                shape = encoder_ckpt['conv1.weight'].shape\n",
        "                altered_input_layer = torch.randn(shape[0], self.opts.input_nc, shape[2], shape[3], dtype=torch.float32)\n",
        "                altered_input_layer[:, :3, :, :] = encoder_ckpt['conv1.weight']\n",
        "                encoder_ckpt['conv1.weight'] = altered_input_layer\n",
        "            mapped_encoder_ckpt = dict(encoder_ckpt)\n",
        "            for p, v in encoder_ckpt.items():\n",
        "                for original_name, psp_name in RESNET_MAPPING.items():\n",
        "                    if original_name in p:\n",
        "                        mapped_encoder_ckpt[p.replace(original_name, psp_name)] = v\n",
        "                        mapped_encoder_ckpt.pop(p)\n",
        "            return encoder_ckpt\n",
        "\n",
        "    @staticmethod\n",
        "    def __get_keys(d, name):\n",
        "        if 'state_dict' in d:\n",
        "            d = d['state_dict']\n",
        "        d_filt = {k[len(name) + 1:]: v for k, v in d.items() if k[:len(name)] == name}\n",
        "        return d_filt"
      ],
      "metadata": {
        "id": "zozrooR4OSTc"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models.encoder.helpers.py\n",
        "from collections import namedtuple\n",
        "import torch\n",
        "from torch.nn import Conv2d, BatchNorm2d, PReLU, ReLU, Sigmoid, MaxPool2d, AdaptiveAvgPool2d, Sequential, Module, Linear\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\"\"\"\n",
        "ArcFace implementation from [TreB1eN](https://github.com/TreB1eN/InsightFace_Pytorch)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class Flatten(Module):\n",
        "\tdef forward(self, input):\n",
        "\t\treturn input.view(input.size(0), -1)\n",
        "\n",
        "\n",
        "def l2_norm(input, axis=1):\n",
        "\tnorm = torch.norm(input, 2, axis, True)\n",
        "\toutput = torch.div(input, norm)\n",
        "\treturn output\n",
        "\n",
        "\n",
        "class Bottleneck(namedtuple('Block', ['in_channel', 'depth', 'stride'])):\n",
        "\t\"\"\" A named tuple describing a ResNet block. \"\"\"\n",
        "\n",
        "\n",
        "def get_block(in_channel, depth, num_units, stride=2):\n",
        "\treturn [Bottleneck(in_channel, depth, stride)] + [Bottleneck(depth, depth, 1) for i in range(num_units - 1)]\n",
        "\n",
        "\n",
        "def get_blocks(num_layers):\n",
        "\tif num_layers == 50:\n",
        "\t\tblocks = [\n",
        "\t\t\tget_block(in_channel=64, depth=64, num_units=3),\n",
        "\t\t\tget_block(in_channel=64, depth=128, num_units=4),\n",
        "\t\t\tget_block(in_channel=128, depth=256, num_units=14),\n",
        "\t\t\tget_block(in_channel=256, depth=512, num_units=3)\n",
        "\t\t]\n",
        "\telif num_layers == 100:\n",
        "\t\tblocks = [\n",
        "\t\t\tget_block(in_channel=64, depth=64, num_units=3),\n",
        "\t\t\tget_block(in_channel=64, depth=128, num_units=13),\n",
        "\t\t\tget_block(in_channel=128, depth=256, num_units=30),\n",
        "\t\t\tget_block(in_channel=256, depth=512, num_units=3)\n",
        "\t\t]\n",
        "\telif num_layers == 152:\n",
        "\t\tblocks = [\n",
        "\t\t\tget_block(in_channel=64, depth=64, num_units=3),\n",
        "\t\t\tget_block(in_channel=64, depth=128, num_units=8),\n",
        "\t\t\tget_block(in_channel=128, depth=256, num_units=36),\n",
        "\t\t\tget_block(in_channel=256, depth=512, num_units=3)\n",
        "\t\t]\n",
        "\telse:\n",
        "\t\traise ValueError(\"Invalid number of layers: {}. Must be one of [50, 100, 152]\".format(num_layers))\n",
        "\treturn blocks\n",
        "\n",
        "\n",
        "class SEModule(Module):\n",
        "\tdef __init__(self, channels, reduction):\n",
        "\t\tsuper(SEModule, self).__init__()\n",
        "\t\tself.avg_pool = AdaptiveAvgPool2d(1)\n",
        "\t\tself.fc1 = Conv2d(channels, channels // reduction, kernel_size=1, padding=0, bias=False)\n",
        "\t\tself.relu = ReLU(inplace=True)\n",
        "\t\tself.fc2 = Conv2d(channels // reduction, channels, kernel_size=1, padding=0, bias=False)\n",
        "\t\tself.sigmoid = Sigmoid()\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tmodule_input = x\n",
        "\t\tx = self.avg_pool(x)\n",
        "\t\tx = self.fc1(x)\n",
        "\t\tx = self.relu(x)\n",
        "\t\tx = self.fc2(x)\n",
        "\t\tx = self.sigmoid(x)\n",
        "\t\treturn module_input * x\n",
        "\n",
        "\n",
        "class bottleneck_IR(Module):\n",
        "\tdef __init__(self, in_channel, depth, stride):\n",
        "\t\tsuper(bottleneck_IR, self).__init__()\n",
        "\t\tif in_channel == depth:\n",
        "\t\t\tself.shortcut_layer = MaxPool2d(1, stride)\n",
        "\t\telse:\n",
        "\t\t\tself.shortcut_layer = Sequential(\n",
        "\t\t\t\tConv2d(in_channel, depth, (1, 1), stride, bias=False),\n",
        "\t\t\t\tBatchNorm2d(depth)\n",
        "\t\t\t)\n",
        "\t\tself.res_layer = Sequential(\n",
        "\t\t\tBatchNorm2d(in_channel),\n",
        "\t\t\tConv2d(in_channel, depth, (3, 3), (1, 1), 1, bias=False), PReLU(depth),\n",
        "\t\t\tConv2d(depth, depth, (3, 3), stride, 1, bias=False), BatchNorm2d(depth)\n",
        "\t\t)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tshortcut = self.shortcut_layer(x)\n",
        "\t\tres = self.res_layer(x)\n",
        "\t\treturn res + shortcut\n",
        "\n",
        "\n",
        "class bottleneck_IR_SE(Module):\n",
        "\tdef __init__(self, in_channel, depth, stride):\n",
        "\t\tsuper(bottleneck_IR_SE, self).__init__()\n",
        "\t\tif in_channel == depth:\n",
        "\t\t\tself.shortcut_layer = MaxPool2d(1, stride)\n",
        "\t\telse:\n",
        "\t\t\tself.shortcut_layer = Sequential(\n",
        "\t\t\t\tConv2d(in_channel, depth, (1, 1), stride, bias=False),\n",
        "\t\t\t\tBatchNorm2d(depth)\n",
        "\t\t\t)\n",
        "\t\tself.res_layer = Sequential(\n",
        "\t\t\tBatchNorm2d(in_channel),\n",
        "\t\t\tConv2d(in_channel, depth, (3, 3), (1, 1), 1, bias=False),\n",
        "\t\t\tPReLU(depth),\n",
        "\t\t\tConv2d(depth, depth, (3, 3), stride, 1, bias=False),\n",
        "\t\t\tBatchNorm2d(depth),\n",
        "\t\t\tSEModule(depth, 16)\n",
        "\t\t)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tshortcut = self.shortcut_layer(x)\n",
        "\t\tres = self.res_layer(x)\n",
        "\t\treturn res + shortcut\n",
        "\n",
        "\n",
        "class SeparableConv2d(torch.nn.Module):\n",
        "\n",
        "\tdef __init__(self, in_channels, out_channels, kernel_size, bias=False):\n",
        "\t\tsuper(SeparableConv2d, self).__init__()\n",
        "\t\tself.depthwise = Conv2d(in_channels, in_channels, kernel_size=kernel_size, groups=in_channels, bias=bias, padding=1)\n",
        "\t\tself.pointwise = Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tout = self.depthwise(x)\n",
        "\t\tout = self.pointwise(out)\n",
        "\t\treturn out\n",
        "\n",
        "\n",
        "def _upsample_add(x, y):\n",
        "\t\"\"\"Upsample and add two feature maps.\n",
        "\tArgs:\n",
        "\t  x: (Variable) top feature map to be upsampled.\n",
        "\t  y: (Variable) lateral feature map.\n",
        "\tReturns:\n",
        "\t  (Variable) added feature map.\n",
        "\tNote in PyTorch, when input size is odd, the upsampled feature map\n",
        "\twith `F.upsample(..., scale_factor=2, mode='nearest')`\n",
        "\tmaybe not equal to the lateral feature map size.\n",
        "\te.g.\n",
        "\toriginal input size: [N,_,15,15] ->\n",
        "\tconv2d feature map size: [N,_,8,8] ->\n",
        "\tupsampled feature map size: [N,_,16,16]\n",
        "\tSo we choose bilinear upsample which supports arbitrary output sizes.\n",
        "\t\"\"\"\n",
        "\t_, _, H, W = y.size()\n",
        "\treturn F.interpolate(x, size=(H, W), mode='bilinear', align_corners=True) + y\n",
        "\n",
        "\n",
        "class SeparableBlock(Module):\n",
        "\n",
        "\tdef __init__(self, input_size, kernel_channels_in, kernel_channels_out, kernel_size):\n",
        "\t\tsuper(SeparableBlock, self).__init__()\n",
        "\n",
        "\t\tself.input_size = input_size\n",
        "\t\tself.kernel_size = kernel_size\n",
        "\t\tself.kernel_channels_in = kernel_channels_in\n",
        "\t\tself.kernel_channels_out = kernel_channels_out\n",
        "\n",
        "\t\tself.make_kernel_in  = Linear(input_size, kernel_size * kernel_size * kernel_channels_in)\n",
        "\t\tself.make_kernel_out = Linear(input_size, kernel_size * kernel_size * kernel_channels_out)\n",
        "\n",
        "\t\tself.kernel_linear_in = Linear(kernel_channels_in, kernel_channels_in)\n",
        "\t\tself.kernel_linear_out = Linear(kernel_channels_out, kernel_channels_out)\n",
        "\n",
        "\tdef forward(self, features):\n",
        "\n",
        "\t\tfeatures = features.view(-1, self.input_size)\n",
        "\n",
        "\t\tkernel_in = self.make_kernel_in(features).view(-1, self.kernel_size, self.kernel_size, 1, self.kernel_channels_in)\n",
        "\t\tkernel_out = self.make_kernel_out(features).view(-1, self.kernel_size, self.kernel_size, self.kernel_channels_out, 1)\n",
        "\n",
        "\t\tkernel = torch.matmul(kernel_out, kernel_in)\n",
        "\n",
        "\t\tkernel = self.kernel_linear_in(kernel).permute(0, 1, 2, 4, 3)\n",
        "\t\tkernel = self.kernel_linear_out(kernel)\n",
        "\t\tkernel = kernel.permute(0, 4, 3, 1, 2)\n",
        "\n",
        "\t\treturn kernel"
      ],
      "metadata": {
        "id": "JCeYr2e7O3_C"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models.encoders.model_ires.py\n",
        "from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, Dropout, Sequential, Module\n",
        "# from models.encoders.helpers import get_blocks, Flatten, bottleneck_IR, bottleneck_IR_SE, l2_norm\n",
        "\n",
        "\"\"\"\n",
        "Modified Backbone implementation from [TreB1eN](https://github.com/TreB1eN/InsightFace_Pytorch)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class Backbone(Module):\n",
        "\tdef __init__(self, input_size, num_layers, mode='ir', drop_ratio=0.4, affine=True):\n",
        "\t\tsuper(Backbone, self).__init__()\n",
        "\t\tassert input_size in [112, 224], \"input_size should be 112 or 224\"\n",
        "\t\tassert num_layers in [50, 100, 152], \"num_layers should be 50, 100 or 152\"\n",
        "\t\tassert mode in ['ir', 'ir_se'], \"mode should be ir or ir_se\"\n",
        "\t\tblocks = get_blocks(num_layers)\n",
        "\t\tif mode == 'ir':\n",
        "\t\t\tunit_module = bottleneck_IR\n",
        "\t\telif mode == 'ir_se':\n",
        "\t\t\tunit_module = bottleneck_IR_SE\n",
        "\t\tself.input_layer = Sequential(Conv2d(3, 64, (3, 3), 1, 1, bias=False),\n",
        "\t\t\t\t\t\t\t\t\t  BatchNorm2d(64),\n",
        "\t\t\t\t\t\t\t\t\t  PReLU(64))\n",
        "\t\tif input_size == 112:\n",
        "\t\t\tself.output_layer = Sequential(BatchNorm2d(512),\n",
        "\t\t\t                               Dropout(drop_ratio),\n",
        "\t\t\t                               Flatten(),\n",
        "\t\t\t                               Linear(512 * 7 * 7, 512),\n",
        "\t\t\t                               BatchNorm1d(512, affine=affine))\n",
        "\t\telse:\n",
        "\t\t\tself.output_layer = Sequential(BatchNorm2d(512),\n",
        "\t\t\t                               Dropout(drop_ratio),\n",
        "\t\t\t                               Flatten(),\n",
        "\t\t\t                               Linear(512 * 14 * 14, 512),\n",
        "\t\t\t                               BatchNorm1d(512, affine=affine))\n",
        "\n",
        "\t\tmodules = []\n",
        "\t\tfor block in blocks:\n",
        "\t\t\tfor bottleneck in block:\n",
        "\t\t\t\tmodules.append(unit_module(bottleneck.in_channel,\n",
        "\t\t\t\t\t\t\t\t\t\t   bottleneck.depth,\n",
        "\t\t\t\t\t\t\t\t\t\t   bottleneck.stride))\n",
        "\t\tself.body = Sequential(*modules)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tx = self.input_layer(x)\n",
        "\t\tx = self.body(x)\n",
        "\t\tx = self.output_layer(x)\n",
        "\t\treturn l2_norm(x)\n",
        "\n",
        "\n",
        "def IR_50(input_size):\n",
        "\t\"\"\"Constructs a ir-50 model.\"\"\"\n",
        "\tmodel = Backbone(input_size, num_layers=50, mode='ir', drop_ratio=0.4, affine=False)\n",
        "\treturn model\n",
        "\n",
        "\n",
        "def IR_101(input_size):\n",
        "\t\"\"\"Constructs a ir-101 model.\"\"\"\n",
        "\tmodel = Backbone(input_size, num_layers=100, mode='ir', drop_ratio=0.4, affine=False)\n",
        "\treturn model\n",
        "\n",
        "\n",
        "def IR_152(input_size):\n",
        "\t\"\"\"Constructs a ir-152 model.\"\"\"\n",
        "\tmodel = Backbone(input_size, num_layers=152, mode='ir', drop_ratio=0.4, affine=False)\n",
        "\treturn model\n",
        "\n",
        "\n",
        "def IR_SE_50(input_size):\n",
        "\t\"\"\"Constructs a ir_se-50 model.\"\"\"\n",
        "\tmodel = Backbone(input_size, num_layers=50, mode='ir_se', drop_ratio=0.4, affine=False)\n",
        "\treturn model\n",
        "\n",
        "\n",
        "def IR_SE_101(input_size):\n",
        "\t\"\"\"Constructs a ir_se-101 model.\"\"\"\n",
        "\tmodel = Backbone(input_size, num_layers=100, mode='ir_se', drop_ratio=0.4, affine=False)\n",
        "\treturn model\n",
        "\n",
        "\n",
        "def IR_SE_152(input_size):\n",
        "\t\"\"\"Constructs a ir_se-152 model.\"\"\"\n",
        "\tmodel = Backbone(input_size, num_layers=152, mode='ir_se', drop_ratio=0.4, affine=False)\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "Pbxv1Dy7Oc4N"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models.encoders.psp.py\n",
        "\"\"\"\n",
        "This file defines the core research contribution\n",
        "\"\"\"\n",
        "import math\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import torch\n",
        "from torch import nn\n",
        "# from models.encoders import w_encoder\n",
        "# from models.stylegan2.model import Generator\n",
        "# from configs.paths_config import model_paths\n",
        "\n",
        "\n",
        "def get_keys(d, name):\n",
        "\tif 'state_dict' in d:\n",
        "\t\td = d['state_dict']\n",
        "\td_filt = {k[len(name) + 1:]: v for k, v in d.items() if k[:len(name)] == name}\n",
        "\treturn d_filt\n",
        "\n",
        "\n",
        "class pSp(nn.Module):\n",
        "\n",
        "\tdef __init__(self, opts):\n",
        "\t\tsuper(pSp, self).__init__()\n",
        "\t\tself.set_opts(opts)\n",
        "\t\t# compute number of style inputs based on the output resolution\n",
        "\t\tself.opts.n_styles = int(math.log(self.opts.output_size, 2)) * 2 - 2\n",
        "\t\t# Define architecture\n",
        "\t\tself.encoder = self.set_encoder()\n",
        "\t\tself.decoder = Generator(self.opts.output_size, 512, 8)\n",
        "\t\tself.face_pool = torch.nn.AdaptiveAvgPool2d((256, 256))\n",
        "\t\t# Load weights if needed\n",
        "\t\tself.load_weights()\n",
        "\n",
        "\tdef set_encoder(self):\n",
        "\t\tif self.opts.encoder_type == 'WEncoder':\n",
        "\t\t\tencoder = w_encoder.WEncoder(50, 'ir_se', self.opts)\n",
        "\t\telse:\n",
        "\t\t\traise Exception('{} is not a valid encoders'.format(self.opts.encoder_type))\n",
        "\t\treturn encoder\n",
        "\n",
        "\tdef load_weights(self):\n",
        "\t\tif self.opts.checkpoint_path is not None:\n",
        "\t\t\tprint('Loading WEncoder from checkpoint: {}'.format(self.opts.checkpoint_path))\n",
        "\t\t\tckpt = torch.load(self.opts.checkpoint_path, map_location='cpu')\n",
        "\t\t\tself.encoder.load_state_dict(get_keys(ckpt, 'encoder'), strict=True)\n",
        "\t\t\tself.decoder.load_state_dict(get_keys(ckpt, 'decoder'), strict=True)\n",
        "\t\t\tself.__load_latent_avg(ckpt)\n",
        "\t\telse:\n",
        "\t\t\tprint('Loading encoders weights from irse50!')\n",
        "\t\t\tencoder_ckpt = torch.load(model_paths['ir_se50'])\n",
        "\t\t\t# if input to encoder is not an RGB image, do not load the input layer weights\n",
        "\t\t\tif self.opts.label_nc != 0:\n",
        "\t\t\t\tencoder_ckpt = {k: v for k, v in encoder_ckpt.items() if \"input_layer\" not in k}\n",
        "\t\t\tself.encoder.load_state_dict(encoder_ckpt, strict=False)\n",
        "\t\t\tprint('Loading decoder weights from pretrained!')\n",
        "\t\t\tckpt = torch.load(self.opts.stylegan_weights)\n",
        "\t\t\tself.decoder.load_state_dict(ckpt['g_ema'], strict=False)\n",
        "\t\t\tif self.opts.learn_in_w:\n",
        "\t\t\t\tself.__load_latent_avg(ckpt, repeat=1)\n",
        "\t\t\telse:\n",
        "\t\t\t\tself.__load_latent_avg(ckpt, repeat=self.opts.n_styles)\n",
        "\n",
        "\tdef forward(self, x, resize=True, latent_mask=None, input_code=False, randomize_noise=True, inject_latent=None,\n",
        "\t\t\t\treturn_latents=False, alpha=None):\n",
        "\n",
        "\t\tif input_code:\n",
        "\t\t\tcodes = x\n",
        "\t\telse:\n",
        "\t\t\tcodes = self.encoder(x)\n",
        "\t\t\tif codes.ndim == 2:\n",
        "\t\t\t\tcodes = codes + self.latent_avg.repeat(codes.shape[0], 1, 1)[:, 0, :]\n",
        "\t\t\telse:\n",
        "\t\t\t\tcodes = codes + self.latent_avg.repeat(codes.shape[0], 1, 1)\n",
        "\n",
        "\t\tif latent_mask is not None:\n",
        "\t\t\tfor i in latent_mask:\n",
        "\t\t\t\tif inject_latent is not None:\n",
        "\t\t\t\t\tif alpha is not None:\n",
        "\t\t\t\t\t\tcodes[:, i] = alpha * inject_latent[:, i] + (1 - alpha) * codes[:, i]\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tcodes[:, i] = inject_latent[:, i]\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tcodes[:, i] = 0\n",
        "\n",
        "\t\tinput_is_latent = not input_code\n",
        "\t\timages, result_latent = self.decoder([codes],\n",
        "\t\t                                     input_is_latent=input_is_latent,\n",
        "\t\t                                     randomize_noise=randomize_noise,\n",
        "\t\t                                     return_latents=return_latents)\n",
        "\n",
        "\t\tif resize:\n",
        "\t\t\timages = self.face_pool(images)\n",
        "\n",
        "\t\tif return_latents:\n",
        "\t\t\treturn images, result_latent\n",
        "\t\telse:\n",
        "\t\t\treturn images\n",
        "\n",
        "\tdef set_opts(self, opts):\n",
        "\t\tself.opts = opts\n",
        "\n",
        "\tdef __load_latent_avg(self, ckpt, repeat=None):\n",
        "\t\tif 'latent_avg' in ckpt:\n",
        "\t\t\tself.latent_avg = ckpt['latent_avg'].to(self.opts.device)\n",
        "\t\t\tif repeat is not None:\n",
        "\t\t\t\tself.latent_avg = self.latent_avg.repeat(repeat, 1)\n",
        "\t\telse:\n",
        "\t\t\tself.latent_avg = None"
      ],
      "metadata": {
        "id": "eZv73x20PEw_"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models.encoders.restyle_e4e_encoders.py\n",
        "\"\"\"\n",
        "This file defines the core research contribution\n",
        "\"\"\"\n",
        "import math\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import torch\n",
        "from torch import nn\n",
        "# from models.encoders import w_encoder\n",
        "# from models.stylegan2.model import Generator\n",
        "# from configs.paths_config import model_paths\n",
        "\n",
        "\n",
        "def get_keys(d, name):\n",
        "\tif 'state_dict' in d:\n",
        "\t\td = d['state_dict']\n",
        "\td_filt = {k[len(name) + 1:]: v for k, v in d.items() if k[:len(name)] == name}\n",
        "\treturn d_filt\n",
        "\n",
        "\n",
        "class pSp(nn.Module):\n",
        "\n",
        "\tdef __init__(self, opts):\n",
        "\t\tsuper(pSp, self).__init__()\n",
        "\t\tself.set_opts(opts)\n",
        "\t\t# compute number of style inputs based on the output resolution\n",
        "\t\tself.opts.n_styles = int(math.log(self.opts.output_size, 2)) * 2 - 2\n",
        "\t\t# Define architecture\n",
        "\t\tself.encoder = self.set_encoder()\n",
        "\t\tself.decoder = Generator(self.opts.output_size, 512, 8)\n",
        "\t\tself.face_pool = torch.nn.AdaptiveAvgPool2d((256, 256))\n",
        "\t\t# Load weights if needed\n",
        "\t\tself.load_weights()\n",
        "\n",
        "\tdef set_encoder(self):\n",
        "\t\tif self.opts.encoder_type == 'WEncoder':\n",
        "\t\t\tencoder = w_encoder.WEncoder(50, 'ir_se', self.opts)\n",
        "\t\telse:\n",
        "\t\t\traise Exception('{} is not a valid encoders'.format(self.opts.encoder_type))\n",
        "\t\treturn encoder\n",
        "\n",
        "\tdef load_weights(self):\n",
        "\t\tif self.opts.checkpoint_path is not None:\n",
        "\t\t\tprint('Loading WEncoder from checkpoint: {}'.format(self.opts.checkpoint_path))\n",
        "\t\t\tckpt = torch.load(self.opts.checkpoint_path, map_location='cpu')\n",
        "\t\t\tself.encoder.load_state_dict(get_keys(ckpt, 'encoder'), strict=True)\n",
        "\t\t\tself.decoder.load_state_dict(get_keys(ckpt, 'decoder'), strict=True)\n",
        "\t\t\tself.__load_latent_avg(ckpt)\n",
        "\t\telse:\n",
        "\t\t\tprint('Loading encoders weights from irse50!')\n",
        "\t\t\tencoder_ckpt = torch.load(model_paths['ir_se50'])\n",
        "\t\t\t# if input to encoder is not an RGB image, do not load the input layer weights\n",
        "\t\t\tif self.opts.label_nc != 0:\n",
        "\t\t\t\tencoder_ckpt = {k: v for k, v in encoder_ckpt.items() if \"input_layer\" not in k}\n",
        "\t\t\tself.encoder.load_state_dict(encoder_ckpt, strict=False)\n",
        "\t\t\tprint('Loading decoder weights from pretrained!')\n",
        "\t\t\tckpt = torch.load(self.opts.stylegan_weights)\n",
        "\t\t\tself.decoder.load_state_dict(ckpt['g_ema'], strict=False)\n",
        "\t\t\tif self.opts.learn_in_w:\n",
        "\t\t\t\tself.__load_latent_avg(ckpt, repeat=1)\n",
        "\t\t\telse:\n",
        "\t\t\t\tself.__load_latent_avg(ckpt, repeat=self.opts.n_styles)\n",
        "\n",
        "\tdef forward(self, x, resize=True, latent_mask=None, input_code=False, randomize_noise=True, inject_latent=None,\n",
        "\t\t\t\treturn_latents=False, alpha=None):\n",
        "\n",
        "\t\tif input_code:\n",
        "\t\t\tcodes = x\n",
        "\t\telse:\n",
        "\t\t\tcodes = self.encoder(x)\n",
        "\t\t\tif codes.ndim == 2:\n",
        "\t\t\t\tcodes = codes + self.latent_avg.repeat(codes.shape[0], 1, 1)[:, 0, :]\n",
        "\t\t\telse:\n",
        "\t\t\t\tcodes = codes + self.latent_avg.repeat(codes.shape[0], 1, 1)\n",
        "\n",
        "\t\tif latent_mask is not None:\n",
        "\t\t\tfor i in latent_mask:\n",
        "\t\t\t\tif inject_latent is not None:\n",
        "\t\t\t\t\tif alpha is not None:\n",
        "\t\t\t\t\t\tcodes[:, i] = alpha * inject_latent[:, i] + (1 - alpha) * codes[:, i]\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tcodes[:, i] = inject_latent[:, i]\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tcodes[:, i] = 0\n",
        "\n",
        "\t\tinput_is_latent = not input_code\n",
        "\t\timages, result_latent = self.decoder([codes],\n",
        "\t\t                                     input_is_latent=input_is_latent,\n",
        "\t\t                                     randomize_noise=randomize_noise,\n",
        "\t\t                                     return_latents=return_latents)\n",
        "\n",
        "\t\tif resize:\n",
        "\t\t\timages = self.face_pool(images)\n",
        "\n",
        "\t\tif return_latents:\n",
        "\t\t\treturn images, result_latent\n",
        "\t\telse:\n",
        "\t\t\treturn images\n",
        "\n",
        "\tdef set_opts(self, opts):\n",
        "\t\tself.opts = opts\n",
        "\n",
        "\tdef __load_latent_avg(self, ckpt, repeat=None):\n",
        "\t\tif 'latent_avg' in ckpt:\n",
        "\t\t\tself.latent_avg = ckpt['latent_avg'].to(self.opts.device)\n",
        "\t\t\tif repeat is not None:\n",
        "\t\t\t\tself.latent_avg = self.latent_avg.repeat(repeat, 1)\n",
        "\t\telse:\n",
        "\t\t\tself.latent_avg = None"
      ],
      "metadata": {
        "id": "Rl6qW9BTPNCF"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models.encoders.w_encoders.py\n",
        "import math\n",
        "import torch\n",
        "from torch.nn import Conv2d, BatchNorm2d, PReLU, Sequential, Module\n",
        "\n",
        "# from models.encoders.helpers import get_blocks, bottleneck_IR, bottleneck_IR_SE\n",
        "# from models.stylegan2.model import EqualLinear\n",
        "\n",
        "\n",
        "class WEncoder(Module):\n",
        "    def __init__(self, num_layers, mode='ir', opts=None):\n",
        "        super(WEncoder, self).__init__()\n",
        "        print('Using WEncoder')\n",
        "        assert num_layers in [50, 100, 152], 'num_layers should be 50,100, or 152'\n",
        "        assert mode in ['ir', 'ir_se'], 'mode should be ir or ir_se'\n",
        "        blocks = get_blocks(num_layers)\n",
        "        if mode == 'ir':\n",
        "            unit_module = bottleneck_IR\n",
        "        elif mode == 'ir_se':\n",
        "            unit_module = bottleneck_IR_SE\n",
        "        self.input_layer = Sequential(Conv2d(3, 64, (3, 3), 1, 1, bias=False),\n",
        "                                      BatchNorm2d(64),\n",
        "                                      PReLU(64))\n",
        "        self.output_pool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.linear = EqualLinear(512, 512, lr_mul=1)\n",
        "        modules = []\n",
        "        for block in blocks:\n",
        "            for bottleneck in block:\n",
        "                modules.append(unit_module(bottleneck.in_channel,\n",
        "                                           bottleneck.depth,\n",
        "                                           bottleneck.stride))\n",
        "        self.body = Sequential(*modules)\n",
        "        log_size = int(math.log(opts.output_size, 2))\n",
        "        self.style_count = 2 * log_size - 2\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_layer(x)\n",
        "        x = self.body(x)\n",
        "        x = self.output_pool(x)\n",
        "        x = x.view(-1, 512)\n",
        "        x = self.linear(x)\n",
        "        return x.repeat(self.style_count, 1, 1).permute(1, 0, 2)\n"
      ],
      "metadata": {
        "id": "0_dAoOtlPUqY"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models.hypernetworks.shared_weights_hypernet.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "class SharedWeightsHypernet(nn.Module):\n",
        "\n",
        "    def __init__(self, f_size=3, z_dim=512, out_size=512, in_size=512, mode=None):\n",
        "        super(SharedWeightsHypernet, self).__init__()\n",
        "        self.mode = mode\n",
        "        self.z_dim = z_dim\n",
        "        self.f_size = f_size\n",
        "        if self.mode == 'delta_per_channel':\n",
        "            self.f_size = 1\n",
        "        self.out_size = out_size\n",
        "        self.in_size = in_size\n",
        "\n",
        "        self.w1 = Parameter(torch.fmod(torch.randn((self.z_dim, self.out_size * self.f_size * self.f_size)).cuda() / 40, 2))\n",
        "        self.b1 = Parameter(torch.fmod(torch.randn((self.out_size * self.f_size * self.f_size)).cuda() / 40, 2))\n",
        "\n",
        "        self.w2 = Parameter(torch.fmod(torch.randn((self.z_dim, self.in_size * self.z_dim)).cuda() / 40, 2))\n",
        "        self.b2 = Parameter(torch.fmod(torch.randn((self.in_size * self.z_dim)).cuda() / 40, 2))\n",
        "\n",
        "    def forward(self, z):\n",
        "        batch_size = z.shape[0]\n",
        "        h_in = torch.matmul(z, self.w2) + self.b2\n",
        "        h_in = h_in.view(batch_size, self.in_size, self.z_dim)\n",
        "\n",
        "        h_final = torch.matmul(h_in, self.w1) + self.b1\n",
        "        kernel = h_final.view(batch_size, self.out_size, self.in_size, self.f_size, self.f_size)\n",
        "        if self.mode == 'delta_per_channel':  # repeat per channel values to the 3x3 conv kernels\n",
        "            kernel = kernel.repeat(1, 1, 1, 3, 3)\n",
        "        return kernel"
      ],
      "metadata": {
        "id": "OXBYG0qCQKnE"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#models.hypernetworks.refinement_blocks.py\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch.nn import Conv2d, Sequential, Module\n",
        "\n",
        "# from models.encoders.helpers import SeparableBlock\n",
        "# from models.stylegan2.model import EqualLinear\n",
        "\n",
        "\n",
        "# layer_idx: [kernel_size, in_channels, out_channels]\n",
        "PARAMETERS = {\n",
        "    0: [3, 512, 512],\n",
        "    1: [1, 512, 3],\n",
        "    2: [3, 512, 512],\n",
        "    3: [3, 512, 512],\n",
        "    4: [1, 512, 3],\n",
        "    5: [3, 512, 512],\n",
        "    6: [3, 512, 512],\n",
        "    7: [1, 512, 3],\n",
        "    8: [3, 512, 512],\n",
        "    9: [3, 512, 512],\n",
        "    10: [1, 512, 3],\n",
        "    11: [3, 512, 512],\n",
        "    12: [3, 512, 512],\n",
        "    13: [1, 512, 3],\n",
        "    14: [3, 512, 256],\n",
        "    15: [3, 256, 256],\n",
        "    16: [1, 256, 3],\n",
        "    17: [3, 256, 128],\n",
        "    18: [3, 128, 128],\n",
        "    19: [1, 128, 3],\n",
        "    20: [3, 128, 64],\n",
        "    21: [3, 64, 64],\n",
        "    22: [1, 64, 3],\n",
        "    23: [3, 64, 32],\n",
        "    24: [3, 32, 32],\n",
        "    25: [1, 32, 3]\n",
        "}\n",
        "TO_RGB_LAYERS = [1, 4, 7, 10, 13, 16, 19, 22, 25]\n",
        "\n",
        "\n",
        "class RefinementBlock(Module):\n",
        "\n",
        "    def __init__(self, layer_idx, opts, n_channels=512, inner_c=256, spatial=16):\n",
        "        super(RefinementBlock, self).__init__()\n",
        "        self.layer_idx = layer_idx\n",
        "        self.opts = opts\n",
        "        self.kernel_size, self.in_channels, self.out_channels = PARAMETERS[self.layer_idx]\n",
        "        self.spatial = spatial\n",
        "        self.n_channels = n_channels\n",
        "        self.inner_c = inner_c\n",
        "        self.out_c = 512\n",
        "        num_pools = int(np.log2(self.spatial)) - 1\n",
        "        if self.kernel_size == 3:\n",
        "            num_pools = num_pools - 1\n",
        "        self.modules = []\n",
        "        self.modules += [Conv2d(self.n_channels, self.inner_c, kernel_size=3, stride=2, padding=1), nn.LeakyReLU()]\n",
        "        for i in range(num_pools - 1):\n",
        "            self.modules += [Conv2d(self.inner_c, self.inner_c, kernel_size=3, stride=2, padding=1), nn.LeakyReLU()]\n",
        "        self.modules += [Conv2d(self.inner_c, self.out_c, kernel_size=3, stride=2, padding=1), nn.LeakyReLU()]\n",
        "        self.convs = nn.Sequential(*self.modules)\n",
        "\n",
        "        if layer_idx in TO_RGB_LAYERS:\n",
        "            self.output = Sequential(\n",
        "                Conv2d(self.out_c, self.in_channels * self.out_channels, kernel_size=1, stride=1, padding=0))\n",
        "        else:\n",
        "            self.output = Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n",
        "                                     Conv2d(self.out_c, self.in_channels * self.out_channels, kernel_size=1, stride=1,\n",
        "                                            padding=0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convs(x)\n",
        "        x = self.output(x)\n",
        "        if self.layer_idx in TO_RGB_LAYERS:\n",
        "            x = x.view(-1, self.out_channels, self.in_channels, self.kernel_size, self.kernel_size)\n",
        "        else:\n",
        "            x = x.view(-1, self.out_channels, self.in_channels)\n",
        "            x = x.unsqueeze(3).repeat(1, 1, 1, self.kernel_size).unsqueeze(4).repeat(1, 1, 1, 1, self.kernel_size)\n",
        "        return x\n",
        "\n",
        "\n",
        "class HyperRefinementBlock(Module):\n",
        "    def __init__(self, hypernet, n_channels=512, inner_c=128, spatial=16):\n",
        "        super(HyperRefinementBlock, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.inner_c = inner_c\n",
        "        self.out_c = 512\n",
        "        num_pools = int(np.log2(spatial))\n",
        "        modules = [Conv2d(self.n_channels, self.inner_c, kernel_size=3, stride=1, padding=1), nn.LeakyReLU()]\n",
        "        for i in range(num_pools - 1):\n",
        "            modules += [Conv2d(self.inner_c, self.inner_c, kernel_size=3, stride=2, padding=1), nn.LeakyReLU()]\n",
        "        modules += [Conv2d(self.inner_c, self.out_c, kernel_size=3, stride=2, padding=1), nn.LeakyReLU()]\n",
        "        self.convs = nn.Sequential(*modules)\n",
        "        self.linear = EqualLinear(self.out_c, self.out_c, lr_mul=1)\n",
        "        self.hypernet = hypernet\n",
        "\n",
        "    def forward(self, features):\n",
        "        code = self.convs(features)\n",
        "        code = code.view(-1, self.out_c)\n",
        "        code = self.linear(code)\n",
        "        weight_delta = self.hypernet(code)\n",
        "        return weight_delta\n",
        "\n",
        "\n",
        "class RefinementBlockSeparable(Module):\n",
        "\n",
        "    def __init__(self, layer_idx, opts, n_channels=512, inner_c=256, spatial=16):\n",
        "        super(RefinementBlockSeparable, self).__init__()\n",
        "        self.layer_idx = layer_idx\n",
        "        self.kernel_size, self.in_channels, self.out_channels = PARAMETERS[self.layer_idx]\n",
        "        self.spatial = spatial\n",
        "        self.n_channels = n_channels\n",
        "        self.inner_c = inner_c\n",
        "        self.out_c = 512\n",
        "        num_pools = int(np.log2(self.spatial)) - 1\n",
        "        self.modules = []\n",
        "        self.modules += [Conv2d(self.n_channels, self.inner_c, kernel_size=3, stride=2, padding=1), nn.LeakyReLU()]\n",
        "        for i in range(num_pools - 1):\n",
        "            self.modules += [Conv2d(self.inner_c, self.inner_c, kernel_size=3, stride=2, padding=1), nn.LeakyReLU()]\n",
        "        self.modules += [Conv2d(self.inner_c, self.out_c, kernel_size=3, stride=2, padding=1), nn.LeakyReLU()]\n",
        "        self.convs = nn.Sequential(*self.modules)\n",
        "\n",
        "        self.opts = opts\n",
        "        if self.layer_idx in TO_RGB_LAYERS:\n",
        "            self.output = Sequential(Conv2d(self.out_c, self.in_channels * self.out_channels,\n",
        "                                            kernel_size=1, stride=1, padding=0))\n",
        "        else:\n",
        "            self.output = Sequential(SeparableBlock(input_size=self.out_c,\n",
        "                                                    kernel_channels_in=self.in_channels,\n",
        "                                                    kernel_channels_out=self.out_channels,\n",
        "                                                    kernel_size=self.kernel_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convs(x)\n",
        "        x = self.output(x)\n",
        "        if self.layer_idx in TO_RGB_LAYERS:\n",
        "            x = x.view(-1, self.out_channels, self.in_channels, self.kernel_size, self.kernel_size)\n",
        "        return x"
      ],
      "metadata": {
        "id": "uaoG3XoMQPq_"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models.hypernetworks.hypernetwork.py\n",
        "from torch import nn\n",
        "from torch.nn import BatchNorm2d, PReLU, Sequential, Module\n",
        "from torchvision.models import resnet34\n",
        "\n",
        "# from models.hypernetworks.refinement_blocks import HyperRefinementBlock, RefinementBlock, RefinementBlockSeparable\n",
        "# from models.hypernetworks.shared_weights_hypernet import SharedWeightsHypernet\n",
        "\n",
        "\n",
        "class SharedWeightsHyperNetResNet(Module):\n",
        "\n",
        "    def __init__(self, opts):\n",
        "        super(SharedWeightsHyperNetResNet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(opts.input_nc, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = BatchNorm2d(64)\n",
        "        self.relu = PReLU(64)\n",
        "\n",
        "        resnet_basenet = resnet34(pretrained=True)\n",
        "        blocks = [\n",
        "            resnet_basenet.layer1,\n",
        "            resnet_basenet.layer2,\n",
        "            resnet_basenet.layer3,\n",
        "            resnet_basenet.layer4\n",
        "        ]\n",
        "        modules = []\n",
        "        for block in blocks:\n",
        "            for bottleneck in block:\n",
        "                modules.append(bottleneck)\n",
        "        self.body = Sequential(*modules)\n",
        "\n",
        "        if len(opts.layers_to_tune) == 0:\n",
        "            self.layers_to_tune = list(range(opts.n_hypernet_outputs))\n",
        "        else:\n",
        "            self.layers_to_tune = [int(l) for l in opts.layers_to_tune.split(',')]\n",
        "\n",
        "        self.shared_layers = [0, 2, 3, 5, 6, 8, 9, 11, 12]\n",
        "        self.shared_weight_hypernet = SharedWeightsHypernet(in_size=512, out_size=512, mode=None)\n",
        "\n",
        "        self.refinement_blocks = nn.ModuleList()\n",
        "        self.n_outputs = opts.n_hypernet_outputs\n",
        "        for layer_idx in range(self.n_outputs):\n",
        "            if layer_idx in self.layers_to_tune:\n",
        "                if layer_idx in self.shared_layers:\n",
        "                    refinement_block = HyperRefinementBlock(self.shared_weight_hypernet, n_channels=512, inner_c=128)\n",
        "                else:\n",
        "                    refinement_block = RefinementBlock(layer_idx, opts, n_channels=512, inner_c=256)\n",
        "            else:\n",
        "                refinement_block = None\n",
        "            self.refinement_blocks.append(refinement_block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.body(x)\n",
        "        weight_deltas = []\n",
        "        for j in range(self.n_outputs):\n",
        "            if self.refinement_blocks[j] is not None:\n",
        "                delta = self.refinement_blocks[j](x)\n",
        "            else:\n",
        "                delta = None\n",
        "            weight_deltas.append(delta)\n",
        "        return weight_deltas\n",
        "\n",
        "\n",
        "class SharedWeightsHyperNetResNetSeparable(Module):\n",
        "\n",
        "    def __init__(self, opts):\n",
        "        super(SharedWeightsHyperNetResNetSeparable, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(opts.input_nc, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = BatchNorm2d(64)\n",
        "        self.relu = PReLU(64)\n",
        "\n",
        "        resnet_basenet = resnet34(pretrained=True)\n",
        "        blocks = [\n",
        "            resnet_basenet.layer1,\n",
        "            resnet_basenet.layer2,\n",
        "            resnet_basenet.layer3,\n",
        "            resnet_basenet.layer4\n",
        "        ]\n",
        "        modules = []\n",
        "        for block in blocks:\n",
        "            for bottleneck in block:\n",
        "                modules.append(bottleneck)\n",
        "        self.body = Sequential(*modules)\n",
        "\n",
        "        if len(opts.layers_to_tune) == 0:\n",
        "            self.layers_to_tune = list(range(opts.n_hypernet_outputs))\n",
        "        else:\n",
        "            self.layers_to_tune = [int(l) for l in opts.layers_to_tune.split(',')]\n",
        "\n",
        "        self.shared_layers = [0, 2, 3, 5, 6, 8, 9, 11, 12]\n",
        "        self.shared_weight_hypernet = SharedWeightsHypernet(in_size=512, out_size=512, mode=None)\n",
        "\n",
        "        self.refinement_blocks = nn.ModuleList()\n",
        "        self.n_outputs = opts.n_hypernet_outputs\n",
        "        for layer_idx in range(self.n_outputs):\n",
        "            if layer_idx in self.layers_to_tune:\n",
        "                if layer_idx in self.shared_layers:\n",
        "                    refinement_block = HyperRefinementBlock(self.shared_weight_hypernet, n_channels=512, inner_c=128)\n",
        "                else:\n",
        "                    refinement_block = RefinementBlockSeparable(layer_idx, opts, n_channels=512, inner_c=256)\n",
        "            else:\n",
        "                refinement_block = None\n",
        "            self.refinement_blocks.append(refinement_block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.body(x)\n",
        "        weight_deltas = []\n",
        "        for j in range(self.n_outputs):\n",
        "            if self.refinement_blocks[j] is not None:\n",
        "                delta = self.refinement_blocks[j](x)\n",
        "            else:\n",
        "                delta = None\n",
        "            weight_deltas.append(delta)\n",
        "        return weight_deltas"
      ],
      "metadata": {
        "id": "8ACezT3fPqvk"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models.mtcnn.mtcnn_pytorch.src.align_trans.py\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Apr 24 15:43:29 2017\n",
        "@author: zhaoy\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# from scipy.linalg import lstsq\n",
        "# from scipy.ndimage import geometric_transform  # , map_coordinates\n",
        "\n",
        "# from models.mtcnn.mtcnn_pytorch.src.matlab_cp2tform import get_similarity_transform_for_cv2\n",
        "\n",
        "# reference facial points, a list of coordinates (x,y)\n",
        "REFERENCE_FACIAL_POINTS = [\n",
        "    [30.29459953, 51.69630051],\n",
        "    [65.53179932, 51.50139999],\n",
        "    [48.02519989, 71.73660278],\n",
        "    [33.54930115, 92.3655014],\n",
        "    [62.72990036, 92.20410156]\n",
        "]\n",
        "\n",
        "DEFAULT_CROP_SIZE = (96, 112)\n",
        "\n",
        "\n",
        "class FaceWarpException(Exception):\n",
        "    def __str__(self):\n",
        "        return 'In File {}:{}'.format(\n",
        "            __file__, super.__str__(self))\n",
        "\n",
        "\n",
        "def get_reference_facial_points(output_size=None,\n",
        "                                inner_padding_factor=0.0,\n",
        "                                outer_padding=(0, 0),\n",
        "                                default_square=False):\n",
        "    \"\"\"\n",
        "    Function:\n",
        "    ----------\n",
        "        get reference 5 key points according to crop settings:\n",
        "        0. Set default crop_size:\n",
        "            if default_square: \n",
        "                crop_size = (112, 112)\n",
        "            else: \n",
        "                crop_size = (96, 112)\n",
        "        1. Pad the crop_size by inner_padding_factor in each side;\n",
        "        2. Resize crop_size into (output_size - outer_padding*2),\n",
        "            pad into output_size with outer_padding;\n",
        "        3. Output reference_5point;\n",
        "    Parameters:\n",
        "    ----------\n",
        "        @output_size: (w, h) or None\n",
        "            size of aligned face image\n",
        "        @inner_padding_factor: (w_factor, h_factor)\n",
        "            padding factor for inner (w, h)\n",
        "        @outer_padding: (w_pad, h_pad)\n",
        "            each row is a pair of coordinates (x, y)\n",
        "        @default_square: True or False\n",
        "            if True:\n",
        "                default crop_size = (112, 112)\n",
        "            else:\n",
        "                default crop_size = (96, 112);\n",
        "        !!! make sure, if output_size is not None:\n",
        "                (output_size - outer_padding) \n",
        "                = some_scale * (default crop_size * (1.0 + inner_padding_factor))\n",
        "    Returns:\n",
        "    ----------\n",
        "        @reference_5point: 5x2 np.array\n",
        "            each row is a pair of transformed coordinates (x, y)\n",
        "    \"\"\"\n",
        "    # print('\\n===> get_reference_facial_points():')\n",
        "\n",
        "    # print('---> Params:')\n",
        "    # print('            output_size: ', output_size)\n",
        "    # print('            inner_padding_factor: ', inner_padding_factor)\n",
        "    # print('            outer_padding:', outer_padding)\n",
        "    # print('            default_square: ', default_square)\n",
        "\n",
        "    tmp_5pts = np.array(REFERENCE_FACIAL_POINTS)\n",
        "    tmp_crop_size = np.array(DEFAULT_CROP_SIZE)\n",
        "\n",
        "    # 0) make the inner region a square\n",
        "    if default_square:\n",
        "        size_diff = max(tmp_crop_size) - tmp_crop_size\n",
        "        tmp_5pts += size_diff / 2\n",
        "        tmp_crop_size += size_diff\n",
        "\n",
        "    # print('---> default:')\n",
        "    # print('              crop_size = ', tmp_crop_size)\n",
        "    # print('              reference_5pts = ', tmp_5pts)\n",
        "\n",
        "    if (output_size and\n",
        "            output_size[0] == tmp_crop_size[0] and\n",
        "            output_size[1] == tmp_crop_size[1]):\n",
        "        # print('output_size == DEFAULT_CROP_SIZE {}: return default reference points'.format(tmp_crop_size))\n",
        "        return tmp_5pts\n",
        "\n",
        "    if (inner_padding_factor == 0 and\n",
        "            outer_padding == (0, 0)):\n",
        "        if output_size is None:\n",
        "            # print('No paddings to do: return default reference points')\n",
        "            return tmp_5pts\n",
        "        else:\n",
        "            raise FaceWarpException(\n",
        "                'No paddings to do, output_size must be None or {}'.format(tmp_crop_size))\n",
        "\n",
        "    # check output size\n",
        "    if not (0 <= inner_padding_factor <= 1.0):\n",
        "        raise FaceWarpException('Not (0 <= inner_padding_factor <= 1.0)')\n",
        "\n",
        "    if ((inner_padding_factor > 0 or outer_padding[0] > 0 or outer_padding[1] > 0)\n",
        "            and output_size is None):\n",
        "        output_size = tmp_crop_size * \\\n",
        "                      (1 + inner_padding_factor * 2).astype(np.int32)\n",
        "        output_size += np.array(outer_padding)\n",
        "        # print('              deduced from paddings, output_size = ', output_size)\n",
        "\n",
        "    if not (outer_padding[0] < output_size[0]\n",
        "            and outer_padding[1] < output_size[1]):\n",
        "        raise FaceWarpException('Not (outer_padding[0] < output_size[0]'\n",
        "                                'and outer_padding[1] < output_size[1])')\n",
        "\n",
        "    # 1) pad the inner region according inner_padding_factor\n",
        "    # print('---> STEP1: pad the inner region according inner_padding_factor')\n",
        "    if inner_padding_factor > 0:\n",
        "        size_diff = tmp_crop_size * inner_padding_factor * 2\n",
        "        tmp_5pts += size_diff / 2\n",
        "        tmp_crop_size += np.round(size_diff).astype(np.int32)\n",
        "\n",
        "    # print('              crop_size = ', tmp_crop_size)\n",
        "    # print('              reference_5pts = ', tmp_5pts)\n",
        "\n",
        "    # 2) resize the padded inner region\n",
        "    # print('---> STEP2: resize the padded inner region')\n",
        "    size_bf_outer_pad = np.array(output_size) - np.array(outer_padding) * 2\n",
        "    # print('              crop_size = ', tmp_crop_size)\n",
        "    # print('              size_bf_outer_pad = ', size_bf_outer_pad)\n",
        "\n",
        "    if size_bf_outer_pad[0] * tmp_crop_size[1] != size_bf_outer_pad[1] * tmp_crop_size[0]:\n",
        "        raise FaceWarpException('Must have (output_size - outer_padding)'\n",
        "                                '= some_scale * (crop_size * (1.0 + inner_padding_factor)')\n",
        "\n",
        "    scale_factor = size_bf_outer_pad[0].astype(np.float32) / tmp_crop_size[0]\n",
        "    # print('              resize scale_factor = ', scale_factor)\n",
        "    tmp_5pts = tmp_5pts * scale_factor\n",
        "    #    size_diff = tmp_crop_size * (scale_factor - min(scale_factor))\n",
        "    #    tmp_5pts = tmp_5pts + size_diff / 2\n",
        "    tmp_crop_size = size_bf_outer_pad\n",
        "    # print('              crop_size = ', tmp_crop_size)\n",
        "    # print('              reference_5pts = ', tmp_5pts)\n",
        "\n",
        "    # 3) add outer_padding to make output_size\n",
        "    reference_5point = tmp_5pts + np.array(outer_padding)\n",
        "    tmp_crop_size = output_size\n",
        "    # print('---> STEP3: add outer_padding to make output_size')\n",
        "    # print('              crop_size = ', tmp_crop_size)\n",
        "    # print('              reference_5pts = ', tmp_5pts)\n",
        "\n",
        "    # print('===> end get_reference_facial_points\\n')\n",
        "\n",
        "    return reference_5point\n",
        "\n",
        "\n",
        "def get_affine_transform_matrix(src_pts, dst_pts):\n",
        "    \"\"\"\n",
        "    Function:\n",
        "    ----------\n",
        "        get affine transform matrix 'tfm' from src_pts to dst_pts\n",
        "    Parameters:\n",
        "    ----------\n",
        "        @src_pts: Kx2 np.array\n",
        "            source points matrix, each row is a pair of coordinates (x, y)\n",
        "        @dst_pts: Kx2 np.array\n",
        "            destination points matrix, each row is a pair of coordinates (x, y)\n",
        "    Returns:\n",
        "    ----------\n",
        "        @tfm: 2x3 np.array\n",
        "            transform matrix from src_pts to dst_pts\n",
        "    \"\"\"\n",
        "\n",
        "    tfm = np.float32([[1, 0, 0], [0, 1, 0]])\n",
        "    n_pts = src_pts.shape[0]\n",
        "    ones = np.ones((n_pts, 1), src_pts.dtype)\n",
        "    src_pts_ = np.hstack([src_pts, ones])\n",
        "    dst_pts_ = np.hstack([dst_pts, ones])\n",
        "\n",
        "    #    #print(('src_pts_:\\n' + str(src_pts_))\n",
        "    #    #print(('dst_pts_:\\n' + str(dst_pts_))\n",
        "\n",
        "    A, res, rank, s = np.linalg.lstsq(src_pts_, dst_pts_)\n",
        "\n",
        "    #    #print(('np.linalg.lstsq return A: \\n' + str(A))\n",
        "    #    #print(('np.linalg.lstsq return res: \\n' + str(res))\n",
        "    #    #print(('np.linalg.lstsq return rank: \\n' + str(rank))\n",
        "    #    #print(('np.linalg.lstsq return s: \\n' + str(s))\n",
        "\n",
        "    if rank == 3:\n",
        "        tfm = np.float32([\n",
        "            [A[0, 0], A[1, 0], A[2, 0]],\n",
        "            [A[0, 1], A[1, 1], A[2, 1]]\n",
        "        ])\n",
        "    elif rank == 2:\n",
        "        tfm = np.float32([\n",
        "            [A[0, 0], A[1, 0], 0],\n",
        "            [A[0, 1], A[1, 1], 0]\n",
        "        ])\n",
        "\n",
        "    return tfm\n",
        "\n",
        "\n",
        "def warp_and_crop_face(src_img,\n",
        "                       facial_pts,\n",
        "                       reference_pts=None,\n",
        "                       crop_size=(96, 112),\n",
        "                       align_type='smilarity'):\n",
        "    \"\"\"\n",
        "    Function:\n",
        "    ----------\n",
        "        apply affine transform 'trans' to uv\n",
        "    Parameters:\n",
        "    ----------\n",
        "        @src_img: 3x3 np.array\n",
        "            input image\n",
        "        @facial_pts: could be\n",
        "            1)a list of K coordinates (x,y)\n",
        "        or\n",
        "            2) Kx2 or 2xK np.array\n",
        "            each row or col is a pair of coordinates (x, y)\n",
        "        @reference_pts: could be\n",
        "            1) a list of K coordinates (x,y)\n",
        "        or\n",
        "            2) Kx2 or 2xK np.array\n",
        "            each row or col is a pair of coordinates (x, y)\n",
        "        or\n",
        "            3) None\n",
        "            if None, use default reference facial points\n",
        "        @crop_size: (w, h)\n",
        "            output face image size\n",
        "        @align_type: transform type, could be one of\n",
        "            1) 'similarity': use similarity transform\n",
        "            2) 'cv2_affine': use the first 3 points to do affine transform,\n",
        "                    by calling cv2.getAffineTransform()\n",
        "            3) 'affine': use all points to do affine transform\n",
        "    Returns:\n",
        "    ----------\n",
        "        @face_img: output face image with size (w, h) = @crop_size\n",
        "    \"\"\"\n",
        "\n",
        "    if reference_pts is None:\n",
        "        if crop_size[0] == 96 and crop_size[1] == 112:\n",
        "            reference_pts = REFERENCE_FACIAL_POINTS\n",
        "        else:\n",
        "            default_square = False\n",
        "            inner_padding_factor = 0\n",
        "            outer_padding = (0, 0)\n",
        "            output_size = crop_size\n",
        "\n",
        "            reference_pts = get_reference_facial_points(output_size,\n",
        "                                                        inner_padding_factor,\n",
        "                                                        outer_padding,\n",
        "                                                        default_square)\n",
        "\n",
        "    ref_pts = np.float32(reference_pts)\n",
        "    ref_pts_shp = ref_pts.shape\n",
        "    if max(ref_pts_shp) < 3 or min(ref_pts_shp) != 2:\n",
        "        raise FaceWarpException(\n",
        "            'reference_pts.shape must be (K,2) or (2,K) and K>2')\n",
        "\n",
        "    if ref_pts_shp[0] == 2:\n",
        "        ref_pts = ref_pts.T\n",
        "\n",
        "    src_pts = np.float32(facial_pts)\n",
        "    src_pts_shp = src_pts.shape\n",
        "    if max(src_pts_shp) < 3 or min(src_pts_shp) != 2:\n",
        "        raise FaceWarpException(\n",
        "            'facial_pts.shape must be (K,2) or (2,K) and K>2')\n",
        "\n",
        "    if src_pts_shp[0] == 2:\n",
        "        src_pts = src_pts.T\n",
        "\n",
        "    #    #print('--->src_pts:\\n', src_pts\n",
        "    #    #print('--->ref_pts\\n', ref_pts\n",
        "\n",
        "    if src_pts.shape != ref_pts.shape:\n",
        "        raise FaceWarpException(\n",
        "            'facial_pts and reference_pts must have the same shape')\n",
        "\n",
        "    if align_type is 'cv2_affine':\n",
        "        tfm = cv2.getAffineTransform(src_pts[0:3], ref_pts[0:3])\n",
        "    #        #print(('cv2.getAffineTransform() returns tfm=\\n' + str(tfm))\n",
        "    elif align_type is 'affine':\n",
        "        tfm = get_affine_transform_matrix(src_pts, ref_pts)\n",
        "    #        #print(('get_affine_transform_matrix() returns tfm=\\n' + str(tfm))\n",
        "    else:\n",
        "        tfm = get_similarity_transform_for_cv2(src_pts, ref_pts)\n",
        "    #        #print(('get_similarity_transform_for_cv2() returns tfm=\\n' + str(tfm))\n",
        "\n",
        "    #    #print('--->Transform matrix: '\n",
        "    #    #print(('type(tfm):' + str(type(tfm)))\n",
        "    #    #print(('tfm.dtype:' + str(tfm.dtype))\n",
        "    #    #print( tfm\n",
        "\n",
        "    face_img = cv2.warpAffine(src_img, tfm, (crop_size[0], crop_size[1]))\n",
        "\n",
        "    return face_img, tfm"
      ],
      "metadata": {
        "id": "VEI_7loJQg_O"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models.mtcnn.mtcnn_pytorch.src.box_utils.py\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def nms(boxes, overlap_threshold=0.5, mode='union'):\n",
        "    \"\"\"Non-maximum suppression.\n",
        "\n",
        "    Arguments:\n",
        "        boxes: a float numpy array of shape [n, 5],\n",
        "            where each row is (xmin, ymin, xmax, ymax, score).\n",
        "        overlap_threshold: a float number.\n",
        "        mode: 'union' or 'min'.\n",
        "\n",
        "    Returns:\n",
        "        list with indices of the selected boxes\n",
        "    \"\"\"\n",
        "\n",
        "    # if there are no boxes, return the empty list\n",
        "    if len(boxes) == 0:\n",
        "        return []\n",
        "\n",
        "    # list of picked indices\n",
        "    pick = []\n",
        "\n",
        "    # grab the coordinates of the bounding boxes\n",
        "    x1, y1, x2, y2, score = [boxes[:, i] for i in range(5)]\n",
        "\n",
        "    area = (x2 - x1 + 1.0) * (y2 - y1 + 1.0)\n",
        "    ids = np.argsort(score)  # in increasing order\n",
        "\n",
        "    while len(ids) > 0:\n",
        "\n",
        "        # grab index of the largest value\n",
        "        last = len(ids) - 1\n",
        "        i = ids[last]\n",
        "        pick.append(i)\n",
        "\n",
        "        # compute intersections\n",
        "        # of the box with the largest score\n",
        "        # with the rest of boxes\n",
        "\n",
        "        # left top corner of intersection boxes\n",
        "        ix1 = np.maximum(x1[i], x1[ids[:last]])\n",
        "        iy1 = np.maximum(y1[i], y1[ids[:last]])\n",
        "\n",
        "        # right bottom corner of intersection boxes\n",
        "        ix2 = np.minimum(x2[i], x2[ids[:last]])\n",
        "        iy2 = np.minimum(y2[i], y2[ids[:last]])\n",
        "\n",
        "        # width and height of intersection boxes\n",
        "        w = np.maximum(0.0, ix2 - ix1 + 1.0)\n",
        "        h = np.maximum(0.0, iy2 - iy1 + 1.0)\n",
        "\n",
        "        # intersections' areas\n",
        "        inter = w * h\n",
        "        if mode == 'min':\n",
        "            overlap = inter / np.minimum(area[i], area[ids[:last]])\n",
        "        elif mode == 'union':\n",
        "            # intersection over union (IoU)\n",
        "            overlap = inter / (area[i] + area[ids[:last]] - inter)\n",
        "\n",
        "        # delete all boxes where overlap is too big\n",
        "        ids = np.delete(\n",
        "            ids,\n",
        "            np.concatenate([[last], np.where(overlap > overlap_threshold)[0]])\n",
        "        )\n",
        "\n",
        "    return pick\n",
        "\n",
        "\n",
        "def convert_to_square(bboxes):\n",
        "    \"\"\"Convert bounding boxes to a square form.\n",
        "\n",
        "    Arguments:\n",
        "        bboxes: a float numpy array of shape [n, 5].\n",
        "\n",
        "    Returns:\n",
        "        a float numpy array of shape [n, 5],\n",
        "            squared bounding boxes.\n",
        "    \"\"\"\n",
        "\n",
        "    square_bboxes = np.zeros_like(bboxes)\n",
        "    x1, y1, x2, y2 = [bboxes[:, i] for i in range(4)]\n",
        "    h = y2 - y1 + 1.0\n",
        "    w = x2 - x1 + 1.0\n",
        "    max_side = np.maximum(h, w)\n",
        "    square_bboxes[:, 0] = x1 + w * 0.5 - max_side * 0.5\n",
        "    square_bboxes[:, 1] = y1 + h * 0.5 - max_side * 0.5\n",
        "    square_bboxes[:, 2] = square_bboxes[:, 0] + max_side - 1.0\n",
        "    square_bboxes[:, 3] = square_bboxes[:, 1] + max_side - 1.0\n",
        "    return square_bboxes\n",
        "\n",
        "\n",
        "def calibrate_box(bboxes, offsets):\n",
        "    \"\"\"Transform bounding boxes to be more like true bounding boxes.\n",
        "    'offsets' is one of the outputs of the nets.\n",
        "\n",
        "    Arguments:\n",
        "        bboxes: a float numpy array of shape [n, 5].\n",
        "        offsets: a float numpy array of shape [n, 4].\n",
        "\n",
        "    Returns:\n",
        "        a float numpy array of shape [n, 5].\n",
        "    \"\"\"\n",
        "    x1, y1, x2, y2 = [bboxes[:, i] for i in range(4)]\n",
        "    w = x2 - x1 + 1.0\n",
        "    h = y2 - y1 + 1.0\n",
        "    w = np.expand_dims(w, 1)\n",
        "    h = np.expand_dims(h, 1)\n",
        "\n",
        "    # this is what happening here:\n",
        "    # tx1, ty1, tx2, ty2 = [offsets[:, i] for i in range(4)]\n",
        "    # x1_true = x1 + tx1*w\n",
        "    # y1_true = y1 + ty1*h\n",
        "    # x2_true = x2 + tx2*w\n",
        "    # y2_true = y2 + ty2*h\n",
        "    # below is just more compact form of this\n",
        "\n",
        "    # are offsets always such that\n",
        "    # x1 < x2 and y1 < y2 ?\n",
        "\n",
        "    translation = np.hstack([w, h, w, h]) * offsets\n",
        "    bboxes[:, 0:4] = bboxes[:, 0:4] + translation\n",
        "    return bboxes\n",
        "\n",
        "\n",
        "def get_image_boxes(bounding_boxes, img, size=24):\n",
        "    \"\"\"Cut out boxes from the image.\n",
        "\n",
        "    Arguments:\n",
        "        bounding_boxes: a float numpy array of shape [n, 5].\n",
        "        img: an instance of PIL.Image.\n",
        "        size: an integer, size of cutouts.\n",
        "\n",
        "    Returns:\n",
        "        a float numpy array of shape [n, 3, size, size].\n",
        "    \"\"\"\n",
        "\n",
        "    num_boxes = len(bounding_boxes)\n",
        "    width, height = img.size\n",
        "\n",
        "    [dy, edy, dx, edx, y, ey, x, ex, w, h] = correct_bboxes(bounding_boxes, width, height)\n",
        "    img_boxes = np.zeros((num_boxes, 3, size, size), 'float32')\n",
        "\n",
        "    for i in range(num_boxes):\n",
        "        img_box = np.zeros((h[i], w[i], 3), 'uint8')\n",
        "\n",
        "        img_array = np.asarray(img, 'uint8')\n",
        "        img_box[dy[i]:(edy[i] + 1), dx[i]:(edx[i] + 1), :] = \\\n",
        "            img_array[y[i]:(ey[i] + 1), x[i]:(ex[i] + 1), :]\n",
        "\n",
        "        # resize\n",
        "        img_box = Image.fromarray(img_box)\n",
        "        img_box = img_box.resize((size, size), Image.BILINEAR)\n",
        "        img_box = np.asarray(img_box, 'float32')\n",
        "\n",
        "        img_boxes[i, :, :, :] = _preprocess(img_box)\n",
        "\n",
        "    return img_boxes\n",
        "\n",
        "\n",
        "def correct_bboxes(bboxes, width, height):\n",
        "    \"\"\"Crop boxes that are too big and get coordinates\n",
        "    with respect to cutouts.\n",
        "\n",
        "    Arguments:\n",
        "        bboxes: a float numpy array of shape [n, 5],\n",
        "            where each row is (xmin, ymin, xmax, ymax, score).\n",
        "        width: a float number.\n",
        "        height: a float number.\n",
        "\n",
        "    Returns:\n",
        "        dy, dx, edy, edx: a int numpy arrays of shape [n],\n",
        "            coordinates of the boxes with respect to the cutouts.\n",
        "        y, x, ey, ex: a int numpy arrays of shape [n],\n",
        "            corrected ymin, xmin, ymax, xmax.\n",
        "        h, w: a int numpy arrays of shape [n],\n",
        "            just heights and widths of boxes.\n",
        "\n",
        "        in the following order:\n",
        "            [dy, edy, dx, edx, y, ey, x, ex, w, h].\n",
        "    \"\"\"\n",
        "\n",
        "    x1, y1, x2, y2 = [bboxes[:, i] for i in range(4)]\n",
        "    w, h = x2 - x1 + 1.0, y2 - y1 + 1.0\n",
        "    num_boxes = bboxes.shape[0]\n",
        "\n",
        "    # 'e' stands for end\n",
        "    # (x, y) -> (ex, ey)\n",
        "    x, y, ex, ey = x1, y1, x2, y2\n",
        "\n",
        "    # we need to cut out a box from the image.\n",
        "    # (x, y, ex, ey) are corrected coordinates of the box\n",
        "    # in the image.\n",
        "    # (dx, dy, edx, edy) are coordinates of the box in the cutout\n",
        "    # from the image.\n",
        "    dx, dy = np.zeros((num_boxes,)), np.zeros((num_boxes,))\n",
        "    edx, edy = w.copy() - 1.0, h.copy() - 1.0\n",
        "\n",
        "    # if box's bottom right corner is too far right\n",
        "    ind = np.where(ex > width - 1.0)[0]\n",
        "    edx[ind] = w[ind] + width - 2.0 - ex[ind]\n",
        "    ex[ind] = width - 1.0\n",
        "\n",
        "    # if box's bottom right corner is too low\n",
        "    ind = np.where(ey > height - 1.0)[0]\n",
        "    edy[ind] = h[ind] + height - 2.0 - ey[ind]\n",
        "    ey[ind] = height - 1.0\n",
        "\n",
        "    # if box's top left corner is too far left\n",
        "    ind = np.where(x < 0.0)[0]\n",
        "    dx[ind] = 0.0 - x[ind]\n",
        "    x[ind] = 0.0\n",
        "\n",
        "    # if box's top left corner is too high\n",
        "    ind = np.where(y < 0.0)[0]\n",
        "    dy[ind] = 0.0 - y[ind]\n",
        "    y[ind] = 0.0\n",
        "\n",
        "    return_list = [dy, edy, dx, edx, y, ey, x, ex, w, h]\n",
        "    return_list = [i.astype('int32') for i in return_list]\n",
        "\n",
        "    return return_list\n",
        "\n",
        "\n",
        "def _preprocess(img):\n",
        "    \"\"\"Preprocessing step before feeding the network.\n",
        "\n",
        "    Arguments:\n",
        "        img: a float numpy array of shape [h, w, c].\n",
        "\n",
        "    Returns:\n",
        "        a float numpy array of shape [1, c, h, w].\n",
        "    \"\"\"\n",
        "    img = img.transpose((2, 0, 1))\n",
        "    img = np.expand_dims(img, 0)\n",
        "    img = (img - 127.5) * 0.0078125\n",
        "    return img"
      ],
      "metadata": {
        "id": "P5CMgNrdQwGf"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models.mtcnn_pytorch.src.detector.py\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "# from .get_nets import PNet, RNet, ONet\n",
        "# from .box_utils import nms, calibrate_box, get_image_boxes, convert_to_square\n",
        "# from .first_stage import run_first_stage\n",
        "\n",
        "\n",
        "def detect_faces(image, min_face_size=20.0,\n",
        "                 thresholds=[0.6, 0.7, 0.8],\n",
        "                 nms_thresholds=[0.7, 0.7, 0.7]):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        image: an instance of PIL.Image.\n",
        "        min_face_size: a float number.\n",
        "        thresholds: a list of length 3.\n",
        "        nms_thresholds: a list of length 3.\n",
        "\n",
        "    Returns:\n",
        "        two float numpy arrays of shapes [n_boxes, 4] and [n_boxes, 10],\n",
        "        bounding boxes and facial landmarks.\n",
        "    \"\"\"\n",
        "\n",
        "    # LOAD MODELS\n",
        "    pnet = PNet()\n",
        "    rnet = RNet()\n",
        "    onet = ONet()\n",
        "    onet.eval()\n",
        "\n",
        "    # BUILD AN IMAGE PYRAMID\n",
        "    width, height = image.size\n",
        "    min_length = min(height, width)\n",
        "\n",
        "    min_detection_size = 12\n",
        "    factor = 0.707  # sqrt(0.5)\n",
        "\n",
        "    # scales for scaling the image\n",
        "    scales = []\n",
        "\n",
        "    # scales the image so that\n",
        "    # minimum size that we can detect equals to\n",
        "    # minimum face size that we want to detect\n",
        "    m = min_detection_size / min_face_size\n",
        "    min_length *= m\n",
        "\n",
        "    factor_count = 0\n",
        "    while min_length > min_detection_size:\n",
        "        scales.append(m * factor ** factor_count)\n",
        "        min_length *= factor\n",
        "        factor_count += 1\n",
        "\n",
        "    # STAGE 1\n",
        "\n",
        "    # it will be returned\n",
        "    bounding_boxes = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # run P-Net on different scales\n",
        "        for s in scales:\n",
        "            boxes = run_first_stage(image, pnet, scale=s, threshold=thresholds[0])\n",
        "            bounding_boxes.append(boxes)\n",
        "\n",
        "        # collect boxes (and offsets, and scores) from different scales\n",
        "        bounding_boxes = [i for i in bounding_boxes if i is not None]\n",
        "        bounding_boxes = np.vstack(bounding_boxes)\n",
        "\n",
        "        keep = nms(bounding_boxes[:, 0:5], nms_thresholds[0])\n",
        "        bounding_boxes = bounding_boxes[keep]\n",
        "\n",
        "        # use offsets predicted by pnet to transform bounding boxes\n",
        "        bounding_boxes = calibrate_box(bounding_boxes[:, 0:5], bounding_boxes[:, 5:])\n",
        "        # shape [n_boxes, 5]\n",
        "\n",
        "        bounding_boxes = convert_to_square(bounding_boxes)\n",
        "        bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n",
        "\n",
        "        # STAGE 2\n",
        "\n",
        "        img_boxes = get_image_boxes(bounding_boxes, image, size=24)\n",
        "        img_boxes = torch.FloatTensor(img_boxes)\n",
        "\n",
        "        output = rnet(img_boxes)\n",
        "        offsets = output[0].data.numpy()  # shape [n_boxes, 4]\n",
        "        probs = output[1].data.numpy()  # shape [n_boxes, 2]\n",
        "\n",
        "        keep = np.where(probs[:, 1] > thresholds[1])[0]\n",
        "        bounding_boxes = bounding_boxes[keep]\n",
        "        bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\n",
        "        offsets = offsets[keep]\n",
        "\n",
        "        keep = nms(bounding_boxes, nms_thresholds[1])\n",
        "        bounding_boxes = bounding_boxes[keep]\n",
        "        bounding_boxes = calibrate_box(bounding_boxes, offsets[keep])\n",
        "        bounding_boxes = convert_to_square(bounding_boxes)\n",
        "        bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n",
        "\n",
        "        # STAGE 3\n",
        "\n",
        "        img_boxes = get_image_boxes(bounding_boxes, image, size=48)\n",
        "        if len(img_boxes) == 0:\n",
        "            return [], []\n",
        "        img_boxes = torch.FloatTensor(img_boxes)\n",
        "        output = onet(img_boxes)\n",
        "        landmarks = output[0].data.numpy()  # shape [n_boxes, 10]\n",
        "        offsets = output[1].data.numpy()  # shape [n_boxes, 4]\n",
        "        probs = output[2].data.numpy()  # shape [n_boxes, 2]\n",
        "\n",
        "        keep = np.where(probs[:, 1] > thresholds[2])[0]\n",
        "        bounding_boxes = bounding_boxes[keep]\n",
        "        bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\n",
        "        offsets = offsets[keep]\n",
        "        landmarks = landmarks[keep]\n",
        "\n",
        "        # compute landmark points\n",
        "        width = bounding_boxes[:, 2] - bounding_boxes[:, 0] + 1.0\n",
        "        height = bounding_boxes[:, 3] - bounding_boxes[:, 1] + 1.0\n",
        "        xmin, ymin = bounding_boxes[:, 0], bounding_boxes[:, 1]\n",
        "        landmarks[:, 0:5] = np.expand_dims(xmin, 1) + np.expand_dims(width, 1) * landmarks[:, 0:5]\n",
        "        landmarks[:, 5:10] = np.expand_dims(ymin, 1) + np.expand_dims(height, 1) * landmarks[:, 5:10]\n",
        "\n",
        "        bounding_boxes = calibrate_box(bounding_boxes, offsets)\n",
        "        keep = nms(bounding_boxes, nms_thresholds[2], mode='min')\n",
        "        bounding_boxes = bounding_boxes[keep]\n",
        "        landmarks = landmarks[keep]\n",
        "\n",
        "    return bounding_boxes, landmarks"
      ],
      "metadata": {
        "id": "1o2efVm7Q5JA"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models.mtcnn_pytorch.src.first_stage.py\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import math\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "# from .box_utils import nms, _preprocess\n",
        "\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = 'cuda:0'\n",
        "\n",
        "\n",
        "def run_first_stage(image, net, scale, threshold):\n",
        "    \"\"\"Run P-Net, generate bounding boxes, and do NMS.\n",
        "\n",
        "    Arguments:\n",
        "        image: an instance of PIL.Image.\n",
        "        net: an instance of pytorch's nn.Module, P-Net.\n",
        "        scale: a float number,\n",
        "            scale width and height of the image by this number.\n",
        "        threshold: a float number,\n",
        "            threshold on the probability of a face when generating\n",
        "            bounding boxes from predictions of the net.\n",
        "\n",
        "    Returns:\n",
        "        a float numpy array of shape [n_boxes, 9],\n",
        "            bounding boxes with scores and offsets (4 + 1 + 4).\n",
        "    \"\"\"\n",
        "\n",
        "    # scale the image and convert it to a float array\n",
        "    width, height = image.size\n",
        "    sw, sh = math.ceil(width * scale), math.ceil(height * scale)\n",
        "    img = image.resize((sw, sh), Image.BILINEAR)\n",
        "    img = np.asarray(img, 'float32')\n",
        "\n",
        "    img = torch.FloatTensor(_preprocess(img)).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = net(img)\n",
        "        probs = output[1].cpu().data.numpy()[0, 1, :, :]\n",
        "        offsets = output[0].cpu().data.numpy()\n",
        "        # probs: probability of a face at each sliding window\n",
        "        # offsets: transformations to true bounding boxes\n",
        "\n",
        "        boxes = _generate_bboxes(probs, offsets, scale, threshold)\n",
        "        if len(boxes) == 0:\n",
        "            return None\n",
        "\n",
        "        keep = nms(boxes[:, 0:5], overlap_threshold=0.5)\n",
        "    return boxes[keep]\n",
        "\n",
        "\n",
        "def _generate_bboxes(probs, offsets, scale, threshold):\n",
        "    \"\"\"Generate bounding boxes at places\n",
        "    where there is probably a face.\n",
        "\n",
        "    Arguments:\n",
        "        probs: a float numpy array of shape [n, m].\n",
        "        offsets: a float numpy array of shape [1, 4, n, m].\n",
        "        scale: a float number,\n",
        "            width and height of the image were scaled by this number.\n",
        "        threshold: a float number.\n",
        "\n",
        "    Returns:\n",
        "        a float numpy array of shape [n_boxes, 9]\n",
        "    \"\"\"\n",
        "\n",
        "    # applying P-Net is equivalent, in some sense, to\n",
        "    # moving 12x12 window with stride 2\n",
        "    stride = 2\n",
        "    cell_size = 12\n",
        "\n",
        "    # indices of boxes where there is probably a face\n",
        "    inds = np.where(probs > threshold)\n",
        "\n",
        "    if inds[0].size == 0:\n",
        "        return np.array([])\n",
        "\n",
        "    # transformations of bounding boxes\n",
        "    tx1, ty1, tx2, ty2 = [offsets[0, i, inds[0], inds[1]] for i in range(4)]\n",
        "    # they are defined as:\n",
        "    # w = x2 - x1 + 1\n",
        "    # h = y2 - y1 + 1\n",
        "    # x1_true = x1 + tx1*w\n",
        "    # x2_true = x2 + tx2*w\n",
        "    # y1_true = y1 + ty1*h\n",
        "    # y2_true = y2 + ty2*h\n",
        "\n",
        "    offsets = np.array([tx1, ty1, tx2, ty2])\n",
        "    score = probs[inds[0], inds[1]]\n",
        "\n",
        "    # P-Net is applied to scaled images\n",
        "    # so we need to rescale bounding boxes back\n",
        "    bounding_boxes = np.vstack([\n",
        "        np.round((stride * inds[1] + 1.0) / scale),\n",
        "        np.round((stride * inds[0] + 1.0) / scale),\n",
        "        np.round((stride * inds[1] + 1.0 + cell_size) / scale),\n",
        "        np.round((stride * inds[0] + 1.0 + cell_size) / scale),\n",
        "        score, offsets\n",
        "    ])\n",
        "    # why one is added?\n",
        "\n",
        "    return bounding_boxes.T"
      ],
      "metadata": {
        "id": "XidYe-wXQ-3s"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models.mtcnn_pytorch.src.get_nets.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "\n",
        "# from configs.paths_config import model_paths\n",
        "PNET_PATH = model_paths[\"mtcnn_pnet\"]\n",
        "ONET_PATH = model_paths[\"mtcnn_onet\"]\n",
        "RNET_PATH = model_paths[\"mtcnn_rnet\"]\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: a float tensor with shape [batch_size, c, h, w].\n",
        "        Returns:\n",
        "            a float tensor with shape [batch_size, c*h*w].\n",
        "        \"\"\"\n",
        "\n",
        "        # without this pretrained model isn't working\n",
        "        x = x.transpose(3, 2).contiguous()\n",
        "\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "\n",
        "class PNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # suppose we have input with size HxW, then\n",
        "        # after first layer: H - 2,\n",
        "        # after pool: ceil((H - 2)/2),\n",
        "        # after second conv: ceil((H - 2)/2) - 2,\n",
        "        # after last conv: ceil((H - 2)/2) - 4,\n",
        "        # and the same for W\n",
        "\n",
        "        self.features = nn.Sequential(OrderedDict([\n",
        "            ('conv1', nn.Conv2d(3, 10, 3, 1)),\n",
        "            ('prelu1', nn.PReLU(10)),\n",
        "            ('pool1', nn.MaxPool2d(2, 2, ceil_mode=True)),\n",
        "\n",
        "            ('conv2', nn.Conv2d(10, 16, 3, 1)),\n",
        "            ('prelu2', nn.PReLU(16)),\n",
        "\n",
        "            ('conv3', nn.Conv2d(16, 32, 3, 1)),\n",
        "            ('prelu3', nn.PReLU(32))\n",
        "        ]))\n",
        "\n",
        "        self.conv4_1 = nn.Conv2d(32, 2, 1, 1)\n",
        "        self.conv4_2 = nn.Conv2d(32, 4, 1, 1)\n",
        "\n",
        "        weights = np.load(PNET_PATH, allow_pickle=True)[()]\n",
        "        for n, p in self.named_parameters():\n",
        "            p.data = torch.FloatTensor(weights[n])\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: a float tensor with shape [batch_size, 3, h, w].\n",
        "        Returns:\n",
        "            b: a float tensor with shape [batch_size, 4, h', w'].\n",
        "            a: a float tensor with shape [batch_size, 2, h', w'].\n",
        "        \"\"\"\n",
        "        x = self.features(x)\n",
        "        a = self.conv4_1(x)\n",
        "        b = self.conv4_2(x)\n",
        "        a = F.softmax(a, dim=-1)\n",
        "        return b, a\n",
        "\n",
        "\n",
        "class RNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = nn.Sequential(OrderedDict([\n",
        "            ('conv1', nn.Conv2d(3, 28, 3, 1)),\n",
        "            ('prelu1', nn.PReLU(28)),\n",
        "            ('pool1', nn.MaxPool2d(3, 2, ceil_mode=True)),\n",
        "\n",
        "            ('conv2', nn.Conv2d(28, 48, 3, 1)),\n",
        "            ('prelu2', nn.PReLU(48)),\n",
        "            ('pool2', nn.MaxPool2d(3, 2, ceil_mode=True)),\n",
        "\n",
        "            ('conv3', nn.Conv2d(48, 64, 2, 1)),\n",
        "            ('prelu3', nn.PReLU(64)),\n",
        "\n",
        "            ('flatten', Flatten()),\n",
        "            ('conv4', nn.Linear(576, 128)),\n",
        "            ('prelu4', nn.PReLU(128))\n",
        "        ]))\n",
        "\n",
        "        self.conv5_1 = nn.Linear(128, 2)\n",
        "        self.conv5_2 = nn.Linear(128, 4)\n",
        "\n",
        "        weights = np.load(RNET_PATH, allow_pickle=True)[()]\n",
        "        for n, p in self.named_parameters():\n",
        "            p.data = torch.FloatTensor(weights[n])\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: a float tensor with shape [batch_size, 3, h, w].\n",
        "        Returns:\n",
        "            b: a float tensor with shape [batch_size, 4].\n",
        "            a: a float tensor with shape [batch_size, 2].\n",
        "        \"\"\"\n",
        "        x = self.features(x)\n",
        "        a = self.conv5_1(x)\n",
        "        b = self.conv5_2(x)\n",
        "        a = F.softmax(a, dim=-1)\n",
        "        return b, a\n",
        "\n",
        "\n",
        "class ONet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = nn.Sequential(OrderedDict([\n",
        "            ('conv1', nn.Conv2d(3, 32, 3, 1)),\n",
        "            ('prelu1', nn.PReLU(32)),\n",
        "            ('pool1', nn.MaxPool2d(3, 2, ceil_mode=True)),\n",
        "\n",
        "            ('conv2', nn.Conv2d(32, 64, 3, 1)),\n",
        "            ('prelu2', nn.PReLU(64)),\n",
        "            ('pool2', nn.MaxPool2d(3, 2, ceil_mode=True)),\n",
        "\n",
        "            ('conv3', nn.Conv2d(64, 64, 3, 1)),\n",
        "            ('prelu3', nn.PReLU(64)),\n",
        "            ('pool3', nn.MaxPool2d(2, 2, ceil_mode=True)),\n",
        "\n",
        "            ('conv4', nn.Conv2d(64, 128, 2, 1)),\n",
        "            ('prelu4', nn.PReLU(128)),\n",
        "\n",
        "            ('flatten', Flatten()),\n",
        "            ('conv5', nn.Linear(1152, 256)),\n",
        "            ('drop5', nn.Dropout(0.25)),\n",
        "            ('prelu5', nn.PReLU(256)),\n",
        "        ]))\n",
        "\n",
        "        self.conv6_1 = nn.Linear(256, 2)\n",
        "        self.conv6_2 = nn.Linear(256, 4)\n",
        "        self.conv6_3 = nn.Linear(256, 10)\n",
        "\n",
        "        weights = np.load(ONET_PATH, allow_pickle=True)[()]\n",
        "        for n, p in self.named_parameters():\n",
        "            p.data = torch.FloatTensor(weights[n])\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: a float tensor with shape [batch_size, 3, h, w].\n",
        "        Returns:\n",
        "            c: a float tensor with shape [batch_size, 10].\n",
        "            b: a float tensor with shape [batch_size, 4].\n",
        "            a: a float tensor with shape [batch_size, 2].\n",
        "        \"\"\"\n",
        "        x = self.features(x)\n",
        "        a = self.conv6_1(x)\n",
        "        b = self.conv6_2(x)\n",
        "        c = self.conv6_3(x)\n",
        "        a = F.softmax(a, dim=-1)\n",
        "        return c, b, a"
      ],
      "metadata": {
        "id": "Zl36Q_z8RDfz"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models.mtcnn_pytorch.src.matlab_cp2tform.py\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Jul 11 06:54:28 2017\n",
        "\n",
        "@author: zhaoyafei\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from numpy.linalg import inv, norm, lstsq\n",
        "from numpy.linalg import matrix_rank as rank\n",
        "\n",
        "\n",
        "class MatlabCp2tormException(Exception):\n",
        "    def __str__(self):\n",
        "        return 'In File {}:{}'.format(\n",
        "            __file__, super.__str__(self))\n",
        "\n",
        "\n",
        "def tformfwd(trans, uv):\n",
        "    \"\"\"\n",
        "    Function:\n",
        "    ----------\n",
        "        apply affine transform 'trans' to uv\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "        @trans: 3x3 np.array\n",
        "            transform matrix\n",
        "        @uv: Kx2 np.array\n",
        "            each row is a pair of coordinates (x, y)\n",
        "\n",
        "    Returns:\n",
        "    ----------\n",
        "        @xy: Kx2 np.array\n",
        "            each row is a pair of transformed coordinates (x, y)\n",
        "    \"\"\"\n",
        "    uv = np.hstack((\n",
        "        uv, np.ones((uv.shape[0], 1))\n",
        "    ))\n",
        "    xy = np.dot(uv, trans)\n",
        "    xy = xy[:, 0:-1]\n",
        "    return xy\n",
        "\n",
        "\n",
        "def tforminv(trans, uv):\n",
        "    \"\"\"\n",
        "    Function:\n",
        "    ----------\n",
        "        apply the inverse of affine transform 'trans' to uv\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "        @trans: 3x3 np.array\n",
        "            transform matrix\n",
        "        @uv: Kx2 np.array\n",
        "            each row is a pair of coordinates (x, y)\n",
        "\n",
        "    Returns:\n",
        "    ----------\n",
        "        @xy: Kx2 np.array\n",
        "            each row is a pair of inverse-transformed coordinates (x, y)\n",
        "    \"\"\"\n",
        "    Tinv = inv(trans)\n",
        "    xy = tformfwd(Tinv, uv)\n",
        "    return xy\n",
        "\n",
        "\n",
        "def findNonreflectiveSimilarity(uv, xy, options=None):\n",
        "    options = {'K': 2}\n",
        "\n",
        "    K = options['K']\n",
        "    M = xy.shape[0]\n",
        "    x = xy[:, 0].reshape((-1, 1))  # use reshape to keep a column vector\n",
        "    y = xy[:, 1].reshape((-1, 1))  # use reshape to keep a column vector\n",
        "    # print('--->x, y:\\n', x, y\n",
        "\n",
        "    tmp1 = np.hstack((x, y, np.ones((M, 1)), np.zeros((M, 1))))\n",
        "    tmp2 = np.hstack((y, -x, np.zeros((M, 1)), np.ones((M, 1))))\n",
        "    X = np.vstack((tmp1, tmp2))\n",
        "    # print('--->X.shape: ', X.shape\n",
        "    # print('X:\\n', X\n",
        "\n",
        "    u = uv[:, 0].reshape((-1, 1))  # use reshape to keep a column vector\n",
        "    v = uv[:, 1].reshape((-1, 1))  # use reshape to keep a column vector\n",
        "    U = np.vstack((u, v))\n",
        "    # print('--->U.shape: ', U.shape\n",
        "    # print('U:\\n', U\n",
        "\n",
        "    # We know that X * r = U\n",
        "    if rank(X) >= 2 * K:\n",
        "        r, _, _, _ = lstsq(X, U, rcond=None)  # Make sure this is what I want\n",
        "        r = np.squeeze(r)\n",
        "    else:\n",
        "        raise Exception('cp2tform:twoUniquePointsReq')\n",
        "\n",
        "    # print('--->r:\\n', r\n",
        "\n",
        "    sc = r[0]\n",
        "    ss = r[1]\n",
        "    tx = r[2]\n",
        "    ty = r[3]\n",
        "\n",
        "    Tinv = np.array([\n",
        "        [sc, -ss, 0],\n",
        "        [ss, sc, 0],\n",
        "        [tx, ty, 1]\n",
        "    ])\n",
        "\n",
        "    # print('--->Tinv:\\n', Tinv\n",
        "\n",
        "    T = inv(Tinv)\n",
        "    # print('--->T:\\n', T\n",
        "\n",
        "    T[:, 2] = np.array([0, 0, 1])\n",
        "\n",
        "    return T, Tinv\n",
        "\n",
        "\n",
        "def findSimilarity(uv, xy, options=None):\n",
        "    options = {'K': 2}\n",
        "\n",
        "    #    uv = np.array(uv)\n",
        "    #    xy = np.array(xy)\n",
        "\n",
        "    # Solve for trans1\n",
        "    trans1, trans1_inv = findNonreflectiveSimilarity(uv, xy, options)\n",
        "\n",
        "    # Solve for trans2\n",
        "\n",
        "    # manually reflect the xy data across the Y-axis\n",
        "    xyR = xy\n",
        "    xyR[:, 0] = -1 * xyR[:, 0]\n",
        "\n",
        "    trans2r, trans2r_inv = findNonreflectiveSimilarity(uv, xyR, options)\n",
        "\n",
        "    # manually reflect the tform to undo the reflection done on xyR\n",
        "    TreflectY = np.array([\n",
        "        [-1, 0, 0],\n",
        "        [0, 1, 0],\n",
        "        [0, 0, 1]\n",
        "    ])\n",
        "\n",
        "    trans2 = np.dot(trans2r, TreflectY)\n",
        "\n",
        "    # Figure out if trans1 or trans2 is better\n",
        "    xy1 = tformfwd(trans1, uv)\n",
        "    norm1 = norm(xy1 - xy)\n",
        "\n",
        "    xy2 = tformfwd(trans2, uv)\n",
        "    norm2 = norm(xy2 - xy)\n",
        "\n",
        "    if norm1 <= norm2:\n",
        "        return trans1, trans1_inv\n",
        "    else:\n",
        "        trans2_inv = inv(trans2)\n",
        "        return trans2, trans2_inv\n",
        "\n",
        "\n",
        "def get_similarity_transform(src_pts, dst_pts, reflective=True):\n",
        "    \"\"\"\n",
        "    Function:\n",
        "    ----------\n",
        "        Find Similarity Transform Matrix 'trans':\n",
        "            u = src_pts[:, 0]\n",
        "            v = src_pts[:, 1]\n",
        "            x = dst_pts[:, 0]\n",
        "            y = dst_pts[:, 1]\n",
        "            [x, y, 1] = [u, v, 1] * trans\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "        @src_pts: Kx2 np.array\n",
        "            source points, each row is a pair of coordinates (x, y)\n",
        "        @dst_pts: Kx2 np.array\n",
        "            destination points, each row is a pair of transformed\n",
        "            coordinates (x, y)\n",
        "        @reflective: True or False\n",
        "            if True:\n",
        "                use reflective similarity transform\n",
        "            else:\n",
        "                use non-reflective similarity transform\n",
        "\n",
        "    Returns:\n",
        "    ----------\n",
        "       @trans: 3x3 np.array\n",
        "            transform matrix from uv to xy\n",
        "        trans_inv: 3x3 np.array\n",
        "            inverse of trans, transform matrix from xy to uv\n",
        "    \"\"\"\n",
        "\n",
        "    if reflective:\n",
        "        trans, trans_inv = findSimilarity(src_pts, dst_pts)\n",
        "    else:\n",
        "        trans, trans_inv = findNonreflectiveSimilarity(src_pts, dst_pts)\n",
        "\n",
        "    return trans, trans_inv\n",
        "\n",
        "\n",
        "def cvt_tform_mat_for_cv2(trans):\n",
        "    \"\"\"\n",
        "    Function:\n",
        "    ----------\n",
        "        Convert Transform Matrix 'trans' into 'cv2_trans' which could be\n",
        "        directly used by cv2.warpAffine():\n",
        "            u = src_pts[:, 0]\n",
        "            v = src_pts[:, 1]\n",
        "            x = dst_pts[:, 0]\n",
        "            y = dst_pts[:, 1]\n",
        "            [x, y].T = cv_trans * [u, v, 1].T\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "        @trans: 3x3 np.array\n",
        "            transform matrix from uv to xy\n",
        "\n",
        "    Returns:\n",
        "    ----------\n",
        "        @cv2_trans: 2x3 np.array\n",
        "            transform matrix from src_pts to dst_pts, could be directly used\n",
        "            for cv2.warpAffine()\n",
        "    \"\"\"\n",
        "    cv2_trans = trans[:, 0:2].T\n",
        "\n",
        "    return cv2_trans\n",
        "\n",
        "\n",
        "def get_similarity_transform_for_cv2(src_pts, dst_pts, reflective=True):\n",
        "    \"\"\"\n",
        "    Function:\n",
        "    ----------\n",
        "        Find Similarity Transform Matrix 'cv2_trans' which could be\n",
        "        directly used by cv2.warpAffine():\n",
        "            u = src_pts[:, 0]\n",
        "            v = src_pts[:, 1]\n",
        "            x = dst_pts[:, 0]\n",
        "            y = dst_pts[:, 1]\n",
        "            [x, y].T = cv_trans * [u, v, 1].T\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "        @src_pts: Kx2 np.array\n",
        "            source points, each row is a pair of coordinates (x, y)\n",
        "        @dst_pts: Kx2 np.array\n",
        "            destination points, each row is a pair of transformed\n",
        "            coordinates (x, y)\n",
        "        reflective: True or False\n",
        "            if True:\n",
        "                use reflective similarity transform\n",
        "            else:\n",
        "                use non-reflective similarity transform\n",
        "\n",
        "    Returns:\n",
        "    ----------\n",
        "        @cv2_trans: 2x3 np.array\n",
        "            transform matrix from src_pts to dst_pts, could be directly used\n",
        "            for cv2.warpAffine()\n",
        "    \"\"\"\n",
        "    trans, trans_inv = get_similarity_transform(src_pts, dst_pts, reflective)\n",
        "    cv2_trans = cvt_tform_mat_for_cv2(trans)\n",
        "\n",
        "    return cv2_trans\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \"\"\"\n",
        "    u = [0, 6, -2]\n",
        "    v = [0, 3, 5]\n",
        "    x = [-1, 0, 4]\n",
        "    y = [-1, -10, 4]\n",
        "\n",
        "    # In Matlab, run:\n",
        "    #\n",
        "    #   uv = [u'; v'];\n",
        "    #   xy = [x'; y'];\n",
        "    #   tform_sim=cp2tform(uv,xy,'similarity');\n",
        "    #\n",
        "    #   trans = tform_sim.tdata.T\n",
        "    #   ans =\n",
        "    #       -0.0764   -1.6190         0\n",
        "    #        1.6190   -0.0764         0\n",
        "    #       -3.2156    0.0290    1.0000\n",
        "    #   trans_inv = tform_sim.tdata.Tinv\n",
        "    #    ans =\n",
        "    #\n",
        "    #       -0.0291    0.6163         0\n",
        "    #       -0.6163   -0.0291         0\n",
        "    #       -0.0756    1.9826    1.0000\n",
        "    #    xy_m=tformfwd(tform_sim, u,v)\n",
        "    #\n",
        "    #    xy_m =\n",
        "    #\n",
        "    #       -3.2156    0.0290\n",
        "    #        1.1833   -9.9143\n",
        "    #        5.0323    2.8853\n",
        "    #    uv_m=tforminv(tform_sim, x,y)\n",
        "    #\n",
        "    #    uv_m =\n",
        "    #\n",
        "    #        0.5698    1.3953\n",
        "    #        6.0872    2.2733\n",
        "    #       -2.6570    4.3314\n",
        "    \"\"\"\n",
        "    u = [0, 6, -2]\n",
        "    v = [0, 3, 5]\n",
        "    x = [-1, 0, 4]\n",
        "    y = [-1, -10, 4]\n",
        "\n",
        "    uv = np.array((u, v)).T\n",
        "    xy = np.array((x, y)).T\n",
        "\n",
        "    print('\\n--->uv:')\n",
        "    print(uv)\n",
        "    print('\\n--->xy:')\n",
        "    print(xy)\n",
        "\n",
        "    trans, trans_inv = get_similarity_transform(uv, xy)\n",
        "\n",
        "    print('\\n--->trans matrix:')\n",
        "    print(trans)\n",
        "\n",
        "    print('\\n--->trans_inv matrix:')\n",
        "    print(trans_inv)\n",
        "\n",
        "    print('\\n---> apply transform to uv')\n",
        "    print('\\nxy_m = uv_augmented * trans')\n",
        "    uv_aug = np.hstack((\n",
        "        uv, np.ones((uv.shape[0], 1))\n",
        "    ))\n",
        "    xy_m = np.dot(uv_aug, trans)\n",
        "    print(xy_m)\n",
        "\n",
        "    print('\\nxy_m = tformfwd(trans, uv)')\n",
        "    xy_m = tformfwd(trans, uv)\n",
        "    print(xy_m)\n",
        "\n",
        "    print('\\n---> apply inverse transform to xy')\n",
        "    print('\\nuv_m = xy_augmented * trans_inv')\n",
        "    xy_aug = np.hstack((\n",
        "        xy, np.ones((xy.shape[0], 1))\n",
        "    ))\n",
        "    uv_m = np.dot(xy_aug, trans_inv)\n",
        "    print(uv_m)\n",
        "\n",
        "    print('\\nuv_m = tformfwd(trans_inv, xy)')\n",
        "    uv_m = tformfwd(trans_inv, xy)\n",
        "    print(uv_m)\n",
        "\n",
        "    uv_m = tforminv(trans, xy)\n",
        "    print('\\nuv_m = tforminv(trans, xy)')\n",
        "    print(uv_m)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh1VsJfeRGPl",
        "outputId": "97e94190-dbff-452f-e019-9555239c5e4c"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--->uv:\n",
            "[[ 0  0]\n",
            " [ 6  3]\n",
            " [-2  5]]\n",
            "\n",
            "--->xy:\n",
            "[[ -1  -1]\n",
            " [  0 -10]\n",
            " [  4   4]]\n",
            "\n",
            "--->trans matrix:\n",
            "[[-0.90974236 -2.05425695  0.        ]\n",
            " [-2.05425695  0.90974236  0.        ]\n",
            " [ 7.69100836 -2.0203037   1.        ]]\n",
            "\n",
            "--->trans_inv matrix:\n",
            "[[-0.18023256 -0.40697674  0.        ]\n",
            " [-0.40697674  0.18023256 -0.        ]\n",
            " [ 0.56395349  3.49418605  1.        ]]\n",
            "\n",
            "---> apply transform to uv\n",
            "\n",
            "xy_m = uv_augmented * trans\n",
            "[[  7.69100836  -2.0203037    1.        ]\n",
            " [ -3.93021669 -11.61661832   1.        ]\n",
            " [ -0.76079167   6.63692203   1.        ]]\n",
            "\n",
            "xy_m = tformfwd(trans, uv)\n",
            "[[  7.69100836  -2.0203037 ]\n",
            " [ -3.93021669 -11.61661832]\n",
            " [ -0.76079167   6.63692203]]\n",
            "\n",
            "---> apply inverse transform to xy\n",
            "\n",
            "uv_m = xy_augmented * trans_inv\n",
            "[[ 0.79069767  2.90697674  1.        ]\n",
            " [ 4.63372093  1.69186047  1.        ]\n",
            " [-0.34302326  5.84302326  1.        ]]\n",
            "\n",
            "uv_m = tformfwd(trans_inv, xy)\n",
            "[[ 0.79069767  2.90697674]\n",
            " [ 4.63372093  1.69186047]\n",
            " [-0.34302326  5.84302326]]\n",
            "\n",
            "uv_m = tforminv(trans, xy)\n",
            "[[ 0.79069767  2.90697674]\n",
            " [ 4.63372093  1.69186047]\n",
            " [-0.34302326  5.84302326]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# models.mtcnn_pytorch.src.visualization_utils.py\n",
        "from PIL import ImageDraw\n",
        "\n",
        "\n",
        "def show_bboxes(img, bounding_boxes, facial_landmarks=[]):\n",
        "    \"\"\"Draw bounding boxes and facial landmarks.\n",
        "\n",
        "    Arguments:\n",
        "        img: an instance of PIL.Image.\n",
        "        bounding_boxes: a float numpy array of shape [n, 5].\n",
        "        facial_landmarks: a float numpy array of shape [n, 10].\n",
        "\n",
        "    Returns:\n",
        "        an instance of PIL.Image.\n",
        "    \"\"\"\n",
        "\n",
        "    img_copy = img.copy()\n",
        "    draw = ImageDraw.Draw(img_copy)\n",
        "\n",
        "    for b in bounding_boxes:\n",
        "        draw.rectangle([\n",
        "            (b[0], b[1]), (b[2], b[3])\n",
        "        ], outline='white')\n",
        "\n",
        "    for p in facial_landmarks:\n",
        "        for i in range(5):\n",
        "            draw.ellipse([\n",
        "                (p[i] - 1.0, p[i + 5] - 1.0),\n",
        "                (p[i] + 1.0, p[i + 5] + 1.0)\n",
        "            ], outline='blue')\n",
        "\n",
        "    return img_copy"
      ],
      "metadata": {
        "id": "42ypcsSuRYdJ"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models.mtcnn.mtcnn_pytorch.src.weights.onet.npy. : numpy  \n"
      ],
      "metadata": {
        "id": "yN8OHBD7SB5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models.mtcnn.mtcnn_pytorch.src.weights.pnet.npy : numpy  \n"
      ],
      "metadata": {
        "id": "F4JRB2DOSCNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models.mtcnn.mtcnn_pytorch.src.weights.rnet.npy : numpy  \n"
      ],
      "metadata": {
        "id": "ZiZd7thMSHfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models.mtcnn.mtcnn.py\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "# from models.mtcnn.mtcnn_pytorch.src.get_nets import PNet, RNet, ONet\n",
        "# from models.mtcnn.mtcnn_pytorch.src.box_utils import nms, calibrate_box, get_image_boxes, convert_to_square\n",
        "# from models.mtcnn.mtcnn_pytorch.src.first_stage import run_first_stage\n",
        "# from models.mtcnn.mtcnn_pytorch.src.align_trans import get_reference_facial_points, warp_and_crop_face\n",
        "\n",
        "device = 'cuda:0'\n",
        "\n",
        "\n",
        "class MTCNN():\n",
        "    def __init__(self):\n",
        "        print(device)\n",
        "        self.pnet = PNet().to(device)\n",
        "        self.rnet = RNet().to(device)\n",
        "        self.onet = ONet().to(device)\n",
        "        self.pnet.eval()\n",
        "        self.rnet.eval()\n",
        "        self.onet.eval()\n",
        "        self.refrence = get_reference_facial_points(default_square=True)\n",
        "\n",
        "    def align(self, img):\n",
        "        _, landmarks = self.detect_faces(img)\n",
        "        if len(landmarks) == 0:\n",
        "            return None, None\n",
        "        facial5points = [[landmarks[0][j], landmarks[0][j + 5]] for j in range(5)]\n",
        "        warped_face, tfm = warp_and_crop_face(np.array(img), facial5points, self.refrence, crop_size=(112, 112))\n",
        "        return Image.fromarray(warped_face), tfm\n",
        "\n",
        "    def align_multi(self, img, limit=None, min_face_size=30.0):\n",
        "        boxes, landmarks = self.detect_faces(img, min_face_size)\n",
        "        if limit:\n",
        "            boxes = boxes[:limit]\n",
        "            landmarks = landmarks[:limit]\n",
        "        faces = []\n",
        "        tfms = []\n",
        "        for landmark in landmarks:\n",
        "            facial5points = [[landmark[j], landmark[j + 5]] for j in range(5)]\n",
        "            warped_face, tfm = warp_and_crop_face(np.array(img), facial5points, self.refrence, crop_size=(112, 112))\n",
        "            faces.append(Image.fromarray(warped_face))\n",
        "            tfms.append(tfm)\n",
        "        return boxes, faces, tfms\n",
        "\n",
        "    def detect_faces(self, image, min_face_size=20.0,\n",
        "                     thresholds=[0.15, 0.25, 0.35],\n",
        "                     nms_thresholds=[0.7, 0.7, 0.7]):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            image: an instance of PIL.Image.\n",
        "            min_face_size: a float number.\n",
        "            thresholds: a list of length 3.\n",
        "            nms_thresholds: a list of length 3.\n",
        "\n",
        "        Returns:\n",
        "            two float numpy arrays of shapes [n_boxes, 4] and [n_boxes, 10],\n",
        "            bounding boxes and facial landmarks.\n",
        "        \"\"\"\n",
        "\n",
        "        # BUILD AN IMAGE PYRAMID\n",
        "        width, height = image.size\n",
        "        min_length = min(height, width)\n",
        "\n",
        "        min_detection_size = 12\n",
        "        factor = 0.707  # sqrt(0.5)\n",
        "\n",
        "        # scales for scaling the image\n",
        "        scales = []\n",
        "\n",
        "        # scales the image so that\n",
        "        # minimum size that we can detect equals to\n",
        "        # minimum face size that we want to detect\n",
        "        m = min_detection_size / min_face_size\n",
        "        min_length *= m\n",
        "\n",
        "        factor_count = 0\n",
        "        while min_length > min_detection_size:\n",
        "            scales.append(m * factor ** factor_count)\n",
        "            min_length *= factor\n",
        "            factor_count += 1\n",
        "\n",
        "        # STAGE 1\n",
        "\n",
        "        # it will be returned\n",
        "        bounding_boxes = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # run P-Net on different scales\n",
        "            for s in scales:\n",
        "                boxes = run_first_stage(image, self.pnet, scale=s, threshold=thresholds[0])\n",
        "                bounding_boxes.append(boxes)\n",
        "\n",
        "            # collect boxes (and offsets, and scores) from different scales\n",
        "            bounding_boxes = [i for i in bounding_boxes if i is not None]\n",
        "            bounding_boxes = np.vstack(bounding_boxes)\n",
        "\n",
        "            keep = nms(bounding_boxes[:, 0:5], nms_thresholds[0])\n",
        "            bounding_boxes = bounding_boxes[keep]\n",
        "\n",
        "            # use offsets predicted by pnet to transform bounding boxes\n",
        "            bounding_boxes = calibrate_box(bounding_boxes[:, 0:5], bounding_boxes[:, 5:])\n",
        "            # shape [n_boxes, 5]\n",
        "\n",
        "            bounding_boxes = convert_to_square(bounding_boxes)\n",
        "            bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n",
        "\n",
        "            # STAGE 2\n",
        "\n",
        "            img_boxes = get_image_boxes(bounding_boxes, image, size=24)\n",
        "            img_boxes = torch.FloatTensor(img_boxes).to(device)\n",
        "\n",
        "            output = self.rnet(img_boxes)\n",
        "            offsets = output[0].cpu().data.numpy()  # shape [n_boxes, 4]\n",
        "            probs = output[1].cpu().data.numpy()  # shape [n_boxes, 2]\n",
        "\n",
        "            keep = np.where(probs[:, 1] > thresholds[1])[0]\n",
        "            bounding_boxes = bounding_boxes[keep]\n",
        "            bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\n",
        "            offsets = offsets[keep]\n",
        "\n",
        "            keep = nms(bounding_boxes, nms_thresholds[1])\n",
        "            bounding_boxes = bounding_boxes[keep]\n",
        "            bounding_boxes = calibrate_box(bounding_boxes, offsets[keep])\n",
        "            bounding_boxes = convert_to_square(bounding_boxes)\n",
        "            bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n",
        "\n",
        "            # STAGE 3\n",
        "\n",
        "            img_boxes = get_image_boxes(bounding_boxes, image, size=48)\n",
        "            if len(img_boxes) == 0:\n",
        "                return [], []\n",
        "            img_boxes = torch.FloatTensor(img_boxes).to(device)\n",
        "            output = self.onet(img_boxes)\n",
        "            landmarks = output[0].cpu().data.numpy()  # shape [n_boxes, 10]\n",
        "            offsets = output[1].cpu().data.numpy()  # shape [n_boxes, 4]\n",
        "            probs = output[2].cpu().data.numpy()  # shape [n_boxes, 2]\n",
        "\n",
        "            keep = np.where(probs[:, 1] > thresholds[2])[0]\n",
        "            bounding_boxes = bounding_boxes[keep]\n",
        "            bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\n",
        "            offsets = offsets[keep]\n",
        "            landmarks = landmarks[keep]\n",
        "\n",
        "            # compute landmark points\n",
        "            width = bounding_boxes[:, 2] - bounding_boxes[:, 0] + 1.0\n",
        "            height = bounding_boxes[:, 3] - bounding_boxes[:, 1] + 1.0\n",
        "            xmin, ymin = bounding_boxes[:, 0], bounding_boxes[:, 1]\n",
        "            landmarks[:, 0:5] = np.expand_dims(xmin, 1) + np.expand_dims(width, 1) * landmarks[:, 0:5]\n",
        "            landmarks[:, 5:10] = np.expand_dims(ymin, 1) + np.expand_dims(height, 1) * landmarks[:, 5:10]\n",
        "\n",
        "            bounding_boxes = calibrate_box(bounding_boxes, offsets)\n",
        "            keep = nms(bounding_boxes, nms_thresholds[2], mode='min')\n",
        "            bounding_boxes = bounding_boxes[keep]\n",
        "            landmarks = landmarks[keep]\n",
        "\n",
        "        return bounding_boxes, landmarks"
      ],
      "metadata": {
        "id": "xLMApieqSa-b"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models.stylegan2.model.py\n",
        "import math\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# from models.stylegan2.op import FusedLeakyReLU, fused_leaky_relu, upfirdn2d\n",
        "\n",
        "\n",
        "class PixelNorm(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input * torch.rsqrt(torch.mean(input ** 2, dim=1, keepdim=True) + 1e-8)\n",
        "\n",
        "\n",
        "def make_kernel(k):\n",
        "    k = torch.tensor(k, dtype=torch.float32)\n",
        "\n",
        "    if k.ndim == 1:\n",
        "        k = k[None, :] * k[:, None]\n",
        "\n",
        "    k /= k.sum()\n",
        "\n",
        "    return k\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, kernel, factor=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.factor = factor\n",
        "        kernel = make_kernel(kernel) * (factor ** 2)\n",
        "        self.register_buffer('kernel', kernel)\n",
        "\n",
        "        p = kernel.shape[0] - factor\n",
        "\n",
        "        pad0 = (p + 1) // 2 + factor - 1\n",
        "        pad1 = p // 2\n",
        "\n",
        "        self.pad = (pad0, pad1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = upfirdn2d(input, self.kernel, up=self.factor, down=1, pad=self.pad)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, kernel, factor=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.factor = factor\n",
        "        kernel = make_kernel(kernel)\n",
        "        self.register_buffer('kernel', kernel)\n",
        "\n",
        "        p = kernel.shape[0] - factor\n",
        "\n",
        "        pad0 = (p + 1) // 2\n",
        "        pad1 = p // 2\n",
        "\n",
        "        self.pad = (pad0, pad1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = upfirdn2d(input, self.kernel, up=1, down=self.factor, pad=self.pad)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Blur(nn.Module):\n",
        "    def __init__(self, kernel, pad, upsample_factor=1):\n",
        "        super().__init__()\n",
        "\n",
        "        kernel = make_kernel(kernel)\n",
        "\n",
        "        if upsample_factor > 1:\n",
        "            kernel = kernel * (upsample_factor ** 2)\n",
        "\n",
        "        self.register_buffer('kernel', kernel)\n",
        "\n",
        "        self.pad = pad\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = upfirdn2d(input, self.kernel, pad=self.pad)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class EqualConv2d(nn.Module):\n",
        "    def __init__(\n",
        "            self, in_channel, out_channel, kernel_size, stride=1, padding=0, bias=True\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.weight = nn.Parameter(\n",
        "            torch.randn(out_channel, in_channel, kernel_size, kernel_size)\n",
        "        )\n",
        "        self.scale = 1 / math.sqrt(in_channel * kernel_size ** 2)\n",
        "\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(out_channel))\n",
        "\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = F.conv2d(\n",
        "            input,\n",
        "            self.weight * self.scale,\n",
        "            bias=self.bias,\n",
        "            stride=self.stride,\n",
        "            padding=self.padding,\n",
        "        )\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (\n",
        "            f'{self.__class__.__name__}({self.weight.shape[1]}, {self.weight.shape[0]},'\n",
        "            f' {self.weight.shape[2]}, stride={self.stride}, padding={self.padding})'\n",
        "        )\n",
        "\n",
        "\n",
        "class EqualLinear(nn.Module):\n",
        "    def __init__(\n",
        "            self, in_dim, out_dim, bias=True, bias_init=0, lr_mul=1, activation=None\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.weight = nn.Parameter(torch.randn(out_dim, in_dim).div_(lr_mul))\n",
        "\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(out_dim).fill_(bias_init))\n",
        "\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "        self.activation = activation\n",
        "\n",
        "        self.scale = (1 / math.sqrt(in_dim)) * lr_mul\n",
        "        self.lr_mul = lr_mul\n",
        "\n",
        "    def forward(self, input):\n",
        "        weight = self.weight\n",
        "        if self.activation:\n",
        "            out = F.linear(input, weight * self.scale)\n",
        "            out = fused_leaky_relu(out, self.bias * self.lr_mul)\n",
        "\n",
        "        else:\n",
        "            out = F.linear(\n",
        "                input, weight * self.scale, bias=self.bias * self.lr_mul\n",
        "            )\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (\n",
        "            f'{self.__class__.__name__}({self.weight.shape[1]}, {self.weight.shape[0]})'\n",
        "        )\n",
        "\n",
        "\n",
        "class ScaledLeakyReLU(nn.Module):\n",
        "    def __init__(self, negative_slope=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.negative_slope = negative_slope\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = F.leaky_relu(input, negative_slope=self.negative_slope)\n",
        "\n",
        "        return out * math.sqrt(2)\n",
        "\n",
        "\n",
        "class ModulatedConv2d(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channel,\n",
        "            out_channel,\n",
        "            kernel_size,\n",
        "            style_dim,\n",
        "            demodulate=True,\n",
        "            upsample=False,\n",
        "            downsample=False,\n",
        "            blur_kernel=[1, 3, 3, 1],\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.eps = 1e-8\n",
        "        self.kernel_size = kernel_size\n",
        "        self.in_channel = in_channel\n",
        "        self.out_channel = out_channel\n",
        "        self.upsample = upsample\n",
        "        self.downsample = downsample\n",
        "\n",
        "        if upsample:\n",
        "            factor = 2\n",
        "            p = (len(blur_kernel) - factor) - (kernel_size - 1)\n",
        "            pad0 = (p + 1) // 2 + factor - 1\n",
        "            pad1 = p // 2 + 1\n",
        "\n",
        "            self.blur = Blur(blur_kernel, pad=(pad0, pad1), upsample_factor=factor)\n",
        "\n",
        "        if downsample:\n",
        "            factor = 2\n",
        "            p = (len(blur_kernel) - factor) + (kernel_size - 1)\n",
        "            pad0 = (p + 1) // 2\n",
        "            pad1 = p // 2\n",
        "\n",
        "            self.blur = Blur(blur_kernel, pad=(pad0, pad1))\n",
        "\n",
        "        fan_in = in_channel * kernel_size ** 2\n",
        "        self.scale = 1 / math.sqrt(fan_in)\n",
        "        self.padding = kernel_size // 2\n",
        "\n",
        "        self.weight = nn.Parameter(\n",
        "            torch.randn(1, out_channel, in_channel, kernel_size, kernel_size)\n",
        "        )\n",
        "\n",
        "        self.modulation = EqualLinear(style_dim, in_channel, bias_init=1)\n",
        "\n",
        "        self.demodulate = demodulate\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (\n",
        "            f'{self.__class__.__name__}({self.in_channel}, {self.out_channel}, {self.kernel_size}, '\n",
        "            f'upsample={self.upsample}, downsample={self.downsample})'\n",
        "        )\n",
        "\n",
        "    def forward(self, input, style, weights_delta=None):\n",
        "        batch, in_channel, height, width = input.shape\n",
        "\n",
        "        style = self.modulation(style).view(batch, 1, in_channel, 1, 1)\n",
        "        if weights_delta is None:\n",
        "            weight = self.scale * self.weight * style\n",
        "        else:\n",
        "            weight = self.scale * (self.weight * (1 + weights_delta) * style)\n",
        "\n",
        "        if self.demodulate:\n",
        "            demod = torch.rsqrt(weight.pow(2).sum([2, 3, 4]) + 1e-8)\n",
        "            weight = weight * demod.view(batch, self.out_channel, 1, 1, 1)\n",
        "\n",
        "        weight = weight.view(\n",
        "            batch * self.out_channel, in_channel, self.kernel_size, self.kernel_size\n",
        "        )\n",
        "\n",
        "        if self.upsample:\n",
        "            input = input.view(1, batch * in_channel, height, width)\n",
        "            weight = weight.view(\n",
        "                batch, self.out_channel, in_channel, self.kernel_size, self.kernel_size\n",
        "            )\n",
        "            weight = weight.transpose(1, 2).reshape(\n",
        "                batch * in_channel, self.out_channel, self.kernel_size, self.kernel_size\n",
        "            )\n",
        "            out = F.conv_transpose2d(input, weight, padding=0, stride=2, groups=batch)\n",
        "            _, _, height, width = out.shape\n",
        "            out = out.view(batch, self.out_channel, height, width)\n",
        "            out = self.blur(out)\n",
        "\n",
        "        elif self.downsample:\n",
        "            input = self.blur(input)\n",
        "            _, _, height, width = input.shape\n",
        "            input = input.view(1, batch * in_channel, height, width)\n",
        "            out = F.conv2d(input, weight, padding=0, stride=2, groups=batch)\n",
        "            _, _, height, width = out.shape\n",
        "            out = out.view(batch, self.out_channel, height, width)\n",
        "\n",
        "        else:\n",
        "            input = input.view(1, batch * in_channel, height, width)\n",
        "            out = F.conv2d(input, weight, padding=self.padding, groups=batch)\n",
        "            _, _, height, width = out.shape\n",
        "            out = out.view(batch, self.out_channel, height, width)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class NoiseInjection(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.weight = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, image, noise=None):\n",
        "        if noise is None:\n",
        "            batch, _, height, width = image.shape\n",
        "            noise = image.new_empty(batch, 1, height, width).normal_()\n",
        "\n",
        "        return image + self.weight * noise\n",
        "\n",
        "\n",
        "class ConstantInput(nn.Module):\n",
        "    def __init__(self, channel, size=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input = nn.Parameter(torch.randn(1, channel, size, size))\n",
        "\n",
        "    def forward(self, input):\n",
        "        batch = input.shape[0]\n",
        "        out = self.input.repeat(batch, 1, 1, 1)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class StyledConv(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channel,\n",
        "            out_channel,\n",
        "            kernel_size,\n",
        "            style_dim,\n",
        "            upsample=False,\n",
        "            blur_kernel=[1, 3, 3, 1],\n",
        "            demodulate=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = ModulatedConv2d(\n",
        "            in_channel,\n",
        "            out_channel,\n",
        "            kernel_size,\n",
        "            style_dim,\n",
        "            upsample=upsample,\n",
        "            blur_kernel=blur_kernel,\n",
        "            demodulate=demodulate,\n",
        "        )\n",
        "\n",
        "        self.noise = NoiseInjection()\n",
        "        # self.bias = nn.Parameter(torch.zeros(1, out_channel, 1, 1))\n",
        "        # self.activate = ScaledLeakyReLU(0.2)\n",
        "        self.activate = FusedLeakyReLU(out_channel)\n",
        "\n",
        "    def forward(self, input, style, noise=None, weights_delta=None):\n",
        "        out = self.conv(input, style, weights_delta=weights_delta)\n",
        "        out = self.noise(out, noise=noise)\n",
        "        # out = out + self.bias\n",
        "        out = self.activate(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ToRGB(nn.Module):\n",
        "    def __init__(self, in_channel, style_dim, upsample=True, blur_kernel=[1, 3, 3, 1]):\n",
        "        super().__init__()\n",
        "\n",
        "        if upsample:\n",
        "            self.upsample = Upsample(blur_kernel)\n",
        "\n",
        "        self.conv = ModulatedConv2d(in_channel, 3, 1, style_dim, demodulate=False)\n",
        "        self.bias = nn.Parameter(torch.zeros(1, 3, 1, 1))\n",
        "\n",
        "    def forward(self, input, style, skip=None, weights_delta=None):\n",
        "        out = self.conv(input, style, weights_delta)\n",
        "        out = out + self.bias\n",
        "\n",
        "        if skip is not None:\n",
        "            skip = self.upsample(skip)\n",
        "\n",
        "            out = out + skip\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            size,\n",
        "            style_dim,\n",
        "            n_mlp,\n",
        "            channel_multiplier=2,\n",
        "            blur_kernel=[1, 3, 3, 1],\n",
        "            lr_mlp=0.01\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.size = size\n",
        "\n",
        "        self.style_dim = style_dim\n",
        "\n",
        "        layers = [PixelNorm()]\n",
        "\n",
        "        for i in range(n_mlp):\n",
        "            layers.append(\n",
        "                EqualLinear(\n",
        "                    style_dim, style_dim, lr_mul=lr_mlp, activation='fused_lrelu'\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.style = nn.Sequential(*layers)\n",
        "\n",
        "        self.channels = {\n",
        "            4: 512,\n",
        "            8: 512,\n",
        "            16: 512,\n",
        "            32: 512,\n",
        "            64: 256 * channel_multiplier,\n",
        "            128: 128 * channel_multiplier,\n",
        "            256: 64 * channel_multiplier,\n",
        "            512: 32 * channel_multiplier,\n",
        "            1024: 16 * channel_multiplier,\n",
        "        }\n",
        "\n",
        "        self.input = ConstantInput(self.channels[4])\n",
        "        self.conv1 = StyledConv(\n",
        "            self.channels[4], self.channels[4], 3, style_dim, blur_kernel=blur_kernel\n",
        "        )\n",
        "        self.to_rgb1 = ToRGB(self.channels[4], style_dim, upsample=False)\n",
        "\n",
        "        self.log_size = int(math.log(size, 2))\n",
        "        self.num_layers = (self.log_size - 2) * 2 + 1\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.upsamples = nn.ModuleList()\n",
        "        self.to_rgbs = nn.ModuleList()\n",
        "        self.noises = nn.Module()\n",
        "\n",
        "        in_channel = self.channels[4]\n",
        "\n",
        "        for layer_idx in range(self.num_layers):\n",
        "            res = (layer_idx + 5) // 2\n",
        "            shape = [1, 1, 2 ** res, 2 ** res]\n",
        "            self.noises.register_buffer(f'noise_{layer_idx}', torch.randn(*shape))\n",
        "\n",
        "        for i in range(3, self.log_size + 1):\n",
        "            out_channel = self.channels[2 ** i]\n",
        "\n",
        "            self.convs.append(\n",
        "                StyledConv(\n",
        "                    in_channel,\n",
        "                    out_channel,\n",
        "                    3,\n",
        "                    style_dim,\n",
        "                    upsample=True,\n",
        "                    blur_kernel=blur_kernel,\n",
        "                )\n",
        "            )\n",
        "\n",
        "            self.convs.append(\n",
        "                StyledConv(\n",
        "                    out_channel, out_channel, 3, style_dim, blur_kernel=blur_kernel\n",
        "                )\n",
        "            )\n",
        "\n",
        "            self.to_rgbs.append(ToRGB(out_channel, style_dim))\n",
        "\n",
        "            in_channel = out_channel\n",
        "\n",
        "        self.n_latent = self.log_size * 2 - 2\n",
        "\n",
        "    def make_noise(self):\n",
        "        device = self.input.input.device\n",
        "\n",
        "        noises = [torch.randn(1, 1, 2 ** 2, 2 ** 2, device=device)]\n",
        "\n",
        "        for i in range(3, self.log_size + 1):\n",
        "            for _ in range(2):\n",
        "                noises.append(torch.randn(1, 1, 2 ** i, 2 ** i, device=device))\n",
        "\n",
        "        return noises\n",
        "\n",
        "    def mean_latent(self, n_latent):\n",
        "        latent_in = torch.randn(\n",
        "            n_latent, self.style_dim, device=self.input.input.device\n",
        "        )\n",
        "        latent = self.style(latent_in).mean(0, keepdim=True)\n",
        "\n",
        "        return latent\n",
        "\n",
        "    def get_latent(self, input):\n",
        "        return self.style(input)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            styles,\n",
        "            return_latents=False,\n",
        "            return_features=False,\n",
        "            inject_index=None,\n",
        "            truncation=1,\n",
        "            truncation_latent=None,\n",
        "            input_is_latent=False,\n",
        "            noise=None,\n",
        "            randomize_noise=True,\n",
        "            weights_deltas=None\n",
        "    ):\n",
        "        total_convs = len(self.convs) + len(self.to_rgbs) + 2   # +2 for first conv and toRGB\n",
        "        if weights_deltas is None:\n",
        "            weights_deltas = [None] * total_convs\n",
        "\n",
        "        if not input_is_latent:\n",
        "            styles = [self.style(s) for s in styles]\n",
        "\n",
        "        if noise is None:\n",
        "            if randomize_noise:\n",
        "                noise = [None] * self.num_layers\n",
        "            else:\n",
        "                noise = [\n",
        "                    getattr(self.noises, f'noise_{i}') for i in range(self.num_layers)\n",
        "                ]\n",
        "\n",
        "        if truncation < 1:\n",
        "            style_t = []\n",
        "\n",
        "            for style in styles:\n",
        "                style_t.append(\n",
        "                    truncation_latent + truncation * (style - truncation_latent)\n",
        "                )\n",
        "\n",
        "            styles = style_t\n",
        "\n",
        "        if len(styles) < 2:\n",
        "            inject_index = self.n_latent\n",
        "\n",
        "            if styles[0].ndim < 3:\n",
        "                latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n",
        "            else:\n",
        "                latent = styles[0]\n",
        "\n",
        "        else:\n",
        "            if inject_index is None:\n",
        "                inject_index = random.randint(1, self.n_latent - 1)\n",
        "\n",
        "            latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n",
        "            latent2 = styles[1].unsqueeze(1).repeat(1, self.n_latent - inject_index, 1)\n",
        "\n",
        "            latent = torch.cat([latent, latent2], 1)\n",
        "\n",
        "        out = self.input(latent)\n",
        "        out = self.conv1(out, latent[:, 0], noise=noise[0], weights_delta=weights_deltas[0])\n",
        "        skip = self.to_rgb1(out, latent[:, 1], weights_delta=weights_deltas[1])\n",
        "\n",
        "        i = 1\n",
        "        weight_idx = 2\n",
        "        for conv1, conv2, noise1, noise2, to_rgb in zip(self.convs[::2], self.convs[1::2], noise[1::2], noise[2::2], self.to_rgbs):\n",
        "            out = conv1(out, latent[:, i], noise=noise1, weights_delta=weights_deltas[weight_idx])\n",
        "            out = conv2(out, latent[:, i + 1], noise=noise2, weights_delta=weights_deltas[weight_idx + 1])\n",
        "            skip = to_rgb(out, latent[:, i + 2], skip, weights_delta=weights_deltas[weight_idx + 2])\n",
        "\n",
        "            i += 2\n",
        "            weight_idx += 3\n",
        "\n",
        "        image = skip\n",
        "\n",
        "        if return_latents:\n",
        "            return image, latent\n",
        "        elif return_features:\n",
        "            return image, out\n",
        "        else:\n",
        "            return image, None\n",
        "\n",
        "\n",
        "class ConvLayer(nn.Sequential):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channel,\n",
        "            out_channel,\n",
        "            kernel_size,\n",
        "            downsample=False,\n",
        "            blur_kernel=[1, 3, 3, 1],\n",
        "            bias=True,\n",
        "            activate=True,\n",
        "    ):\n",
        "        layers = []\n",
        "\n",
        "        if downsample:\n",
        "            factor = 2\n",
        "            p = (len(blur_kernel) - factor) + (kernel_size - 1)\n",
        "            pad0 = (p + 1) // 2\n",
        "            pad1 = p // 2\n",
        "\n",
        "            layers.append(Blur(blur_kernel, pad=(pad0, pad1)))\n",
        "\n",
        "            stride = 2\n",
        "            self.padding = 0\n",
        "\n",
        "        else:\n",
        "            stride = 1\n",
        "            self.padding = kernel_size // 2\n",
        "\n",
        "        layers.append(\n",
        "            EqualConv2d(\n",
        "                in_channel,\n",
        "                out_channel,\n",
        "                kernel_size,\n",
        "                padding=self.padding,\n",
        "                stride=stride,\n",
        "                bias=bias and not activate,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        if activate:\n",
        "            if bias:\n",
        "                layers.append(FusedLeakyReLU(out_channel))\n",
        "\n",
        "            else:\n",
        "                layers.append(ScaledLeakyReLU(0.2))\n",
        "\n",
        "        super().__init__(*layers)\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, blur_kernel=[1, 3, 3, 1]):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = ConvLayer(in_channel, in_channel, 3)\n",
        "        self.conv2 = ConvLayer(in_channel, out_channel, 3, downsample=True)\n",
        "\n",
        "        self.skip = ConvLayer(\n",
        "            in_channel, out_channel, 1, downsample=True, activate=False, bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.conv1(input)\n",
        "        out = self.conv2(out)\n",
        "\n",
        "        skip = self.skip(input)\n",
        "        out = (out + skip) / math.sqrt(2)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, size, channel_multiplier=2, blur_kernel=[1, 3, 3, 1]):\n",
        "        super().__init__()\n",
        "\n",
        "        channels = {\n",
        "            4: 512,\n",
        "            8: 512,\n",
        "            16: 512,\n",
        "            32: 512,\n",
        "            64: 256 * channel_multiplier,\n",
        "            128: 128 * channel_multiplier,\n",
        "            256: 64 * channel_multiplier,\n",
        "            512: 32 * channel_multiplier,\n",
        "            1024: 16 * channel_multiplier,\n",
        "        }\n",
        "\n",
        "        convs = [ConvLayer(3, channels[size], 1)]\n",
        "\n",
        "        log_size = int(math.log(size, 2))\n",
        "\n",
        "        in_channel = channels[size]\n",
        "\n",
        "        for i in range(log_size, 2, -1):\n",
        "            out_channel = channels[2 ** (i - 1)]\n",
        "\n",
        "            convs.append(ResBlock(in_channel, out_channel, blur_kernel))\n",
        "\n",
        "            in_channel = out_channel\n",
        "\n",
        "        self.convs = nn.Sequential(*convs)\n",
        "\n",
        "        self.stddev_group = 4\n",
        "        self.stddev_feat = 1\n",
        "\n",
        "        self.final_conv = ConvLayer(in_channel + 1, channels[4], 3)\n",
        "        self.final_linear = nn.Sequential(\n",
        "            EqualLinear(channels[4] * 4 * 4, channels[4], activation='fused_lrelu'),\n",
        "            EqualLinear(channels[4], 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.convs(input)\n",
        "\n",
        "        batch, channel, height, width = out.shape\n",
        "        group = min(batch, self.stddev_group)\n",
        "        stddev = out.view(\n",
        "            group, -1, self.stddev_feat, channel // self.stddev_feat, height, width\n",
        "        )\n",
        "        stddev = torch.sqrt(stddev.var(0, unbiased=False) + 1e-8)\n",
        "        stddev = stddev.mean([2, 3, 4], keepdims=True).squeeze(2)\n",
        "        stddev = stddev.repeat(group, 1, height, width)\n",
        "        out = torch.cat([out, stddev], 1)\n",
        "\n",
        "        out = self.final_conv(out)\n",
        "\n",
        "        out = out.view(batch, -1)\n",
        "        out = self.final_linear(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "kLOLUwU8TAG3"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models.stylegan2.op.fused_act.py\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Function\n",
        "from torch.utils.cpp_extension import load\n",
        "\n",
        "module_path = os.path.dirname(__file__)\n",
        "fused = load(\n",
        "    'fused',\n",
        "    sources=[\n",
        "        os.path.join(module_path, 'fused_bias_act.cpp'),\n",
        "        os.path.join(module_path, 'fused_bias_act_kernel.cu'),\n",
        "    ],\n",
        ")\n",
        "\n",
        "\n",
        "class FusedLeakyReLUFunctionBackward(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, grad_output, out, negative_slope, scale):\n",
        "        ctx.save_for_backward(out)\n",
        "        ctx.negative_slope = negative_slope\n",
        "        ctx.scale = scale\n",
        "\n",
        "        empty = grad_output.new_empty(0)\n",
        "\n",
        "        grad_input = fused.fused_bias_act(\n",
        "            grad_output, empty, out, 3, 1, negative_slope, scale\n",
        "        )\n",
        "\n",
        "        dim = [0]\n",
        "\n",
        "        if grad_input.ndim > 2:\n",
        "            dim += list(range(2, grad_input.ndim))\n",
        "\n",
        "        grad_bias = grad_input.sum(dim).detach()\n",
        "\n",
        "        return grad_input, grad_bias\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, gradgrad_input, gradgrad_bias):\n",
        "        out, = ctx.saved_tensors\n",
        "        gradgrad_out = fused.fused_bias_act(\n",
        "            gradgrad_input, gradgrad_bias, out, 3, 1, ctx.negative_slope, ctx.scale\n",
        "        )\n",
        "\n",
        "        return gradgrad_out, None, None, None\n",
        "\n",
        "\n",
        "class FusedLeakyReLUFunction(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, bias, negative_slope, scale):\n",
        "        empty = input.new_empty(0)\n",
        "        out = fused.fused_bias_act(input, bias, empty, 3, 0, negative_slope, scale)\n",
        "        ctx.save_for_backward(out)\n",
        "        ctx.negative_slope = negative_slope\n",
        "        ctx.scale = scale\n",
        "\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        out, = ctx.saved_tensors\n",
        "\n",
        "        grad_input, grad_bias = FusedLeakyReLUFunctionBackward.apply(\n",
        "            grad_output, out, ctx.negative_slope, ctx.scale\n",
        "        )\n",
        "\n",
        "        return grad_input, grad_bias, None, None\n",
        "\n",
        "\n",
        "class FusedLeakyReLU(nn.Module):\n",
        "    def __init__(self, channel, negative_slope=0.2, scale=2 ** 0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.bias = nn.Parameter(torch.zeros(channel))\n",
        "        self.negative_slope = negative_slope\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, input):\n",
        "        return fused_leaky_relu(input, self.bias, self.negative_slope, self.scale)\n",
        "\n",
        "\n",
        "def fused_leaky_relu(input, bias, negative_slope=0.2, scale=2 ** 0.5):\n",
        "    return FusedLeakyReLUFunction.apply(input, bias, negative_slope, scale)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "YORSbTOdTAQ_",
        "outputId": "9432fbf3-fdf5-4abd-fa92-855f1d17e57f"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-ece80954b43b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpp_extension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodule_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m fused = load(\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m'fused'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDlcKRa2UcTc",
        "outputId": "c726421b-a169-4143-dc4f-c90ddd850048"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-30 06:34:53--  https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220630%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220630T063454Z&X-Amz-Expires=300&X-Amz-Signature=9e54e1790c76feda672c9e0bc837387796616fe86463450fe5a809ac4559c64d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=1335132&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-06-30 06:34:54--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220630%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220630T063454Z&X-Amz-Expires=300&X-Amz-Signature=9e54e1790c76feda672c9e0bc837387796616fe86463450fe5a809ac4559c64d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=1335132&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 77854 (76K) [application/octet-stream]\n",
            "Saving to: ninja-linux.zip\n",
            "\n",
            "ninja-linux.zip     100%[===================>]  76.03K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2022-06-30 06:34:54 (8.92 MB/s) - ninja-linux.zip saved [77854/77854]\n",
            "\n",
            "Archive:  ninja-linux.zip\n",
            "  inflating: /usr/local/bin/ninja    \n",
            "update-alternatives: using /usr/local/bin/ninja to provide /usr/bin/ninja (ninja) in auto mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# models.stylegan2.op.fused_bias_act.py"
      ],
      "metadata": {
        "id": "8Spc1oLqTAYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models.stylegan2.op.upfirdn2d.cpp"
      ],
      "metadata": {
        "id": "iTktdrMgTMqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models.stylegan2.op.upfirdn2d.py"
      ],
      "metadata": {
        "id": "LimEzVU8TM0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models.stylegan2.op.upfirdn2d_kernel.cu"
      ],
      "metadata": {
        "id": "PhLFcep0TM8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models.hyperstyle.py\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "import copy\n",
        "from argparse import Namespace\n",
        "\n",
        "# from models.encoders.psp import pSp\n",
        "# from models.stylegan2.model import Generator\n",
        "# from configs.paths_config import model_paths\n",
        "# from models.hypernetworks.hypernetwork import SharedWeightsHyperNetResNet, SharedWeightsHyperNetResNetSeparable\n",
        "# from utils.resnet_mapping import RESNET_MAPPING\n",
        "\n",
        "\n",
        "class HyperStyle(nn.Module):\n",
        "\n",
        "    def __init__(self, opts):\n",
        "        super(HyperStyle, self).__init__()\n",
        "        self.set_opts(opts)\n",
        "        self.n_styles = int(math.log(self.opts.output_size, 2)) * 2 - 2\n",
        "        # Define architecture\n",
        "        self.hypernet = self.set_hypernet()\n",
        "        self.decoder = Generator(self.opts.output_size, 512, 8, channel_multiplier=2)\n",
        "        self.face_pool = torch.nn.AdaptiveAvgPool2d((256, 256))\n",
        "        # Load weights if needed\n",
        "        self.load_weights()\n",
        "        if self.opts.load_w_encoder:\n",
        "            self.w_encoder.eval()\n",
        "\n",
        "    def set_hypernet(self):\n",
        "        if self.opts.output_size == 1024:\n",
        "            self.opts.n_hypernet_outputs = 26\n",
        "        elif self.opts.output_size == 512:\n",
        "            self.opts.n_hypernet_outputs = 23\n",
        "        elif self.opts.output_size == 256:\n",
        "            self.opts.n_hypernet_outputs = 20\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid Output Size! Support sizes: [1024, 512, 256]!\")\n",
        "        networks = {\n",
        "            \"SharedWeightsHyperNetResNet\": SharedWeightsHyperNetResNet(opts=self.opts),\n",
        "            \"SharedWeightsHyperNetResNetSeparable\": SharedWeightsHyperNetResNetSeparable(opts=self.opts),\n",
        "        }\n",
        "        return networks[self.opts.encoder_type]\n",
        "\n",
        "    def load_weights(self):\n",
        "        if self.opts.checkpoint_path is not None:\n",
        "            print(f'Loading HyperStyle from checkpoint: {self.opts.checkpoint_path}')\n",
        "            ckpt = torch.load(self.opts.checkpoint_path, map_location='cpu')\n",
        "            self.hypernet.load_state_dict(self.__get_keys(ckpt, 'hypernet'), strict=True)\n",
        "            self.decoder.load_state_dict(self.__get_keys(ckpt, 'decoder'), strict=True)\n",
        "            self.__load_latent_avg(ckpt)\n",
        "            if self.opts.load_w_encoder:\n",
        "                self.w_encoder = self.__get_pretrained_w_encoder()\n",
        "        else:\n",
        "            hypernet_ckpt = self.__get_hypernet_checkpoint()\n",
        "            self.hypernet.load_state_dict(hypernet_ckpt, strict=False)\n",
        "            print(f'Loading decoder weights from pretrained path: {self.opts.stylegan_weights}')\n",
        "            ckpt = torch.load(self.opts.stylegan_weights)\n",
        "            self.decoder.load_state_dict(ckpt['g_ema'], strict=True)\n",
        "            self.__load_latent_avg(ckpt, repeat=self.n_styles)\n",
        "            if self.opts.load_w_encoder:\n",
        "                self.w_encoder = self.__get_pretrained_w_encoder()\n",
        "\n",
        "    def forward(self, x, resize=True, input_code=False, randomize_noise=True, return_latents=False,\n",
        "                return_weight_deltas_and_codes=False, weights_deltas=None, y_hat=None, codes=None):\n",
        "\n",
        "        if input_code:\n",
        "            codes = x\n",
        "        else:\n",
        "            if y_hat is None:\n",
        "                assert self.opts.load_w_encoder, \"Cannot infer latent code when e4e isn't loaded.\"\n",
        "                y_hat, codes = self.__get_initial_inversion(x, resize=True)\n",
        "\n",
        "            # concatenate original input with w-reconstruction or current reconstruction\n",
        "            x_input = torch.cat([x, y_hat], dim=1)\n",
        "\n",
        "            # pass through hypernet to get per-layer deltas\n",
        "            hypernet_outputs = self.hypernet(x_input)\n",
        "            if weights_deltas is None:\n",
        "                weights_deltas = hypernet_outputs\n",
        "            else:\n",
        "                weights_deltas = [weights_deltas[i] + hypernet_outputs[i] if weights_deltas[i] is not None else None\n",
        "                                  for i in range(len(hypernet_outputs))]\n",
        "\n",
        "        input_is_latent = (not input_code)\n",
        "        images, result_latent = self.decoder([codes],\n",
        "                                             weights_deltas=weights_deltas,\n",
        "                                             input_is_latent=input_is_latent,\n",
        "                                             randomize_noise=randomize_noise,\n",
        "                                             return_latents=return_latents)\n",
        "\n",
        "        if resize:\n",
        "            images = self.face_pool(images)\n",
        "\n",
        "        if return_latents and return_weight_deltas_and_codes:\n",
        "            return images, result_latent, weights_deltas, codes, y_hat\n",
        "        elif return_latents:\n",
        "            return images, result_latent\n",
        "        elif return_weight_deltas_and_codes:\n",
        "            return images, weights_deltas, codes\n",
        "        else:\n",
        "            return images\n",
        "\n",
        "    def set_opts(self, opts):\n",
        "        self.opts = opts\n",
        "\n",
        "    def __load_latent_avg(self, ckpt, repeat=None):\n",
        "        if 'latent_avg' in ckpt:\n",
        "            self.latent_avg = ckpt['latent_avg'].to(self.opts.device)\n",
        "            if repeat is not None:\n",
        "                self.latent_avg = self.latent_avg.repeat(repeat, 1)\n",
        "        else:\n",
        "            self.latent_avg = None\n",
        "\n",
        "    def __get_hypernet_checkpoint(self):\n",
        "        print('Loading hypernet weights from resnet34!')\n",
        "        hypernet_ckpt = torch.load(model_paths['resnet34'])\n",
        "        # Transfer the RGB input of the resnet34 network to the first 3 input channels of hypernet\n",
        "        if self.opts.input_nc != 3:\n",
        "            shape = hypernet_ckpt['conv1.weight'].shape\n",
        "            altered_input_layer = torch.randn(shape[0], self.opts.input_nc, shape[2], shape[3], dtype=torch.float32)\n",
        "            altered_input_layer[:, :3, :, :] = hypernet_ckpt['conv1.weight']\n",
        "            hypernet_ckpt['conv1.weight'] = altered_input_layer\n",
        "        mapped_hypernet_ckpt = dict(hypernet_ckpt)\n",
        "        for p, v in hypernet_ckpt.items():\n",
        "            for original_name, net_name in RESNET_MAPPING.items():\n",
        "                if original_name in p:\n",
        "                    mapped_hypernet_ckpt[p.replace(original_name, net_name)] = v\n",
        "                    mapped_hypernet_ckpt.pop(p)\n",
        "        return hypernet_ckpt\n",
        "\n",
        "    @staticmethod\n",
        "    def __get_keys(d, name):\n",
        "        if 'state_dict' in d:\n",
        "            d = d['state_dict']\n",
        "        d_filt = {k[len(name) + 1:]: v for k, v in d.items() if k[:len(name)] == name}\n",
        "        return d_filt\n",
        "\n",
        "    def __get_pretrained_w_encoder(self):\n",
        "        print(\"Loading pretrained W encoder...\")\n",
        "        opts_w_encoder = vars(copy.deepcopy(self.opts))\n",
        "        opts_w_encoder['checkpoint_path'] = self.opts.w_encoder_checkpoint_path\n",
        "        opts_w_encoder['encoder_type'] = self.opts.w_encoder_type\n",
        "        opts_w_encoder['input_nc'] = 3\n",
        "        opts_w_encoder = Namespace(**opts_w_encoder)\n",
        "        w_net = pSp(opts_w_encoder)\n",
        "        w_net = w_net.encoder\n",
        "        w_net.eval()\n",
        "        w_net.cuda()\n",
        "        return w_net\n",
        "\n",
        "    def __get_initial_inversion(self, x, resize=True):\n",
        "        # get initial inversion and reconstruction of batch\n",
        "        with torch.no_grad():\n",
        "            return self.__get_w_inversion(x, resize)\n",
        "\n",
        "    def __get_w_inversion(self, x, resize=True):\n",
        "        if self.w_encoder.training:\n",
        "            self.w_encoder.eval()\n",
        "        codes = self.w_encoder.forward(x)\n",
        "        if codes.ndim == 2:\n",
        "            codes = codes + self.latent_avg.repeat(codes.shape[0], 1, 1)[:, 0, :]\n",
        "        else:\n",
        "            codes = codes + self.latent_avg.repeat(codes.shape[0], 1, 1)\n",
        "        y_hat, _ = self.decoder([codes],\n",
        "                                weights_deltas=None,\n",
        "                                input_is_latent=True,\n",
        "                                randomize_noise=False,\n",
        "                                return_latents=False)\n",
        "        if resize:\n",
        "            y_hat = self.face_pool(y_hat)\n",
        "        if \"cars\" in self.opts.dataset_type:\n",
        "            y_hat = y_hat[:, :, 32:224, :]\n",
        "        return y_hat, codes"
      ],
      "metadata": {
        "id": "ebS8Xx3wIEnU"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# criteria.ms_ssim.py\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from math import exp\n",
        "import numpy as np\n",
        "\n",
        "\"\"\"\n",
        "Taken from https://github.com/jorge-pessoa/pytorch-msssim\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def gaussian(window_size, sigma):\n",
        "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
        "    return gauss/gauss.sum()\n",
        "\n",
        "\n",
        "def create_window(window_size, channel=1):\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
        "    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
        "    return window\n",
        "\n",
        "\n",
        "def ssim(img1, img2, window_size=11, window=None, size_average=True, full=False, val_range=None):\n",
        "    # Value range can be different from 255. Other common ranges are 1 (sigmoid) and 2 (tanh).\n",
        "    if val_range is None:\n",
        "        if torch.max(img1) > 128:\n",
        "            max_val = 255\n",
        "        else:\n",
        "            max_val = 1\n",
        "\n",
        "        if torch.min(img1) < -0.5:\n",
        "            min_val = -1\n",
        "        else:\n",
        "            min_val = 0\n",
        "        L = max_val - min_val\n",
        "    else:\n",
        "        L = val_range\n",
        "\n",
        "    padd = 0\n",
        "    (_, channel, height, width) = img1.size()\n",
        "    if window is None:\n",
        "        real_size = min(window_size, height, width)\n",
        "        window = create_window(real_size, channel=channel).to(img1.device)\n",
        "\n",
        "    mu1 = F.conv2d(img1, window, padding=padd, groups=channel)\n",
        "    mu2 = F.conv2d(img2, window, padding=padd, groups=channel)\n",
        "\n",
        "    mu1_sq = mu1.pow(2)\n",
        "    mu2_sq = mu2.pow(2)\n",
        "    mu1_mu2 = mu1 * mu2\n",
        "\n",
        "    sigma1_sq = F.conv2d(img1 * img1, window, padding=padd, groups=channel) - mu1_sq\n",
        "    sigma2_sq = F.conv2d(img2 * img2, window, padding=padd, groups=channel) - mu2_sq\n",
        "    sigma12 = F.conv2d(img1 * img2, window, padding=padd, groups=channel) - mu1_mu2\n",
        "\n",
        "    C1 = (0.01 * L) ** 2\n",
        "    C2 = (0.03 * L) ** 2\n",
        "\n",
        "    v1 = 2.0 * sigma12 + C2\n",
        "    v2 = sigma1_sq + sigma2_sq + C2\n",
        "    cs = v1 / v2  # contrast sensitivity\n",
        "\n",
        "    ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)\n",
        "\n",
        "    if size_average:\n",
        "        cs = cs.mean()\n",
        "        ret = ssim_map.mean()\n",
        "    else:\n",
        "        cs = cs.mean(1).mean(1).mean(1)\n",
        "        ret = ssim_map.mean(1).mean(1).mean(1)\n",
        "\n",
        "    if full:\n",
        "        return ret, cs\n",
        "    return ret\n",
        "\n",
        "\n",
        "def msssim(img1, img2, window_size=11, size_average=True, val_range=None, normalize=None):\n",
        "    device = img1.device\n",
        "    weights = torch.FloatTensor([0.0448, 0.2856, 0.3001, 0.2363, 0.1333]).to(device)\n",
        "    levels = weights.size()[0]\n",
        "    ssims = []\n",
        "    mcs = []\n",
        "    for _ in range(levels):\n",
        "        sim, cs = ssim(img1, img2, window_size=window_size, size_average=size_average, full=True, val_range=val_range)\n",
        "\n",
        "        # Relu normalize (not compliant with original definition)\n",
        "        if normalize == \"relu\":\n",
        "            ssims.append(torch.relu(sim))\n",
        "            mcs.append(torch.relu(cs))\n",
        "        else:\n",
        "            ssims.append(sim)\n",
        "            mcs.append(cs)\n",
        "\n",
        "        img1 = F.avg_pool2d(img1, (2, 2))\n",
        "        img2 = F.avg_pool2d(img2, (2, 2))\n",
        "\n",
        "    ssims = torch.stack(ssims)\n",
        "    mcs = torch.stack(mcs)\n",
        "\n",
        "    # Simple normalize (not compliant with original definition)\n",
        "    if normalize == \"simple\" or normalize == True:\n",
        "        ssims = (ssims + 1) / 2\n",
        "        mcs = (mcs + 1) / 2\n",
        "\n",
        "    pow1 = mcs ** weights\n",
        "    pow2 = ssims ** weights\n",
        "\n",
        "    # From Matlab implementation https://ece.uwaterloo.ca/~z70wang/research/iwssim/\n",
        "    output = torch.prod(pow1[:-1]) * pow2[-1]\n",
        "    return output\n",
        "\n",
        "\n",
        "# Classes to re-use window\n",
        "class SSIM(torch.nn.Module):\n",
        "    def __init__(self, window_size=11, size_average=True, val_range=None):\n",
        "        super(SSIM, self).__init__()\n",
        "        self.window_size = window_size\n",
        "        self.size_average = size_average\n",
        "        self.val_range = val_range\n",
        "\n",
        "        # Assume 1 channel for SSIM\n",
        "        self.channel = 1\n",
        "        self.window = create_window(window_size)\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        (_, channel, _, _) = img1.size()\n",
        "\n",
        "        if channel == self.channel and self.window.dtype == img1.dtype:\n",
        "            window = self.window\n",
        "        else:\n",
        "            window = create_window(self.window_size, channel).to(img1.device).type(img1.dtype)\n",
        "            self.window = window\n",
        "            self.channel = channel\n",
        "\n",
        "        return ssim(img1, img2, window=window, window_size=self.window_size, size_average=self.size_average)\n",
        "\n",
        "class MSSSIM(torch.nn.Module):\n",
        "    def __init__(self, window_size=11, size_average=True, channel=3):\n",
        "        super(MSSSIM, self).__init__()\n",
        "        self.window_size = window_size\n",
        "        self.size_average = size_average\n",
        "        self.channel = channel\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        return msssim(img1, img2, window_size=self.window_size, size_average=self.size_average)"
      ],
      "metadata": {
        "id": "a2cjDnixFciu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# criteria.moco_loss.py\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "# from configs.paths_config import model_paths\n",
        "\n",
        "\n",
        "class MocoLoss(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MocoLoss, self).__init__()\n",
        "        print(\"Loading MOCO model from path: {}\".format(model_paths[\"moco\"]))\n",
        "        self.model = self.__load_model()\n",
        "        self.model.cuda()\n",
        "        self.model.eval()\n",
        "\n",
        "    @staticmethod\n",
        "    def __load_model():\n",
        "        import torchvision.models as models\n",
        "        model = models.__dict__[\"resnet50\"]()\n",
        "        # freeze all layers but the last fc\n",
        "        for name, param in model.named_parameters():\n",
        "            if name not in ['fc.weight', 'fc.bias']:\n",
        "                param.requires_grad = False\n",
        "        checkpoint = torch.load(model_paths['moco'], map_location=\"cpu\")\n",
        "        state_dict = checkpoint['state_dict']\n",
        "        # rename moco pre-trained keys\n",
        "        for k in list(state_dict.keys()):\n",
        "            # retain only encoder_q up to before the embedding layer\n",
        "            if k.startswith('module.encoder_q') and not k.startswith('module.encoder_q.fc'):\n",
        "                # remove prefix\n",
        "                state_dict[k[len(\"module.encoder_q.\"):]] = state_dict[k]\n",
        "            # delete renamed or unused k\n",
        "            del state_dict[k]\n",
        "        msg = model.load_state_dict(state_dict, strict=False)\n",
        "        assert set(msg.missing_keys) == {\"fc.weight\", \"fc.bias\"}\n",
        "        # remove output layer\n",
        "        model = nn.Sequential(*list(model.children())[:-1]).cuda()\n",
        "        return model\n",
        "\n",
        "    def extract_feats(self, x):\n",
        "        x = F.interpolate(x, size=224)\n",
        "        x_feats = self.model(x)\n",
        "        x_feats = nn.functional.normalize(x_feats, dim=1)\n",
        "        x_feats = x_feats.squeeze()\n",
        "        return x_feats\n",
        "\n",
        "    def forward(self, y_hat, y, x):\n",
        "        n_samples = x.shape[0]\n",
        "        x_feats = self.extract_feats(x)\n",
        "        y_feats = self.extract_feats(y)\n",
        "        y_hat_feats = self.extract_feats(y_hat)\n",
        "        y_feats = y_feats.detach()\n",
        "        loss = 0\n",
        "        sim_improvement = 0\n",
        "        sim_logs = []\n",
        "        count = 0\n",
        "        for i in range(n_samples):\n",
        "            diff_target = y_hat_feats[i].dot(y_feats[i])\n",
        "            diff_input = y_hat_feats[i].dot(x_feats[i])\n",
        "            diff_views = y_feats[i].dot(x_feats[i])\n",
        "            sim_logs.append({'diff_target': float(diff_target),\n",
        "                             'diff_input': float(diff_input),\n",
        "                             'diff_views': float(diff_views)})\n",
        "            loss += 1 - diff_target\n",
        "            sim_diff = float(diff_target) - float(diff_views)\n",
        "            sim_improvement += sim_diff\n",
        "            count += 1\n",
        "\n",
        "        return loss / count, sim_improvement / count, sim_logs"
      ],
      "metadata": {
        "id": "1t_SyedfFT53"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import ArgumentParser\n",
        "\n",
        "# from configs.paths_config import model_paths\n",
        "\n",
        "\n",
        "class TrainOptions:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.parser = ArgumentParser()\n",
        "        self.initialize()\n",
        "\n",
        "    def initialize(self):\n",
        "        # general setup\n",
        "        self.parser.add_argument('--exp_dir', type=str,\n",
        "                                 help='Path to experiment output directory')\n",
        "        self.parser.add_argument('--dataset_type', default='ffhq_encode', type=str,\n",
        "                                 help='Type of dataset/experiment to run')\n",
        "        self.parser.add_argument('--encoder_type', default='HyperNet', type=str,\n",
        "                                 help='Which encoder to use')\n",
        "        self.parser.add_argument('--input_nc', default=6, type=int,\n",
        "                                 help='Number of input image channels to the HyperStyle network. Should be set to 6.')\n",
        "        self.parser.add_argument('--output_size', default=1024, type=int,\n",
        "                                 help='Output size of generator')\n",
        "\n",
        "        # batch size and dataloader works\n",
        "        self.parser.add_argument('--batch_size', default=4, type=int,\n",
        "                                 help='Batch size for training')\n",
        "        self.parser.add_argument('--test_batch_size', default=2, type=int,\n",
        "                                 help='Batch size for testing and inference')\n",
        "        self.parser.add_argument('--workers', default=4, type=int,\n",
        "                                 help='Number of train dataloader workers')\n",
        "        self.parser.add_argument('--test_workers', default=2, type=int,\n",
        "                                 help='Number of test/inference dataloader workers')\n",
        "\n",
        "        # optimizers\n",
        "        self.parser.add_argument('--learning_rate', default=0.0001, type=float,\n",
        "                                 help='Optimizer learning rate')\n",
        "        self.parser.add_argument('--optim_name', default='ranger', type=str,\n",
        "                                 help='Which optimizer to use')\n",
        "        self.parser.add_argument('--train_decoder', default=False, type=bool,\n",
        "                                 help='Whether to train the decoder model')\n",
        "\n",
        "        # loss lambdas\n",
        "        self.parser.add_argument('--lpips_lambda', default=0, type=float,\n",
        "                                 help='LPIPS loss multiplier factor')\n",
        "        self.parser.add_argument('--id_lambda', default=0, type=float,\n",
        "                                 help='ID loss multiplier factor')\n",
        "        self.parser.add_argument('--l2_lambda', default=0, type=float,\n",
        "                                 help='L2 loss multiplier factor')\n",
        "        self.parser.add_argument('--moco_lambda', default=0, type=float,\n",
        "                                 help='Moco feature loss multiplier factor')\n",
        "\n",
        "        # weights and checkpoint paths\n",
        "        self.parser.add_argument('--stylegan_weights', default=model_paths[\"stylegan_ffhq\"], type=str,\n",
        "                                 help='Path to StyleGAN model weights')\n",
        "        self.parser.add_argument('--checkpoint_path', default=None, type=str,\n",
        "                                 help='Path to HyperStyle model checkpoint')\n",
        "\n",
        "        # intervals for logging, validation, and saving\n",
        "        self.parser.add_argument('--max_steps', default=500000, type=int,\n",
        "                                 help='Maximum number of training steps')\n",
        "        self.parser.add_argument('--max_val_batches', type=int, default=None,\n",
        "                                 help='Number of batches to run validation on. If None, run on all batches.')\n",
        "        self.parser.add_argument('--image_interval', default=100, type=int,\n",
        "                                 help='Interval for logging train images during training')\n",
        "        self.parser.add_argument('--board_interval', default=50, type=int,\n",
        "                                 help='Interval for logging metrics to tensorboard')\n",
        "        self.parser.add_argument('--val_interval', default=1000, type=int,\n",
        "                                 help='Validation interval')\n",
        "        self.parser.add_argument('--save_interval', default=None, type=int,\n",
        "                                 help='Model checkpoint interval')\n",
        "\n",
        "        # arguments for iterative encoding\n",
        "        self.parser.add_argument('--n_iters_per_batch', default=1, type=int,\n",
        "                                 help='Number of forward passes per batch during training')\n",
        "\n",
        "        # hypernet parameters\n",
        "        self.parser.add_argument('--load_w_encoder', action='store_true', help='Whether to load the w e4e encoder.')\n",
        "        self.parser.add_argument('--w_encoder_checkpoint_path', default=model_paths[\"e4e_w_encoder\"], type=str,\n",
        "                                 help='Path to pre-trained W-encoder.')\n",
        "        self.parser.add_argument('--w_encoder_type', default='WEncoder',\n",
        "                                 help='Encoder type for the encoder used to get the initial inversion')\n",
        "        self.parser.add_argument('--layers_to_tune', default='0,2,3,5,6,8,9,11,12,14,15,17,18,20,21,23,24', type=str,\n",
        "                                 help='comma-separated list of which layers of the StyleGAN generator to tune')\n",
        "\n",
        "    def parse(self):\n",
        "        opts = self.parser.parse_args()\n",
        "        return "
      ],
      "metadata": {
        "id": "RoQrHNbX6eXj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# criteria.id_loss.py\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "# from configs.paths_config import model_paths\n",
        "# from models.encoders.model_irse import Backbone\n",
        "\n",
        "\n",
        "class IDLoss(nn.Module):\n",
        "    def __init__(self, opts):\n",
        "        super(IDLoss, self).__init__()\n",
        "        print('Loading ResNet ArcFace')\n",
        "        self.facenet = Backbone(input_size=112, num_layers=50, drop_ratio=0.6, mode='ir_se')\n",
        "        self.facenet.load_state_dict(torch.load(model_paths['ir_se50']))\n",
        "        self.face_pool = torch.nn.AdaptiveAvgPool2d((112, 112))\n",
        "        self.facenet.eval()\n",
        "        self.opts = opts\n",
        "\n",
        "    def extract_feats(self, x):\n",
        "        x = x[:, :, 35:223, 32:220]  # Crop interesting region\n",
        "        x = self.face_pool(x)\n",
        "        x_feats = self.facenet(x)\n",
        "        return x_feats\n",
        "\n",
        "    def forward(self, y_hat, y, x):\n",
        "        n_samples = x.shape[0]\n",
        "        x_feats = self.extract_feats(x)\n",
        "        y_feats = self.extract_feats(y)  # Otherwise use the feature from there\n",
        "        y_hat_feats = self.extract_feats(y_hat)\n",
        "        y_feats = y_feats.detach()\n",
        "        loss = 0\n",
        "        sim_improvement = 0\n",
        "        id_logs = []\n",
        "        count = 0\n",
        "        for i in range(n_samples):\n",
        "            diff_target = y_hat_feats[i].dot(y_feats[i])\n",
        "            diff_input = y_hat_feats[i].dot(x_feats[i])\n",
        "            diff_views = y_feats[i].dot(x_feats[i])\n",
        "            id_logs.append({'diff_target': float(diff_target),\n",
        "                            'diff_input': float(diff_input),\n",
        "                            'diff_views': float(diff_views)})\n",
        "            loss += 1 - diff_target\n",
        "            id_diff = float(diff_target) - float(diff_views)\n",
        "            sim_improvement += id_diff\n",
        "            count += 1\n",
        "\n",
        "        return loss / count, sim_improvement / count, id_logs"
      ],
      "metadata": {
        "id": "a8VPPKa6DWAQ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "c0xctupj486l",
        "outputId": "64b67eb1-ea72-4aab-abee-c3af0582c87e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-8081e4909452>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-8081e4909452>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mcreate_initial_experiment_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mcoach\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-d4b6ea8cd221>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArgumentParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-d4b6ea8cd221>\u001b[0m in \u001b[0;36minitialize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# hypernet parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--load_w_encoder'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'store_true'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Whether to load the w e4e encoder.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         self.parser.add_argument('--w_encoder_checkpoint_path', default=model_paths[\"e4e_w_encoder\"], type=str,\n\u001b[0m\u001b[1;32m     80\u001b[0m                                  help='Path to pre-trained W-encoder.')\n\u001b[1;32m     81\u001b[0m         self.parser.add_argument('--w_encoder_type', default='WEncoder',\n",
            "\u001b[0;31mKeyError\u001b[0m: 'e4e_w_encoder'"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "This file runs the main training/val loop\n",
        "\"\"\"\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import pprint\n",
        "\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "# from options.train_options import TrainOptions\n",
        "# from training.coach_hyperstyle import Coach\n",
        "\n",
        "\n",
        "def main():\n",
        "\topts = TrainOptions().parse()\n",
        "\tcreate_initial_experiment_dir(opts)\n",
        "\tcoach = Coach(opts)\n",
        "\tcoach.train()\n",
        "\n",
        "\n",
        "def create_initial_experiment_dir(opts):\n",
        "\tos.makedirs(opts.exp_dir, exist_ok=True)\n",
        "\topts_dict = vars(opts)\n",
        "\tpprint.pprint(opts_dict)\n",
        "\twith open(os.path.join(opts.exp_dir, 'opt.json'), 'w') as f:\n",
        "\t\tjson.dump(opts_dict, f, indent=4, sort_keys=True)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\tmain()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "x-xiEQQJ6cUU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}